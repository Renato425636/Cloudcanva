<!DOCTYPE html>
<html lang="pt-BR" class="">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cloud Canvas - Simulador de Pipeline de Nuvem</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            darkMode: 'class',
        }
    </script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .service-icon {
            transition: all 0.2s ease-in-out;
            cursor: grab;
        }
        .service-icon:active {
            cursor: grabbing;
            transform: scale(1.1);
        }
        .drop-zone {
            transition: all 0.2s ease-in-out;
            border: 2px dashed; /* Color is handled by Tailwind now */
            aspect-ratio: 1 / 1;
        }
        .drop-zone.drag-over {
            border-color: #4f46e5; /* indigo-600 */
            background-color: #e0e7ff; /* indigo-100 */
        }
        .dark .drop-zone.drag-over {
            border-color: #818cf8; /* indigo-400 */
            background-color: #3730a3; /* indigo-800 */
        }
        .modal-overlay {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: rgba(0, 0, 0, 0.5);
            display: flex;
            align-items: center;
            justify-content: center;
            z-index: 50;
            backdrop-filter: blur(4px);
        }
        .arrow {
            font-size: 2.5rem;
            line-height: 1;
            align-self: center;
        }
    </style>
    <script>
        // Handle dark mode toggling and initial load
        if (localStorage.getItem('theme') === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
            document.documentElement.classList.add('dark');
        } else {
            document.documentElement.classList.remove('dark');
        }

        function toggleTheme() {
            const html = document.documentElement;
            const isDark = html.classList.toggle('dark');
            localStorage.setItem('theme', isDark ? 'dark' : 'light');
            updateThemeIcons();
        }
        
        function updateThemeIcons(){
             const sunIcon = document.getElementById('sun-icon');
             const moonIcon = document.getElementById('moon-icon');
             if (document.documentElement.classList.contains('dark')) {
                sunIcon.classList.remove('hidden');
                moonIcon.classList.add('hidden');
             } else {
                sunIcon.classList.add('hidden');
                moonIcon.classList.remove('hidden');
             }
        }

        window.addEventListener('DOMContentLoaded', updateThemeIcons);
    </script>
</head>
<body class="bg-slate-100 dark:bg-slate-900 text-slate-800 dark:text-slate-200">

    <div class="flex h-screen antialiased">
        <!-- Sidebar -->
        <aside class="w-64 flex-shrink-0 bg-white dark:bg-slate-800 p-4 flex flex-col justify-between border-r border-slate-200 dark:border-slate-700">
            <div>
                <a href="#" onclick="goBack(event)" class="flex items-center space-x-3 px-2 mb-10 cursor-pointer">
                    <!-- New Logo SVG -->
                    <svg class="h-8 w-8 text-indigo-600 dark:text-indigo-400" width="24" height="24" viewBox="0 0 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M19.333 9.14286C21.46 10.1143 23 12.1857 23 14.5714C23 17.8143 20.313 20.5 17.07 20.5H7.11C3.868 20.5 1.18 17.8143 1.18 14.5714C1.18 11.5286 3.558 9.04286 6.53 8.57143C7.11 5.32857 9.968 3 13.409 3C16.095 3 18.397 4.54286 19.333 6.85714V9.14286Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"/>
                        <path d="M7 15L10 12L13 15L16 12" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"/>
                    </svg>
                    <h2 class="text-2xl font-bold text-slate-800 dark:text-white">Cloud Canvas</h2>
                </a>
                <nav class="space-y-2">
                    <a href="#" onclick="goBack(event)" class="sidebar-link flex items-center space-x-3 text-slate-600 dark:text-slate-300 p-3 rounded-lg hover:bg-slate-100 dark:hover:bg-slate-700">
                        <svg class="h-6 w-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24" stroke-width="2" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" d="M21 7.5l-9-5.25L3 7.5m18 0l-9 5.25m9-5.25v9l-9 5.25M3 7.5l9 5.25M3 7.5v9l9 5.25m0-9v9" />
                        </svg>
                        <span>Simulados</span>
                    </a>
                </nav>
            </div>
            <!-- Dark Mode Toggle -->
             <div class="mt-4">
                <button onclick="toggleTheme()" class="w-full flex items-center justify-center p-3 rounded-lg text-slate-600 dark:text-slate-300 hover:bg-slate-100 dark:hover:bg-slate-700">
                    <svg id="sun-icon" xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 hidden" fill="none" viewBox="0 0 24" stroke="currentColor" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" /></svg>
                    <svg id="moon-icon" xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 hidden" fill="none" viewBox="0 0 24" stroke="currentColor" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" /></svg>
                    <span class="ml-2">Alternar Tema</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="flex-1 overflow-y-auto">
            <div id="app-container" class="container mx-auto p-4 md:p-8 max-w-7xl">

                <!-- TELA DE SELEÇÃO DE PLATAFORMA -->
                <div id="platform-selection-screen" class="text-center">
                    <h1 class="text-3xl md:text-4xl font-bold text-slate-900 dark:text-white mb-2">Simulador de Pipeline de Nuvem</h1>
                    <p class="text-slate-600 dark:text-slate-400 mb-8 max-w-2xl mx-auto">Passo 1: Escolha uma plataforma de nuvem para começar.</p>
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-6">
                        <div onclick="selectPlatform('AWS')" class="bg-white dark:bg-slate-800 rounded-2xl p-6 md:p-8 text-center cursor-pointer transition-all duration-300 border-2 border-transparent shadow-lg hover:shadow-xl hover:-translate-y-1 hover:border-orange-500">
                            <img src="https://upload.wikimedia.org/wikipedia/commons/9/93/Amazon_Web_Services_Logo.svg" alt="AWS Logo" class="h-16 mx-auto mb-4 filter dark:brightness-0 dark:invert">
                            <h2 class="text-2xl font-semibold text-slate-800 dark:text-white">AWS</h2>
                        </div>
                        <div onclick="selectPlatform('Azure')" class="bg-white dark:bg-slate-800 rounded-2xl p-6 md:p-8 text-center cursor-pointer transition-all duration-300 border-2 border-transparent shadow-lg hover:shadow-xl hover:-translate-y-1 hover:border-blue-500">
                            <img src="https://upload.wikimedia.org/wikipedia/commons/a/a8/Microsoft_Azure_Logo.svg" alt="Azure Logo" class="h-16 mx-auto mb-4">
                            <h2 class="text-2xl font-semibold text-slate-800 dark:text-white">Azure</h2>
                        </div>
                        <div onclick="selectPlatform('GCP')" class="bg-white dark:bg-slate-800 rounded-2xl p-6 md:p-8 text-center cursor-pointer transition-all duration-300 border-2 border-transparent shadow-lg hover:shadow-xl hover:-translate-y-1 hover:border-green-500">
                            <img src="https://www.gstatic.com/devrel-devsite/prod/v55e81371229cf93fbb4781915f01d3bef8e4cb4b674c7c839a1879ebb706855a/cloud/images/social-icon-google-cloud-1200-630.png" alt="GCP Logo" class="h-16 mx-auto mb-4">
                            <h2 class="text-2xl font-semibold text-slate-800 dark:text-white">GCP</h2>
                        </div>
                    </div>
                </div>

                <!-- TELA DE SELEÇÃO DE OBJETIVO -->
                <div id="objective-selection-screen" class="hidden text-center">
                    <h1 class="text-3xl md:text-4xl font-bold text-slate-900 dark:text-white mb-2">Objetivo do Simulado</h1>
                    <p class="text-slate-600 dark:text-slate-400 mb-8 max-w-2xl mx-auto">Passo 2: Qual área de conhecimento você quer praticar?</p>
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-6">
                        <div onclick="selectObjective('data-engineering')" class="bg-white dark:bg-slate-800 rounded-2xl p-6 md:p-8 text-center cursor-pointer transition-all duration-300 border-2 border-transparent shadow-lg hover:shadow-xl hover:-translate-y-1 hover:border-indigo-500">
                            <h2 class="text-2xl font-semibold text-slate-800 dark:text-white">Engenharia de Dados</h2>
                            <p class="text-slate-500 dark:text-slate-400 mt-2">ETL, Big Data e pipelines de streaming.</p>
                        </div>
                        <div onclick="selectObjective('data-analysis')" class="bg-white dark:bg-slate-800 rounded-2xl p-6 md:p-8 text-center cursor-pointer transition-all duration-300 border-2 border-transparent shadow-lg hover:shadow-xl hover:-translate-y-1 hover:border-indigo-500">
                             <h2 class="text-2xl font-semibold text-slate-800 dark:text-white">Análise de Dados</h2>
                             <p class="text-slate-500 dark:text-slate-400 mt-2">BI, Data Warehousing e consultas.</p>
                        </div>
                        <div onclick="selectObjective('cloud-operations')" class="bg-white dark:bg-slate-800 rounded-2xl p-6 md:p-8 text-center cursor-pointer transition-all duration-300 border-2 border-transparent shadow-lg hover:shadow-xl hover:-translate-y-1 hover:border-indigo-500">
                             <h2 class="text-2xl font-semibold text-slate-800 dark:text-white">Operações & Automação</h2>
                             <p class="text-slate-500 dark:text-slate-400 mt-2">APIs, microsserviços e automação de TI.</p>
                        </div>
                    </div>
                </div>
                
                <!-- TELA DE SELEÇÃO DE DIFICULDADE -->
                <div id="difficulty-selection-screen" class="hidden text-center">
                    <h1 class="text-3xl md:text-4xl font-bold text-slate-900 dark:text-white mb-2">Nível do Desafio</h1>
                    <p class="text-slate-600 dark:text-slate-400 mb-8 max-w-2xl mx-auto">Passo 3: Escolha a dificuldade do seu simulado.</p>
                    <div class="grid grid-cols-2 md:grid-cols-5 gap-6">
                        <div onclick="selectDifficulty(1)" class="bg-white dark:bg-slate-800 rounded-2xl p-6 md:p-8 text-center cursor-pointer transition-all duration-300 border-2 border-transparent shadow-lg hover:shadow-xl hover:-translate-y-1 hover:border-green-500"><h2 class="text-2xl font-semibold text-slate-800 dark:text-white">Nível 1</h2><p class="text-slate-500 dark:text-slate-400 mt-2">Fundamental</p></div>
                        <div onclick="selectDifficulty(2)" class="bg-white dark:bg-slate-800 rounded-2xl p-6 md:p-8 text-center cursor-pointer transition-all duration-300 border-2 border-transparent shadow-lg hover:shadow-xl hover:-translate-y-1 hover:border-yellow-500"><h2 class="text-2xl font-semibold text-slate-800 dark:text-white">Nível 2</h2><p class="text-slate-500 dark:text-slate-400 mt-2">Intermediário</p></div>
                        <div onclick="selectDifficulty(3)" class="bg-white dark:bg-slate-800 rounded-2xl p-6 md:p-8 text-center cursor-pointer transition-all duration-300 border-2 border-transparent shadow-lg hover:shadow-xl hover:-translate-y-1 hover:border-orange-500"><h2 class="text-2xl font-semibold text-slate-800 dark:text-white">Nível 3</h2><p class="text-slate-500 dark:text-slate-400 mt-2">Avançado</p></div>
                        <div onclick="selectDifficulty(4)" class="bg-white dark:bg-slate-800 rounded-2xl p-6 md:p-8 text-center cursor-pointer transition-all duration-300 border-2 border-transparent shadow-lg hover:shadow-xl hover:-translate-y-1 hover:border-red-500"><h2 class="text-2xl font-semibold text-slate-800 dark:text-white">Nível 4</h2><p class="text-slate-500 dark:text-slate-400 mt-2">Especialista</p></div>
                        <div onclick="selectDifficulty(5)" class="bg-white dark:bg-slate-800 rounded-2xl p-6 md:p-8 text-center cursor-pointer transition-all duration-300 border-2 border-transparent shadow-lg hover:shadow-xl hover:-translate-y-1 hover:border-purple-500"><h2 class="text-2xl font-semibold text-slate-800 dark:text-white">Nível 5</h2><p class="text-slate-500 dark:text-slate-400 mt-2">Arquiteto</p></div>
                    </div>
                </div>


                <!-- TELA DE SIMULAÇÃO PRINCIPAL -->
                <div id="main-simulation-screen" class="hidden">
                    <div class="flex flex-wrap justify-between items-center mb-6 gap-4">
                         <h1 class="text-3xl font-bold text-slate-900 dark:text-white">Desafio: <span id="platform-name" class="font-extrabold text-indigo-600 dark:text-indigo-400"></span></h1>
                         <div class="flex items-center space-x-4">
                            <span id="quiz-progress" class="text-lg font-semibold bg-white dark:bg-slate-800 px-4 py-2 rounded-lg shadow-sm">Questão 1 de 2</span>
                         </div>
                    </div>
                    
                    <div id="scenario-container" class="bg-white dark:bg-slate-800 p-6 rounded-2xl mb-6 shadow-lg">
                        <h2 class="text-xl font-semibold text-slate-800 dark:text-white mb-2">Situação Proposta:</h2>
                        <p id="scenario-question" class="text-slate-600 dark:text-slate-300 leading-relaxed"></p>
                    </div>

                    <div id="challenge-ui" class="grid grid-cols-1 lg:grid-cols-4 gap-6">
                        <div class="lg:col-span-1 bg-white dark:bg-slate-800 p-6 rounded-2xl shadow-lg">
                            <h3 class="text-lg font-semibold mb-4 border-b border-slate-200 dark:border-slate-700 pb-2 text-slate-800 dark:text-white">Caixa de Ferramentas</h3>
                            <p class="text-sm text-slate-500 dark:text-slate-400 mb-4">Arraste os serviços para construir o pipeline.</p>
                            <div id="services-palette" class="grid grid-cols-2 gap-4"></div>
                        </div>

                        <div class="lg:col-span-3 bg-white dark:bg-slate-800 p-6 rounded-2xl shadow-lg">
                            <h3 class="text-lg font-semibold mb-4 border-b border-slate-200 dark:border-slate-700 pb-2 text-slate-800 dark:text-white">Construa seu Pipeline</h3>
                            <p class="text-sm text-slate-500 dark:text-slate-400 mb-4">Organize os serviços na ordem correta.</p>
                            <div class="w-full">
                                <div id="pipeline-canvas" class="flex flex-nowrap items-center justify-evenly w-full gap-2 p-4 bg-slate-100 dark:bg-slate-900/50 rounded-lg border border-slate-200 dark:border-slate-700 min-h-[192px]"></div>
                            </div>
                            
                            <div class="mt-6 flex justify-end space-x-4">
                                <button onclick="resetPipeline()" class="bg-slate-500 hover:bg-slate-600 dark:bg-slate-600 dark:hover:bg-slate-500 text-white font-bold py-2 px-6 rounded-lg transition-colors">Resetar</button>
                                <button onclick="evaluateSolution()" class="bg-indigo-600 hover:bg-indigo-700 dark:bg-indigo-500 dark:hover:bg-indigo-600 text-white font-bold py-2 px-6 rounded-lg transition-colors">Avaliar Solução</button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </main>
    </div>

    <!-- MODAL DE FEEDBACK -->
    <div id="feedback-modal" class="modal-overlay hidden">
        <div class="bg-white dark:bg-slate-800 border border-slate-200 dark:border-slate-700 rounded-2xl shadow-2xl p-8 max-w-4xl w-full transform transition-all" id="feedback-content">
            <h2 id="feedback-title" class="text-3xl font-bold mb-4 text-center"></h2>
            <div id="feedback-message" class="text-slate-600 dark:text-slate-300 mb-6"></div>
            <button id="feedback-button" class="text-white font-bold py-3 px-8 rounded-lg transition-colors w-full">Tentar Novamente</button>
        </div>
    </div>
    
    <!-- MODAL DO PIX -->
    <div id="pix-modal" class="modal-overlay hidden">
        <div class="bg-white dark:bg-slate-800 border border-slate-200 dark:border-slate-700 rounded-2xl shadow-2xl p-8 max-w-md w-full text-center transform transition-all flex flex-col items-center">
            <h2 class="text-3xl font-bold mb-4 text-slate-800 dark:text-white">Apoie o <span class="text-indigo-600 dark:text-indigo-400">Cloud Canvas</span>!</h2>
            <div class="p-2 bg-white rounded-lg shadow-md my-4">
                <img src="https://raw.githubusercontent.com/Renato425636/Cloudcanva/refs/heads/main/qrcode-pix.png" alt="PIX QR Code para doação" class="rounded-lg">
            </div>
            <p class="text-slate-600 dark:text-slate-400 mb-6">Sua jornada na nuvem está decolando! Se este simulador está te ajudando, que tal mandar um PIX para abastecer nosso café e mantermos a plataforma voando alto? Qualquer valor é um mega incentivo!</p>
            <div class="flex justify-center space-x-4 w-full">
                <button id="pix-secondary-button" class="bg-slate-200 hover:bg-slate-300 dark:bg-slate-700 dark:hover:bg-slate-600 text-slate-800 dark:text-slate-200 font-bold py-3 px-6 rounded-lg transition-colors w-1/2">Fechar</button>
                <button id="pix-primary-button" class="bg-indigo-600 hover:bg-indigo-700 dark:bg-indigo-500 dark:hover:bg-indigo-600 text-white font-bold py-3 px-6 rounded-lg transition-colors w-1/2">Iniciar Simulado</button>
            </div>
        </div>
    </div>

    <!-- MODAL DE CONFIRMAÇÃO DE SAÍDA -->
    <div id="confirm-exit-modal" class="modal-overlay hidden">
        <div class="bg-white dark:bg-slate-800 border-slate-200 dark:border-slate-700 rounded-2xl shadow-2xl p-8 max-w-md w-full text-center transform transition-all">
            <h2 class="text-2xl font-bold mb-4 text-slate-800 dark:text-white">Sair do Simulado?</h2>
            <p class="text-slate-600 dark:text-slate-400 mb-8">Você tem certeza que deseja sair? Todo o seu progresso neste simulado será perdido.</p>
            <div class="flex justify-center space-x-4 w-full">
                <button onclick="document.getElementById('confirm-exit-modal').classList.add('hidden')" class="bg-slate-200 hover:bg-slate-300 dark:bg-slate-700 dark:hover:bg-slate-600 text-slate-800 dark:text-slate-200 font-bold py-3 px-6 rounded-lg transition-colors w-1/2">Cancelar</button>
                <button onclick="confirmExit()" class="bg-red-600 hover:bg-red-700 text-white font-bold py-3 px-6 rounded-lg transition-colors w-1/2">Sair</button>
            </div>
        </div>
    </div>

    <!-- MODAL DE RESUMO DO SIMULADO -->
    <div id="quiz-summary-modal" class="modal-overlay hidden">
        <div class="bg-white dark:bg-slate-800 border-slate-200 dark:border-slate-700 rounded-2xl shadow-2xl p-8 max-w-4xl w-full transform transition-all">
            <h2 class="text-3xl font-bold mb-6 text-center text-slate-800 dark:text-white">Resumo do Simulado</h2>
            <div id="summary-content" class="space-y-6 max-h-[60vh] overflow-y-auto pr-4">
                <!-- O conteúdo do resumo será inserido aqui dinamicamente -->
            </div>
            <div class="flex justify-center mt-8">
                 <button onclick="confirmExit()" class="bg-indigo-600 hover:bg-indigo-700 dark:bg-indigo-500 dark:hover:bg-indigo-600 text-white font-bold py-3 px-8 rounded-lg transition-colors">Jogar Novamente</button>
            </div>
        </div>
    </div>


    <script>
        // --- DATA ---
        const services = {
            AWS: {
                's3': { name: 'S3', icon: 'https://cdn.worldvectorlogo.com/logos/amazon-s3-simple-storage-service.svg' },
                'lambda': { name: 'Lambda', icon: 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Amazon_Lambda_architecture_logo.svg/1200px-Amazon_Lambda_architecture_logo.svg.png' },
                'kinesis': { name: 'Kinesis Data Streams', icon: 'https://miro.medium.com/v2/1*zpLyF0dS35Xqtb98KBokUQ.png' },
                'glue': { name: 'Glue', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-glue-9ztw380gkkd1g54iwwsq7.png/aws-glue-g9i4j0s3igbjmai4vernz9.png?_a=DATAg1AAZAA0' },
                
                'redshift': { name: 'Redshift', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-redshift-66wh61ox3onpkqug27epnp.png/aws-redshift-5ucj7jrp6gm0c806t97m0w7.png?_a=DATAg1AAZAA0' },
                'dynamodb': { name: 'DynamoDB', icon: 'https://assets.streamlinehq.com/image/private/w_100,h_100,ar_1/f_auto/v1/icons/1/aws-dynamodb-jx5xy9kzifih9n75x0ty.png/aws-dynamodb-75zeqyp2syrkdpowzfwkfk.png?_a=DATAg1AAZAA0' },
                'step-functions': { name: 'Step Functions', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-step-functions-bdagl2uve6mc5xyzf8ut3i.png/aws-step-functions-yoa6fs8dq2q0mx085wgofne.png?_a=DATAg1AAZAA0' },
                
                'api-gateway': { name: 'API Gateway', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-api-gateway-tj2dkgxvdofaqvkyugqgoq.png/aws-api-gateway-pqt4djhl3zincgtqcvevf.png?_a=DATAg1AAZAA0' },
                
                'sqs': { name: 'SQS', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-sqs-7kygzksl919u4buvdh6ue.png/aws-sqs-86gd6l3l1p4q97mmn6pq.png?_a=DATAg1AAZAA0' },
                
                'sns': { name: 'SNS', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-sns-80t5m6hy89y74i2ta3yrz3.png/aws-sns-y7pv8d8lqjroqxkucbnv.png?_a=DATAg1AAZAA0' },
                
                'quicksight': { name: 'QuickSight', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-quicksight-ifwv0kt6mikq1bavest9.png/aws-quicksight-8js43tpc1tbuehqwbcdbz.png?_a=DATAg1AAZAA0' },
                
                'athena': { name: 'Athena', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-athena-hv6gsv93ozj2o0gsxdtg6m.png/aws-athena-jan6k55udjsv6va5uwobn.png?_a=DATAg1AAZAA0' },
                
                'rds': { name: 'RDS', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-rds-5qblz3nvfjmipvbykjpba.png/aws-rds-o6rtmustsgv3emnpozj4.png?_a=DATAg1AAZAA0' },
                
                'emr': { name: 'EMR', icon: 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSuWSNRRjNDqzoFqTElllBAZ3hqwx7ht8wmag&s' },
                
                'dms': { name: 'DMS', icon: 'https://res.cloudinary.com/hy4kyit2a/f_auto,fl_lossy,q_70/learn/modules/aws-cloud-acquisition/migrate-to-the-aws-cloud/images/ba8da6c9295b1aea44d1bdf69b4282a2_kix.ogmmcnow3wz5.png' },
                
                'lake-formation': { name: 'Lake Formation', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-lake-formation-4k6cs4q7djvfa5aq5w6aia.png/aws-lake-formation-23j99h65239rm49v0apt2.png?_a=DATAg1AAZAA0' },
                
                'msk': { name: 'MSK', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-msk-u1q98yujn797oz22k03v45.png/aws-msk-k7u06h5figugvrf4undx.png?_a=DATAg1AAZAA0' },
                
                'opensearch': { name: 'OpenSearch', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-open-search-1yzngnyfogqu7r8pfp894e.png/aws-open-search-maq7fazqolbct8d7m9ou8s.png?_a=DATAg1AAZAA0' },
                
                'mwaa': { name: 'MWAA', icon: 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ_kK7Z29b3Ojl5_xtS85ymlj2B7ezhAtEpNQ&s' },
                
                'sagemaker': { name: 'SageMaker', icon: 'https://d2908q01vomqb2.cloudfront.net/77de68daecd823babbb58edb1c8e14d7106e83bb/2018/04/24/SageMaker.jpg' },
                'kinesis-firehose': { name: 'Kinesis Firehose', icon: 'https://newrelic.com/sites/default/files/quickstarts/images/icons/aws-kinesis-data-firehose--logo.svg' },
                
                'ec2': { name: 'EC2', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-ec2-bi87vqoabfbmadlrtmtp1n.png/aws-ec2-8tgwuch5vagt0ftrucnghq.png?_a=DATAg1AAZAA0' },
                
                'ecs': { name: 'ECS', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-ecs-r57h5csktlbovwelzaln3.png/aws-ecs-mstmcs5adrqf2r2w1lxyqm.png?_a=DATAg1AAZAA0' },
                
                'cloudformation': { name: 'CloudFormation', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-cloudformation-xhfgoczq9vcaicwekcz3x4.png/aws-cloudformation-ld8d8ypjpnd18eskmfgbo.png?_a=DATAg1AAZAA0' },
                
                'cognito': { name: 'Cognito', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-cognito-jh49xr1uergt6lq61nzw7.png/aws-cognito-1lwcmhnknd8rynq09p18rh.png?_a=DATAg1AAZAA0' },
                
                
                'eventbridge': { name: 'EventBridge', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-eventbridge-5yn7gpt4na9nq8ut175la.png/aws-eventbridge-07t2qrie8h9cc7b75iu9y9c.png?_a=DATAg1AAZAA0' },
                
                
                'timestream': { name: 'Timestream', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-timestream-4m78rbjmsdgvn9ajje4te.png/aws-timestream-r6hohld6d2kwoqu1dz5zqh.png?_a=DATAg1AAZAA0'},
                
                'iot-core': { name: 'IoT Core', icon: 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQOEvg1IKOYgigghNxZVCeQD5xTpKm4qkWDsQ&s'},
                
                'personalize': { name: 'Personalize', icon: 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTramBbAYoHDQ7d_vDIQlTMQyS4PUaUhOAxWQ&s'},
                
                'comprehend': { name: 'Comprehend', icon: 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTO_7n545oBMuOpCf3eaff0oDzbz81jalvqYA&s'},
                
                'textract': { name: 'Textract', icon: 'https://pypi-camo.freetls.fastly.net/5004099b26f8f8bf78d8303d8a925377eebca1b2/68747470733a2f2f6769746875622e636f6d2f4d616348752d4757552f6177735f74657874726163742d70726f6a6563742f6173736574732f363830303431312f61306637623063622d383234312d346463342d613933612d663062306262386362663364'},
                
                'eks': { name: 'EKS', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-eks-silujmlxqanjvxwoebv7af.png/aws-eks-aqbdhxuqgubdf7dmbpc46.png?_a=DATAg1AAZAA0'},
                
                'fargate': { name: 'Fargate', icon: 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRqmnQxIV0P_DLhMT_I5a4pSehaGpxWl0HhFQ&s'},
                
                'codepipeline': { name: 'CodePipeline', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-codepipeline-kxgnrmm9kd5ew4ehjmjxh.png/aws-codepipeline-z7s9tbutqt9qtiiu06kr7.png?_a=DATAg1AAZAA0'},
                'secrets-manager': { name: 'Secrets Manager', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-secrets-manager-svfwj1evvibd7760sbcl.png/aws-secrets-manager-7auwkav1r2c3xpvg5eoz3r.png?_a=DATAg1AAZAA0'},
                
                'cloudfront': { name: 'CloudFront', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-cloudfront-uxznobtbzzoxukzbjawan.png/aws-cloudfront-2tfhphew863aoukvktdtjp.png?_a=DATAg1AAZAA0'},
                
                'cloudwatch': { name: 'CloudWatch', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-cloudwatch-emo8muaj7q97wdnd5rko7.png/aws-cloudwatch-odt9x7dcayoh0fpefx91zm.png?_a=DATAg1AAZAA0'},
                
                'elasticache': { name: 'ElastiCache', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-elasticache-2hseki24om1t50cpvmasja.png/aws-elasticache-zixyw7qimds5m9144eza.png?_a=DATAg1AAZAA0'},
                
                'iam': { name: 'IAM', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-iam-r29jwob6ccgfn38unzs7lb.png/aws-iam-ml4zst94zbf0oxpt4vszxuc.png?_a=DATAg1AAZAA0'},
                
                'aurora': { name: 'Aurora', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-aurora-osqzsk4t1jlp408k9cv0s.png/aws-aurora-8756msl25662nc8atgguts.png?_a=DATAg1AAZAA0'},
                'route 53': { name: 'Route 53', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-route53-p3t47q0lx2lq1srdr8hx.png/aws-route53-fk4qtcwyolq0zlrfn93s8i.png?_a=DATAg1AAZAA0'},
                'glacier': { name: 'Glacier', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-glacier-b1unbbfx6jupeuzgwd5dym.png/aws-glacier-w8nkzj63voyynejbw4axi.png?_a=DATAg1AAZAA0'},
                
                'neptune': { name: 'Neptune', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-neptune-swytkotnlhg7bzpma02k3.png/aws-neptune-4kzyybyptjd3b0fys5yxrp.png?_a=DATAg1AAZAA0'},
                'keyspaces': { name: 'Keyspaces', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-keyspaces-f7hzy7q18u76p3v9r2gnn5.png/aws-keyspaces-t4m7wiusq1aklzm2qcg24i.png?_a=DATAg1AAZAA0'},

                'macie': { name: 'Macie', icon: 'https://awsvideocatalog.com/images/aws/png/PNG%20Light/Security,%20Identity,%20&%20Compliance/Amazon-Macie.png'},

                'codecommit': { name: 'Codecommit', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-codecommit-a5gis9sjuowskzjtxlhzb.png/aws-codecommit-xbid1md7f3yrgeunkvxa.png?_a=DATAg1AAZAA0'},

                'kinesis-analytics': { name: 'Kineis analytics', icon: 'https://miro.medium.com/v2/1*wZnz1aXDeMK6c1--dfgpeA.png'},

                'kms': { name: 'KMS', icon: 'https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/1/aws-kms-iafz4c6zbtlb81zkjdhwq5.png/aws-kms-8yhylffjjdihvjsi95y8de.png?_a=DATAg1AAZAA0'}
            },
            Azure: { },
            GCP: { }
        };

        const scenarios = {
            AWS: [
    {
        "id": "aws-de-001",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Projete um pipeline de ETL que replica dados continuamente de um banco de dados on-premise (simulado por RDS) para um Data Lake no S3 e, em seguida, os carrega em um Data Warehouse no Redshift para análise de BI.",
        "solution": ["rds", "dms", "s3", "redshift"],
        "availableServices": ["rds", "dms", "s3", "redshift", "lambda", "kinesis"],
        "explanation": "O AWS DMS (Database Migration Service) é ideal para replicar dados de uma fonte como RDS para um destino. O S3 serve como a camada de armazenamento do Data Lake, recebendo os dados replicados. A partir do S3, os dados podem ser carregados no Redshift usando o comando COPY para performance otimizada em análises."
    },
    {
        "id": "aws-de-002",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Como você pode ingerir um fluxo contínuo de dados de logs e entregá-los diretamente a um bucket S3 para arquivamento e análise futura, de forma totalmente gerenciada?",
        "solution": ["kinesis-firehose", "s3"],
        "availableServices": ["kinesis-firehose", "s3", "kinesis", "lambda", "ec2"],
        "explanation": "O Kinesis Data Firehose é o serviço mais simples para carregar dados de streaming em destinos como o S3. Ele é totalmente gerenciado, cuidando do buffering, compressão e entrega dos dados, exigindo configuração mínima."
    },
    {
        "id": "aws-de-003",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie uma arquitetura que colete dados de cliques de um site em tempo real, execute uma agregação baseada em janela de tempo (ex: contagem de cliques por minuto) e armazene o resultado em um banco de dados para dashboards.",
        "solution": ["kinesis", "kinesis-analytics", "lambda", "dynamodb"],
        "availableServices": ["kinesis", "kinesis-analytics", "lambda", "dynamodb", "s3", "glue"],
        "explanation": "O Kinesis Data Streams ingere o fluxo de cliques. O Kinesis Data Analytics permite executar consultas SQL contínuas sobre o stream para realizar agregações em janelas de tempo. Uma função Lambda pode ser usada como destino para receber os resultados da análise e gravá-los no DynamoDB para acesso de baixa latência por um dashboard."
    },
    {
        "id": "aws-de-004",
        "category": "cloud-operations",
        "difficulty": 5,
        "question": "Orquestre um fluxo de processamento de pedidos. Um novo pedido chega via API Gateway, é validado por uma função Lambda e depois se divide em duas ações paralelas: atualizar o inventário no DynamoDB e enviar uma notificação de confirmação via SNS. Use um orquestrador para gerenciar o fluxo.",
        "solution": ["api-gateway", {"orchestrator": "step-functions", "flow": ["lambda", [["dynamodb"], ["sns"]]]}],
        "availableServices": ["api-gateway", "step-functions", "lambda", "dynamodb", "sns", "sqs"],
        "explanation": "O API Gateway recebe o pedido. O Step Functions é o orquestrador ideal para fluxos de trabalho com múltiplos passos e lógica condicional. Ele primeiro invoca uma Lambda para validação. Em seguida, usa um estado 'Parallel' para executar as atualizações no DynamoDB e o envio de notificações SNS simultaneamente, otimizando o tempo de processamento."
    },
    {
        "id": "aws-de-005",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Construa um Data Lake simples onde arquivos CSV são depositados em um bucket S3. Use um serviço para descobrir o esquema desses dados e catalogá-los, tornando-os consultáveis via SQL padrão.",
        "solution": ["s3", "glue", "athena"],
        "availableServices": ["s3", "glue", "athena", "redshift", "emr", "lambda"],
        "explanation": "O S3 armazena os dados brutos. O AWS Glue Crawler pode varrer o S3, inferir o esquema dos arquivos CSV e criar uma tabela no Glue Data Catalog. O Athena, então, utiliza esse catálogo para permitir que os usuários consultem os dados diretamente no S3 usando SQL padrão."
    },
    {
        "id": "aws-de-006",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Desenvolva um pipeline que seja acionado por um agendamento diário. O gatilho inicia um job que processa um grande volume de dados (terabytes) usando Spark e armazena o resultado em S3. Qual serviço de agendamento e qual serviço de processamento Spark gerenciado você usaria?",
        "solution": ["eventbridge", "emr", "s3"],
        "availableServices": ["eventbridge", "emr", "s3", "lambda", "glue", "step-functions"],
        "explanation": "O Amazon EventBridge é o serviço ideal para criar regras baseadas em agendamento (schedule) para acionar outros serviços da AWS. Para processar terabytes de dados com Spark, o Amazon EMR (Elastic MapReduce) é a escolha mais robusta e escalável. O resultado do processamento é então armazenado no S3."
    },
    {
        "id": "aws-de-007",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Crie uma arquitetura serverless simples onde o upload de uma imagem em um bucket S3 aciona uma função que gera uma miniatura (thumbnail) e a salva em outro bucket S3.",
        "solution": ["s3", "lambda", "s3"],
        "availableServices": ["s3", "lambda", "ec2", "sqs", "sns"],
        "explanation": "Esta é uma arquitetura clássica orientada a eventos. O S3 pode ser configurado para enviar um evento de 'ObjectCreated'. Esse evento aciona uma função Lambda, que contém a lógica para redimensionar a imagem e salvar o resultado em um bucket S3 de destino."
    },
    {
        "id": "aws-de-008",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete uma solução para ingerir dados de telemetria de dispositivos IoT, armazenando os dados brutos em S3, os dados de série temporal em um banco de dados otimizado e, ao mesmo tempo, enviando alertas via SNS se uma métrica específica exceder um limite.",
        "solution": ["iot-core", "kinesis", [["kinesis-firehose", "s3"], ["lambda", "timestream"], ["lambda", "sns"]]],
        "availableServices": ["iot-core", "kinesis", "kinesis-firehose", "s3", "lambda", "timestream", "sns", "dynamodb"],
        "explanation": "O IoT Core recebe os dados. Eles são enviados para um Kinesis Data Stream, que atua como um distribuidor. A partir do stream: 1) Um Kinesis Firehose consome os dados e os arquiva no S3. 2) Uma função Lambda processa os dados e os insere no Timestream, o banco de dados otimizado para séries temporais. 3) Outra função Lambda analisa os dados em busca de anomalias e envia um alerta via SNS se necessário."
    },
    {
        "id": "aws-de-009",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Construa uma arquitetura de API desacoplada. Um serviço de API recebe requisições, as coloca em uma fila para garantir durabilidade e processamento assíncrono, e uma frota de contêineres processa as mensagens da fila, salvando o resultado em um banco de dados relacional.",
        "solution": ["api-gateway", "sqs", "ecs", "rds"],
        "availableServices": ["api-gateway", "sqs", "ecs", "rds", "lambda", "sns", "ec2"],
        "explanation": "O API Gateway serve como front-end. O SQS (Simple Queue Service) desacopla a API dos workers, absorvendo picos de tráfego e garantindo que nenhuma requisição seja perdida. O ECS (Elastic Container Service) gerencia a execução dos contêineres que consomem as mensagens da fila. O resultado é persistido em um banco de dados RDS."
    },
    {
        "id": "aws-de-010",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Você precisa disponibilizar dashboards interativos para analistas de negócios sobre dados armazenados em seu Data Warehouse no Redshift. Qual serviço de BI da AWS se integra nativamente para essa finalidade?",
        "solution": ["redshift", "quicksight"],
        "availableServices": ["redshift", "quicksight", "athena", "s3", "emr"],
        "explanation": "O Amazon QuickSight é o serviço de Business Intelligence da AWS. Ele se integra nativamente com várias fontes de dados da AWS, incluindo o Redshift, permitindo a criação rápida de dashboards e análises visuais."
    },
    {
        "id": "aws-de-011",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie um pipeline de dados que orquestra jobs de ETL usando Airflow. O serviço gerenciado de Airflow deve executar um script que transforma dados no S3 e os carrega no Redshift. Como seria essa arquitetura?",
        "solution": [{"orchestrator": "mwaa", "flow": ["s3", "redshift"]}],
        "availableServices": ["mwaa", "s3", "redshift", "step-functions", "glue", "ec2"],
        "explanation": "O MWAA (Managed Workflows for Apache Airflow) é o serviço gerenciado da AWS para Airflow. Uma DAG no MWAA pode ser configurada para executar operadores que interagem com outros serviços, como processar arquivos no S3 (usando um pod do EKS ou um job do Glue invocado pelo Airflow) e carregar os dados transformados no Redshift."
    },
    {
        "id": "aws-de-012",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Desenvolva uma arquitetura de Data Lakehouse governada. Os dados brutos chegam ao S3. Um serviço centralizado deve gerenciar as permissões de acesso a tabelas e colunas para os dados no S3. Os dados devem ser transformados por um job Spark (Glue) e consultados pelo Athena.",
        "solution": ["s3", "lake-formation", "glue", "athena"],
        "availableServices": ["s3", "lake-formation", "glue", "athena", "iam", "macie"],
        "explanation": "O S3 armazena os dados. O AWS Lake Formation atua como uma camada de governança centralizada sobre o S3, permitindo definir permissões granulares (nível de tabela, coluna e célula) para diferentes usuários e serviços. O Glue executa a transformação dos dados e o Athena consulta os dados, ambos respeitando as permissões definidas no Lake Formation."
    },
    {
        "id": "aws-de-013",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Implemente um padrão de fan-out. Uma mensagem sobre um novo cliente é publicada em um tópico. Múltiplos sistemas precisam reagir a esse evento: um para enviar um e-mail de boas-vindas, outro para atualizar um sistema de CRM e um terceiro para arquivar a notificação.",
        "solution": ["sns", [["sqs", "lambda"], ["sqs", "lambda"], ["sqs", "lambda"]]],
        "availableServices": ["sns", "sqs", "lambda", "kinesis", "eventbridge"],
        "explanation": "O SNS (Simple Notification Service) é ideal para o padrão publish/subscribe (fan-out). A mensagem é publicada em um tópico SNS. Três filas SQS diferentes são inscritas nesse tópico, cada uma recebendo uma cópia da mensagem. Cada fila SQS, por sua vez, aciona uma função Lambda específica para realizar sua tarefa (e-mail, CRM, arquivamento), garantindo processamento desacoplado e resiliente."
    },
    {
        "id": "aws-de-014",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Você precisa migrar um cluster Kafka autogerenciado para a AWS. Qual serviço gerenciado você usaria? E como você consumiria as mensagens desse cluster para processá-las e salvá-las no S3?",
        "solution": ["msk", "lambda", "s3"],
        "availableServices": ["msk", "lambda", "s3", "kinesis", "sqs", "ec2"],
        "explanation": "O Amazon MSK (Managed Streaming for Kafka) é o serviço gerenciado da AWS para Apache Kafka. Para consumir as mensagens, você pode usar uma função Lambda como um consumidor serverless, que pode ser acionada por eventos do MSK. A função processa os lotes de mensagens e os escreve no S3 para armazenamento e análise posterior."
    },
    {
        "id": "aws-de-015",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Crie uma arquitetura para análise de logs de aplicação. Os logs são enviados para um serviço que permite indexação e busca em tempo real, além da criação de dashboards de monitoramento.",
        "solution": ["opensearch"],
        "availableServices": ["opensearch", "redshift", "athena", "dynamodb", "cloudwatch"],
        "explanation": "O Amazon OpenSearch Service (sucessor do Elasticsearch Service) é projetado especificamente para casos de uso de busca e análise de logs. Ele indexa os dados recebidos, permitindo pesquisas de texto completo de baixa latência e a criação de visualizações e dashboards com o OpenSearch Dashboards."
    },
    {
        "id": "aws-de-016",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete um pipeline de Machine Learning. Um job do Glue pré-processa os dados no S3. Em seguida, um fluxo orquestrado treina um modelo e, se o treinamento for bem-sucedido, o implanta como um endpoint para inferência. Notifique o resultado final.",
        "solution": ["s3", "glue", {"orchestrator": "step-functions", "flow": ["sagemaker", "sns"]}],
        "availableServices": ["s3", "glue", "step-functions", "sagemaker", "sns", "lambda"],
        "explanation": "O Glue realiza o pré-processamento (feature engineering). O Step Functions orquestra o pipeline de MLOps. Ele invoca uma tarefa de treinamento do SageMaker. Com base no resultado, ele pode usar lógica condicional para invocar outra tarefa do SageMaker para criar um endpoint. Por fim, ele envia uma notificação via SNS sobre o sucesso ou falha do pipeline."
    },
    {
        "id": "aws-de-017",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Como você armazenaria dados de configuração de aplicação ou metadados que exigem acesso rápido de chave-valor e alta escalabilidade?",
        "solution": ["dynamodb"],
        "availableServices": ["dynamodb", "rds", "redshift", "s3"],
        "explanation": "O Amazon DynamoDB é um banco de dados NoSQL de chave-valor totalmente gerenciado que oferece desempenho de milissegundos de um dígito em qualquer escala. É a escolha perfeita para casos de uso que exigem acesso rápido a itens individuais por meio de uma chave primária."
    },
    {
        "id": "aws-de-018",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um fluxo de dados em tempo real precisa ser entregue a dois destinos: um cluster Redshift para análise e um bucket S3 para backup, com transformação de dados em trânsito. Qual serviço pode capturar, transformar e entregar para múltiplos destinos de forma gerenciada?",
        "solution": ["kinesis", "kinesis-firehose", [["redshift"], ["s3"]]],
        "availableServices": ["kinesis", "kinesis-firehose", "redshift", "s3", "lambda", "glue"],
        "explanation": "O Kinesis Data Streams ingere os dados. Ele pode ter múltiplos consumidores. Um Kinesis Data Firehose pode consumir do stream, usar uma função Lambda integrada para realizar transformações em tempo real e, em seguida, entregar os dados transformados para múltiplos destinos, como Redshift e S3, simultaneamente."
    },
    {
        "id": "aws-de-019",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Você precisa implantar toda a infraestrutura de um pipeline de dados (S3, Lambda, DynamoDB) de forma automatizada e repetível. Qual serviço da AWS você usaria para definir sua infraestrutura como código (IaC)?",
        "solution": ["cloudformation"],
        "availableServices": ["cloudformation", "ec2", "iam", "s3", "lambda", "dynamodb"],
        "explanation": "O AWS CloudFormation permite que você modele e configure seus recursos da AWS em arquivos de texto (YAML ou JSON). Ele provisiona e gerencia a 'pilha' (stack) de recursos de forma ordenada e previsível, tornando-o a principal ferramenta de IaC da AWS."
    },
    {
        "id": "aws-de-020",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie um sistema que extrai texto de documentos PDF e imagens enviados para o S3 e, em seguida, analisa o texto extraído para identificar entidades (como nomes de pessoas e lugares), armazenando os resultados em um banco de dados pesquisável.",
        "solution": ["s3", "lambda", "textract", "comprehend", "opensearch"],
        "availableServices": ["s3", "lambda", "textract", "comprehend", "opensearch", "dynamodb"],
        "explanation": "Um evento S3 aciona uma Lambda. A Lambda orquestra o processo: primeiro, chama o Amazon Textract para extrair o texto do documento. Em seguida, passa o texto extraído para o Amazon Comprehend para realizar o reconhecimento de entidades nomeadas (NER). Por fim, a Lambda armazena o texto e as entidades extraídas no Amazon OpenSearch para indexação e pesquisa."
    },
    {
        "id": "aws-de-021",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Como você pode acelerar o desempenho de um banco de dados relacional (RDS) para leituras frequentes de dados comuns, como perfis de usuário ou catálogos de produtos?",
        "solution": ["rds", "elasticache"],
        "availableServices": ["rds", "elasticache", "dynamodb", "s3"],
        "explanation": "O Amazon ElastiCache (com Redis ou Memcached) fornece um serviço de cache em memória. Ao colocar um cache ElastiCache na frente do RDS, as leituras frequentes podem ser atendidas a partir da memória de alta velocidade, reduzindo a latência e a carga sobre o banco de dados principal."
    },
    {
        "id": "aws-de-022",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Projete uma arquitetura para executar um contêiner Docker que processa arquivos em S3 sob demanda, sem gerenciar servidores ou clusters. A execução deve ser acionada por uma mensagem em uma fila SQS.",
        "solution": ["sqs", "fargate", "s3"],
        "availableServices": ["sqs", "fargate", "s3", "ec2", "ecs", "eks"],
        "explanation": "O SQS desacopla o gatilho da execução. O AWS Fargate é um mecanismo de computação serverless para contêineres que funciona com o Amazon ECS. Você pode configurar um serviço ECS usando Fargate para escalar automaticamente o número de tarefas (contêineres) com base no número de mensagens na fila SQS, processando os arquivos do S3 de forma eficiente e sem gerenciamento de infraestrutura."
    },
    {
        "id": "aws-de-023",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Construa um pipeline de CI/CD para um job de ETL do AWS Glue. O código do job está em um repositório. O pipeline deve ser acionado por um commit, construir o pacote e implantar a nova versão do job.",
        "solution": ["codecommit", "codepipeline", "glue"],
        "availableServices": ["codecommit", "codepipeline", "glue", "s3", "cloudformation"],
        "explanation": "O AWS CodeCommit hospeda o código-fonte do job. Um commit no repositório aciona o AWS CodePipeline. O pipeline pode ter um estágio de 'Build' (usando o AWS CodeBuild, não listado, mas implícito no pipeline) para empacotar as dependências e o script, e um estágio de 'Deploy' que usa o CloudFormation ou a CLI da AWS para atualizar o job do AWS Glue existente com a nova versão do código."
    },
    {
        "id": "aws-de-024",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Você precisa armazenar as senhas e chaves de API de um banco de dados de forma segura, para que suas funções Lambda possam acessá-las sem expô-las no código. Qual serviço é projetado para isso?",
        "solution": ["secrets-manager"],
        "availableServices": ["secrets-manager", "iam", "kms", "s3"],
        "explanation": "O AWS Secrets Manager é o serviço ideal para gerenciar, rotacionar e recuperar segredos (como credenciais de banco de dados, chaves de API) ao longo de seu ciclo de vida. As aplicações, como as funções Lambda, podem recuperar os segredos programaticamente por meio de uma API, evitando a necessidade de codificá-los."
    },
    {
        "id": "aws-de-025",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Como você serviria o conteúdo de um Data Lake no S3 para usuários em todo o mundo com baixa latência e alta velocidade de transferência?",
        "solution": ["s3", "cloudfront"],
        "availableServices": ["s3", "cloudfront", "ec2", "route 53"],
        "explanation": "O Amazon CloudFront é uma rede de entrega de conteúdo (CDN) global. Ao configurar o CloudFront com uma origem S3, o conteúdo do bucket é armazenado em cache em 'edge locations' ao redor do mundo. Quando um usuário solicita o conteúdo, ele é entregue a partir da edge location mais próxima, resultando em latência significativamente menor."
    },
    {
        "id": "aws-de-026",
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Você precisa monitorar a utilização da CPU de uma instância EC2 e receber um alerta por e-mail quando ela ultrapassar 80%. Como você configuraria isso?",
        "solution": ["ec2", "cloudwatch", "sns"],
        "availableServices": ["ec2", "cloudwatch", "sns", "lambda", "eventbridge"],
        "explanation": "O Amazon CloudWatch coleta métricas de recursos da AWS, incluindo a utilização da CPU de instâncias EC2. Você pode criar um Alarme do CloudWatch que monitora essa métrica. Na configuração do alarme, você define a condição (CPU > 80%) e especifica uma Ação, que seria publicar uma mensagem em um tópico SNS. Uma assinatura de e-mail nesse tópico garantirá que o alerta seja recebido."
    },
    {
        "id": "aws-de-027",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Projete uma arquitetura de banco de dados relacional com alta disponibilidade e recuperação de desastres. O banco de dados deve ter réplicas de leitura para escalar as consultas.",
        "solution": ["aurora"],
        "availableServices": ["aurora", "rds", "ec2", "dms"],
        "explanation": "O Amazon Aurora é um banco de dados relacional compatível com MySQL e PostgreSQL, projetado para a nuvem. Ele oferece alta disponibilidade por padrão, replicando os dados em 3 Zonas de Disponibilidade. Ele também suporta até 15 réplicas de leitura de baixa latência que compartilham o mesmo armazenamento subjacente, tornando a escalabilidade de leitura muito eficiente."
    },
    {
        "id": "aws-de-028",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Qual serviço você usaria para arquivamento de dados a longo prazo e de baixo custo, onde o tempo de recuperação de algumas horas é aceitável?",
        "solution": ["s3", "glacier"],
        "availableServices": ["s3", "glacier", "ebs", "efs"],
        "explanation": "O Amazon S3 Glacier é um serviço de armazenamento seguro, durável e de custo extremamente baixo para arquivamento de dados. Ele é ideal para backups e arquivos mortos. Você pode usar as políticas de ciclo de vida do S3 para mover objetos automaticamente para as classes de armazenamento do Glacier após um certo período."
    },
    {
        "id": "aws-de-029",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Você precisa modelar e consultar dados altamente conectados, como redes sociais ou grafos de conhecimento. Qual banco de dados gerenciado da AWS é otimizado para esse tipo de dado?",
        "solution": ["neptune"],
        "availableServices": ["neptune", "dynamodb", "aurora", "redshift"],
        "explanation": "O Amazon Neptune é um serviço de banco de dados de grafos rápido, confiável e totalmente gerenciado. Ele facilita a criação e a execução de aplicações que trabalham com conjuntos de dados altamente conectados, sendo otimizado para consultas complexas de grafos."
    },
    {
        "id": "aws-de-030",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Migre uma aplicação que usa Apache Cassandra on-premise para um serviço compatível, serverless e gerenciado na AWS. Qual serviço você escolheria?",
        "solution": ["keyspaces"],
        "availableServices": ["keyspaces", "dynamodb", "rds", "neptune"],
        "explanation": "O Amazon Keyspaces (for Apache Cassandra) é um serviço de banco de dados compatível com Apache Cassandra, escalável, de alta disponibilidade e gerenciado. Por ser serverless, ele dimensiona tabelas automaticamente para cima e para baixo em resposta ao tráfego da aplicação, e você paga apenas pelos recursos que utiliza."
    },
    {
        "id": "aws-de-031",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Implemente um sistema para descobrir e proteger dados sensíveis (como informações de identificação pessoal - PII) armazenados em seu Data Lake no S3. O sistema deve classificar os dados e alertar sobre políticas de segurança.",
        "solution": ["s3", "macie"],
        "availableServices": ["s3", "macie", "glue", "lake-formation", "kms"],
        "explanation": "O Amazon Macie é um serviço de segurança e privacidade de dados que usa machine learning para descobrir, classificar e proteger automaticamente dados confidenciais na AWS. Ao habilitar o Macie para seus buckets S3, ele pode identificar PII, dados financeiros e outras informações sensíveis, fornecendo dashboards e alertas sobre a segurança dos seus dados."
    },
    {
        "id": "aws-de-032",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Qual serviço da AWS você usaria para criptografar os dados em repouso em um bucket S3, garantindo que você tenha controle sobre as chaves de criptografia?",
        "solution": ["s3", "kms"],
        "availableServices": ["s3", "kms", "iam", "secrets-manager"],
        "explanation": "O AWS Key Management Service (KMS) permite criar e gerenciar chaves criptográficas e controlar seu uso em uma ampla gama de serviços da AWS. Ao configurar a criptografia do lado do servidor (SSE) em um bucket S3, você pode especificar uma chave gerenciada pelo cliente (CMK) do KMS, dando a você controle total sobre a chave usada para criptografar e descriptografar seus objetos."
    },
    {
        "id": "aws-de-033",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie uma arquitetura que receba eventos de mudança de múltiplos microsserviços e os direcione para diferentes alvos (Lambdas, filas SQS) com base no conteúdo do evento, usando um barramento de eventos centralizado.",
        "solution": ["eventbridge", [["lambda"], ["sqs"]]],
        "availableServices": ["eventbridge", "lambda", "sqs", "sns", "kinesis"],
        "explanation": "O Amazon EventBridge é um barramento de eventos serverless que facilita a conexão de aplicações com dados de uma variedade de fontes. Você pode criar regras no barramento de eventos que filtram os eventos com base em seu conteúdo (o payload do evento) e os encaminham para alvos específicos, como uma função Lambda para processamento imediato ou uma fila SQS para processamento em lote."
    },
    {
        "id": "aws-de-034",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Desenvolva uma solução de recomendação de produtos para um site de e-commerce. O serviço deve ser treinado com dados de interação do usuário e fornecer recomendações em tempo real via API.",
        "solution": ["s3", "personalize", "api-gateway", "lambda"],
        "availableServices": ["s3", "personalize", "sagemaker", "api-gateway", "lambda"],
        "explanation": "O Amazon Personalize é um serviço de machine learning totalmente gerenciado que facilita a criação de recomendações personalizadas para os clientes. Você fornece os dados de atividade (do S3), e o Personalize processa os dados, identifica o que é significativo, treina e otimiza um modelo de personalização. O modelo treinado pode ser exposto via uma API, que pode ser chamada por uma Lambda acionada pelo API Gateway para fornecer recomendações em tempo real."
    },
    {
        "id": "aws-de-035",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete uma arquitetura para executar um cluster Kubernetes para processamento de dados em contêineres, sem gerenciar o plano de controle (control plane) do Kubernetes. O processamento deve ser acionado por um evento.",
        "solution": ["eventbridge", "eks", "fargate"],
        "availableServices": ["eventbridge", "eks", "fargate", "ec2", "ecs"],
        "explanation": "O Amazon EKS (Elastic Kubernetes Service) é o serviço gerenciado de Kubernetes da AWS. Ele gerencia o control plane para você. Para uma abordagem serverless, você pode usar o AWS Fargate com o EKS para executar os pods do Kubernetes sem precisar provisionar e gerenciar servidores (nós de trabalho). Um evento no EventBridge pode acionar um mecanismo (como uma Lambda) que cria um Kubernetes Job no cluster EKS/Fargate para executar o processamento."
    },
    {
        "id": "aws-de-036",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um aplicativo precisa de um banco de dados relacional que seja totalmente gerenciado pela AWS, cuidando de tarefas como provisionamento, patches e backups. Qual é o serviço fundamental para isso?",
        "solution": ["rds"],
        "availableServices": ["rds", "ec2", "aurora", "dynamodb"],
        "explanation": "O Amazon RDS (Relational Database Service) é um serviço gerenciado que facilita a configuração, operação e escalabilidade de bancos de dados relacionais na nuvem. Ele automatiza tarefas de administração demoradas, permitindo que você se concentre em suas aplicações e dados."
    },
    {
        "id": "aws-de-037",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie um sistema que permita aos usuários fazerem login e se registrarem em sua aplicação de análise de dados, gerenciando identidades e federação com provedores sociais. O acesso aos recursos da AWS, como dashboards do QuickSight, deve ser controlado por meio dessa autenticação.",
        "solution": ["cognito", "quicksight"],
        "availableServices": ["cognito", "quicksight", "iam", "lambda"],
        "explanation": "O Amazon Cognito fornece autenticação, autorização e gerenciamento de usuários para suas aplicações web e móveis. Ele permite que os usuários façam login com nome de usuário e senha, ou por meio de um terceiro, como Facebook ou Google. Após a autenticação, o Cognito pode fornecer credenciais temporárias da AWS para acessar outros serviços, como o QuickSight, de forma segura."
    },
    {
        "id": "aws-de-038",
        "category": "cloud-operations",
        "difficulty": 2,
        "question": "Qual serviço da AWS atua como um DNS (Domain Name System) para traduzir nomes de domínio legíveis por humanos (como www.exemplo.com) em endereços IP?",
        "solution": ["route 53"],
        "availableServices": ["route 53", "cloudfront", "ec2", "api-gateway"],
        "explanation": "O Amazon Route 53 é um serviço web de DNS na nuvem, altamente disponível e escalável. Ele foi projetado para oferecer aos desenvolvedores e empresas uma maneira extremamente confiável e econômica de direcionar os usuários finais para aplicações na Internet."
    },
    {
        "id": "aws-de-039",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Você tem um pipeline que processa dados e os armazena em um bucket S3. Como você pode garantir que apenas usuários e roles específicos do IAM tenham permissão para ler ou escrever nesse bucket?",
        "solution": ["s3", "iam"],
        "availableServices": ["s3", "iam", "kms", "lake-formation"],
        "explanation": "O AWS IAM (Identity and Access Management) é o serviço usado para gerenciar o acesso aos serviços e recursos da AWS com segurança. Você pode criar políticas do IAM que definem permissões e anexá-las a usuários, grupos ou roles. Além disso, você pode usar Bucket Policies (um tipo de política baseada em recursos do IAM) diretamente no bucket S3 para controlar o acesso."
    },
    {
        "id": "aws-de-040",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Orquestre um processo de ETL complexo com dependências: um job A (Glue) deve ser executado primeiro. Se for bem-sucedido, os jobs B (Glue) e C (Lambda) devem ser executados em paralelo. Se A falhar, uma notificação de erro deve ser enviada.",
        "solution": [{"orchestrator": "step-functions", "flow": ["glue", [["glue"], ["lambda"]], "sns"]}],
        "availableServices": ["step-functions", "glue", "lambda", "sns", "mwaa"],
        "explanation": "O AWS Step Functions é perfeito para orquestrar fluxos de trabalho com lógica condicional e paralelismo. Você pode definir um fluxo onde o primeiro estado executa o job A. Usando um estado 'Choice', você pode direcionar o fluxo para um estado 'Parallel' (executando B e C) em caso de sucesso, ou para um estado de notificação SNS em caso de falha, utilizando os recursos de tratamento de erros do Step Functions."
    },
    {
        "id": "aws-de-041",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Projete uma arquitetura para coletar logs de várias instâncias EC2, centralizá-los e permitir consultas SQL para análise de padrões e erros.",
        "solution": ["ec2", "kinesis-firehose", "s3", "athena"],
        "availableServices": ["ec2", "kinesis-firehose", "s3", "athena", "cloudwatch", "glue"],
        "explanation": "Você pode instalar o Agente do Kinesis nas instâncias EC2 para enviar os logs para um stream do Kinesis Data Firehose. O Firehose pode agregar os logs e entregá-los a um bucket S3 em um formato colunar (como Parquet) para otimização de custos e performance. Com os dados no S3, o Athena pode ser usado para executar consultas SQL ad-hoc diretamente nos arquivos de log."
    },
    {
        "id": "aws-de-042",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um job de ETL que roda em uma função Lambda precisa acessar credenciais de um banco de dados Redshift. Qual a forma mais segura de prover essas credenciais para a função?",
        "solution": ["lambda", "secrets-manager", "redshift"],
        "availableServices": ["lambda", "secrets-manager", "redshift", "iam", "kms"],
        "explanation": "A melhor prática é armazenar as credenciais do Redshift no AWS Secrets Manager. A função Lambda receberia uma role do IAM com permissão para ler o segredo específico. Dentro do código da função, você usaria o SDK da AWS para buscar as credenciais do Secrets Manager em tempo de execução, evitando que elas sejam expostas no código ou em variáveis de ambiente."
    },
    {
        "id": "aws-de-043",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie um pipeline que ingere dados de streaming, aplica uma transformação simples (ex: filtragem de campos) e os armazena em um banco de dados NoSQL para acesso rápido. Toda a arquitetura deve ser serverless.",
        "solution": ["kinesis", "lambda", "dynamodb"],
        "availableServices": ["kinesis", "lambda", "dynamodb", "ec2", "rds"],
        "explanation": "Esta é uma arquitetura serverless de streaming clássica. O Kinesis Data Streams ingere os dados. Uma função Lambda é configurada como consumidor do stream, sendo invocada automaticamente com lotes de registros. A função executa a lógica de transformação e, em seguida, grava os resultados no DynamoDB, que fornece o armazenamento NoSQL serverless e de baixa latência."
    },
    {
        "id": "aws-de-044",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Você precisa replicar uma tabela de um banco de dados Aurora em uma região para outra para fins de recuperação de desastres e, ao mesmo tempo, alimentar um Data Lake (S3) na segunda região.",
        "solution": ["aurora", "dms", [["aurora"], ["s3"]]],
        "availableServices": ["aurora", "dms", "s3", "lambda", "redshift"],
        "explanation": "O AWS DMS pode ser configurado para replicação contínua (Change Data Capture - CDC). Você pode configurar uma tarefa de replicação do DMS com a instância Aurora na região de origem como fonte. O DMS pode ter múltiplos endpoints de destino. Um endpoint seria a instância Aurora na região de destino, e o outro seria um bucket S3 na mesma região de destino, alcançando ambos os objetivos com uma única ferramenta."
    },
    {
        "id": "aws-de-045",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Como você pode automatizar a descoberta de esquema e o versionamento de tabelas para dados que chegam continuamente a um bucket S3?",
        "solution": ["s3", "glue"],
        "availableServices": ["s3", "glue", "lambda", "athena"],
        "explanation": "O AWS Glue Crawler pode ser configurado para rodar de forma agendada. A cada execução, ele varre as novas partições ou arquivos no S3, detecta mudanças de esquema e atualiza a definição da tabela no Glue Data Catalog, versionando o esquema. Isso garante que ferramentas como o Athena sempre consultem a estrutura de dados mais recente."
    },
    {
        "id": "aws-de-046",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Construa uma arquitetura que sirva um modelo de machine learning (pré-treinado) através de um endpoint HTTP. A infraestrutura para hospedar o modelo deve ser totalmente gerenciada.",
        "solution": ["sagemaker", "api-gateway", "lambda"],
        "availableServices": ["sagemaker", "api-gateway", "lambda", "ec2", "ecs"],
        "explanation": "Você pode implantar o modelo treinado usando o Amazon SageMaker para criar um endpoint de inferência gerenciado. Para expor este endpoint de forma segura e escalável, você pode usar o API Gateway, que recebe as requisições HTTP. Uma função Lambda pode atuar como uma camada de integração entre o API Gateway e o endpoint do SageMaker, tratando da formatação da requisição e da resposta."
    },
    {
        "id": "aws-de-047",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Você precisa executar um script de transformação de dados que leva 20 minutos para rodar, uma vez por dia. Uma instância EC2 seria um desperdício de recursos. Qual serviço de computação serverless é ideal para tarefas de longa duração (até 15 minutos)?",
        "solution": ["lambda"],
        "availableServices": ["lambda", "ec2", "fargate", "step-functions"],
        "explanation": "O AWS Lambda suporta tempos de execução de até 15 minutos, o que o torna uma opção viável para muitas tarefas de ETL e processamento de dados que não justificam uma infraestrutura permanentemente ativa. Para execuções mais longas, outras opções como Fargate ou Glue seriam mais adequadas."
    },
    {
        "id": "aws-de-048",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete uma solução para análise de sentimento em tempo real de tweets sobre sua marca. Os tweets são ingeridos, seu sentimento é analisado por um serviço de IA, e os resultados são visualizados em um dashboard.",
        "solution": ["kinesis", "lambda", "comprehend", "opensearch", "quicksight"],
        "availableServices": ["kinesis", "lambda", "comprehend", "opensearch", "quicksight", "s3"],
        "explanation": "Uma aplicação (não inclusa) enviaria os tweets para um Kinesis Data Stream. Uma função Lambda consome o stream, envia o texto de cada tweet para o Amazon Comprehend para análise de sentimento. A Lambda então armazena o tweet e o resultado do sentimento (positivo, negativo, neutro) no Amazon OpenSearch para indexação. Finalmente, o QuickSight pode se conectar ao OpenSearch para criar dashboards e visualizar os dados em tempo real."
    },
    {
        "id": "aws-de-049",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Como você implementaria um pipeline simples de extração, carregamento e transformação (ELT)? Os dados são primeiro carregados de uma fonte para o S3 e depois transformados usando SQL diretamente no data warehouse.",
        "solution": ["dms", "s3", "redshift"],
        "availableServices": ["dms", "s3", "redshift", "glue", "lambda"],
        "explanation": "No padrão ELT, a transformação ocorre no destino. O DMS (ou outra ferramenta de ingestão) extrai e carrega os dados brutos no S3 (Extract, Load). Em seguida, o comando COPY do Redshift carrega os dados do S3 para tabelas de staging. A transformação (Transform) é então realizada usando procedimentos armazenados ou scripts SQL que rodam diretamente no Redshift, movendo os dados das tabelas de staging para as tabelas finais."
    },
    {
        "id": "aws-de-050",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Orquestre um pipeline de dados usando uma ferramenta visual de arrastar e soltar, que gerencia a execução de jobs, gatilhos e tratamento de erros. O pipeline move dados do S3 para o Redshift.",
        "solution": [{"orchestrator": "glue", "flow": ["s3", "redshift"]}],
        "availableServices": ["glue", "s3", "redshift", "step-functions", "lambda"],
        "explanation": "O AWS Glue Studio oferece uma interface gráfica de arrastar e soltar (drag-and-drop) para criar, executar e monitorar jobs de ETL. Você pode visualmente definir uma fonte (S3), aplicar transformações e especificar um destino (Redshift), e o Glue Studio gera o código Spark correspondente e orquestra a execução do job."
    },
    {
        "id": "aws-de-051",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Armazene dados de eventos de IoT com carimbos de data/hora (timestamps) para análises de tendências ao longo do tempo. Qual banco de dados da AWS é especificamente otimizado para esse tipo de dado de série temporal?",
        "solution": ["timestream"],
        "availableServices": ["timestream", "dynamodb", "rds", "redshift"],
        "explanation": "O Amazon Timestream é um serviço de banco de dados de séries temporais rápido, escalável e serverless para aplicações de IoT e operacionais. Ele facilita o armazenamento e a análise de trilhões de eventos por dia, com custo até 1.000 vezes menor do que o de bancos de dados relacionais."
    },
    {
        "id": "aws-de-052",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um processo batch precisa ser executado em um contêiner. A execução é esporádica e você quer pagar apenas pelo tempo de execução do contêiner, sem se preocupar com a infraestrutura subjacente. O contêiner precisa de 8GB de RAM.",
        "solution": ["fargate"],
        "availableServices": ["fargate", "lambda", "ec2", "ecs"],
        "explanation": "O AWS Fargate é a escolha ideal. O Lambda tem um limite de memória (atualmente 10GB, mas Fargate é mais flexível para cargas maiores) e é mais focado em funções, enquanto o Fargate é projetado para executar contêineres de forma serverless. Você define os requisitos de CPU e memória da sua tarefa e o Fargate a executa, cobrando apenas pelos recursos consumidos durante a execução."
    },
    {
        "id": "aws-de-053",
        "category": "cloud-operations",
        "difficulty": 5,
        "question": "Crie um fluxo de trabalho que, após o upload de um vídeo no S3, inicie transcodificações para diferentes resoluções em paralelo. Quando todas terminarem, atualize um registro no DynamoDB com os links para os novos arquivos e envie uma notificação. O fluxo deve ser resiliente a falhas em uma das transcodificações.",
        "solution": ["s3", {"orchestrator": "step-functions", "flow": [[["lambda"], ["lambda"], ["lambda"]], "dynamodb", "sns"]}],
        "availableServices": ["s3", "step-functions", "lambda", "dynamodb", "sns", "sqs"],
        "explanation": "O S3 aciona o Step Functions. Um estado 'Parallel' no Step Functions invoca múltiplas instâncias de uma função Lambda (ou diferentes funções) para cada resolução de transcodificação. O Step Functions aguarda a conclusão de todos os ramos paralelos. Com seu tratamento de erros integrado, ele pode capturar falhas. Se todos forem bem-sucedidos, ele prossegue para atualizar o DynamoDB e enviar a notificação via SNS."
    },
    {
        "id": "aws-de-054",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Você precisa consultar dados que estão em um banco de dados Aurora e, ao mesmo tempo, dados que estão em um bucket S3, em uma única consulta SQL. Qual serviço permite essa consulta federada?",
        "solution": ["athena"],
        "availableServices": ["athena", "aurora", "s3", "redshift", "glue"],
        "explanation": "O Amazon Athena suporta consultas federadas, permitindo que você execute consultas SQL em dados armazenados em fontes de dados relacionais, não relacionais, de objeto e personalizadas. Usando conectores de fonte de dados, o Athena pode se conectar ao Aurora e ao S3 (via Glue Catalog) e executar uma única consulta que une os dados de ambas as fontes."
    },
    {
        "id": "aws-de-055",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Como você pode carregar um stream de dados JSON no Redshift em tempo real, realizando uma conversão para um formato colunar (como Parquet) antes do carregamento para otimizar o armazenamento e as consultas?",
        "solution": ["kinesis", "kinesis-firehose", "s3", "redshift"],
        "availableServices": ["kinesis", "kinesis-firehose", "s3", "redshift", "lambda"],
        "explanation": "O Kinesis Data Streams ingere os dados JSON. Um Kinesis Data Firehose pode ser configurado para consumir desse stream. O Firehose tem um recurso integrado de conversão de formato de registro, que pode transformar os dados JSON de entrada para Parquet. Ele então entrega os arquivos Parquet convertidos para um bucket S3. A partir daí, o comando COPY do Redshift pode carregar os dados de forma altamente eficiente a partir do S3."
    },
    {
        "id": "aws-de-056",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete um pipeline de dados que use Apache Spark em um ambiente serverless para transformar dados. A orquestração do pipeline é feita com base em eventos e deve seguir uma lógica de passos sequenciais.",
        "solution": ["eventbridge", {"orchestrator": "step-functions", "flow": ["glue", "s3"]}],
        "availableServices": ["eventbridge", "step-functions", "glue", "s3", "emr", "lambda"],
        "explanation": "O EventBridge captura o evento inicial e aciona a máquina de estados do Step Functions. O Step Functions orquestra o fluxo. Para executar Spark de forma serverless, o AWS Glue é a escolha ideal. O Step Functions inicia o job do Glue. Após a conclusão bem-sucedida do job, o Step Functions pode prosseguir para a próxima etapa, como mover os dados processados para um local final no S3 ou acionar outra ação."
    },
    {
        "id": "aws-de-057",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Sua aplicação está crescendo e o banco de dados RDS está se tornando um gargalo de leitura. Como você pode escalar a capacidade de leitura do banco de dados sem alterar a instância principal?",
        "solution": ["rds"],
        "availableServices": ["rds", "elasticache", "dms", "aurora"],
        "explanation": "O Amazon RDS permite criar facilmente uma ou mais 'Réplicas de Leitura' (Read Replicas) de sua instância de banco de dados principal. As réplicas de leitura são cópias assíncronas da instância principal que podem ser usadas para descarregar as consultas de leitura, liberando a instância principal para lidar com as operações de escrita."
    },
    {
        "id": "aws-de-058",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie uma arquitetura para processar arquivos grandes (ex: 50 GB) que são enviados para o S3. Uma função Lambda não é adequada devido aos limites de tempo de execução e armazenamento. O processamento deve ser em contêineres e serverless.",
        "solution": ["s3", "sqs", "fargate"],
        "availableServices": ["s3", "sqs", "fargate", "lambda", "ecs"],
        "explanation": "O upload no S3 pode acionar um evento que envia uma mensagem para uma fila SQS com os detalhes do arquivo. Um serviço ECS configurado com o tipo de inicialização Fargate pode ser configurado para escalar com base no número de mensagens na fila. Uma tarefa do Fargate (contêiner) pega a mensagem, baixa o arquivo do S3, processa-o e, em seguida, exclui a mensagem da fila. Isso lida com arquivos grandes e processamento de longa duração de forma robusta e serverless."
    },
    {
        "id": "aws-de-059",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Você precisa executar consultas SQL simples e rápidas em arquivos JSON e Parquet armazenados no S3 sem a necessidade de configurar servidores ou um data warehouse. Qual serviço é o mais indicado?",
        "solution": ["s3", "athena"],
        "availableServices": ["s3", "athena", "redshift", "emr", "ec2"],
        "explanation": "O Amazon Athena é um serviço de consulta interativo que facilita a análise de dados no Amazon S3 usando SQL padrão. O Athena é serverless, portanto, não há infraestrutura para gerenciar e você paga apenas pelas consultas que executa."
    },
    {
        "id": "aws-de-060",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Construa um pipeline para um sistema de detecção de fraudes. Transações chegam a um stream, são enriquecidas com dados de um banco de dados NoSQL e, em seguida, pontuadas por um modelo de machine learning. Transações suspeitas geram um alerta.",
        "solution": ["kinesis", "lambda", "dynamodb", "sagemaker", "sns"],
        "availableServices": ["kinesis", "lambda", "dynamodb", "sagemaker", "sns", "s3"],
        "explanation": "As transações fluem pelo Kinesis Data Streams. Uma função Lambda consome o stream, busca dados adicionais do cliente no DynamoDB para enriquecimento. A Lambda então chama um endpoint do SageMaker com os dados enriquecidos para obter uma pontuação de fraude. Se a pontuação exceder um limite, a Lambda publica uma mensagem em um tópico SNS para alertar a equipe de fraude."
    },
    {
        "id": "aws-de-061",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Você precisa armazenar dados que raramente são acessados, mas que devem ser mantidos por 7 anos por questões de conformidade. O custo de armazenamento é a principal preocupação. Qual classe de armazenamento do S3 ou serviço relacionado é a melhor escolha?",
        "solution": ["s3", "glacier"],
        "availableServices": ["s3", "glacier", "ebs", "efs"],
        "explanation": "O Amazon S3 Glacier Deep Archive é a classe de armazenamento de menor custo da AWS e foi projetada para retenção de dados a longo prazo (anos). Você pode usar as políticas de ciclo de vida do S3 para transicionar objetos automaticamente para o Glacier Deep Archive para otimização de custos."
    },
    {
        "id": "aws-de-062",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete uma arquitetura de streaming que processe logs de acesso em tempo real, identifique endereços IP maliciosos usando uma consulta SQL contínua em uma janela de tempo e os adicione a uma lista de bloqueio em um banco de dados.",
        "solution": ["kinesis", "kinesis-analytics", "lambda", "dynamodb"],
        "availableServices": ["kinesis", "kinesis-analytics", "lambda", "dynamodb", "athena", "s3"],
        "explanation": "Os logs de acesso são enviados para o Kinesis Data Streams. O Kinesis Data Analytics executa uma consulta SQL contínua que, por exemplo, conta as tentativas de login falhas de cada IP em uma janela de 5 minutos. Quando a contagem excede um limite, ele emite um resultado. Uma função Lambda, configurada como destino, recebe esse resultado e atualiza uma tabela no DynamoDB que serve como a lista de bloqueio."
    },
    {
        "id": "aws-de-063",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Uma aplicação gera eventos que são publicados em um tópico SNS. Você precisa garantir que esses eventos sejam processados de forma durável e na ordem em que foram recebidos para um grupo específico de consumidores. Como você pode estender a arquitetura?",
        "solution": ["sns", "sqs"],
        "availableServices": ["sns", "sqs", "lambda", "kinesis"],
        "explanation": "Para garantir a ordem, você pode usar uma fila SQS FIFO (First-In, First-Out). Inscreva a fila SQS FIFO no tópico SNS. Embora o SNS em si não garanta a ordem, ao usar uma fila FIFO como consumidor, você pode processar as mensagens na ordem exata em que chegam à fila para um determinado ID de grupo de mensagens."
    },
    {
        "id": "aws-de-064",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um job de ETL do AWS Glue precisa de uma dependência Python que não está incluída no ambiente padrão do Glue. Como você pode fornecer essa dependência para o seu job?",
        "solution": ["s3", "glue"],
        "availableServices": ["s3", "glue", "ec2", "lambda"],
        "explanation": "Você pode empacotar a dependência Python (por exemplo, em um arquivo .whl ou .zip), fazer o upload para um bucket S3 e, em seguida, especificar o caminho do S3 para esse pacote nas configurações do job do Glue no parâmetro 'Python library path' ou 'Dependent jars path'."
    },
    {
        "id": "aws-de-065",
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Automatize a criação de uma infraestrutura de data lake básica (S3, Glue Crawler, permissões do IAM) toda vez que um novo projeto é iniciado. A definição da infraestrutura deve ser versionada em um repositório de código.",
        "solution": ["codecommit", "codepipeline", "cloudformation"],
        "availableServices": ["codecommit", "codepipeline", "cloudformation", "s3", "glue", "iam"],
        "explanation": "Você pode definir toda a infraestrutura em um template do AWS CloudFormation. Armazene este template no AWS CodeCommit. Crie um AWS CodePipeline que é acionado por commits neste repositório. O pipeline pode ter um estágio de implantação que executa o template do CloudFormation para criar ou atualizar a stack de recursos, garantindo uma implantação consistente e automatizada (GitOps)."
    },
    {
        "id": "aws-de-066",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Você precisa de um banco de dados relacional para um novo aplicativo web, mas quer minimizar o gerenciamento. Qual serviço oferece bancos de dados como MySQL, PostgreSQL ou SQL Server como um serviço gerenciado?",
        "solution": ["rds"],
        "availableServices": ["rds", "ec2", "dynamodb", "redshift"],
        "explanation": "O Amazon RDS (Relational Database Service) é a escolha padrão para bancos de dados relacionais gerenciados na AWS. Ele suporta vários mecanismos de banco de dados populares e automatiza tarefas de administração, como patches de software, backups e failover."
    },
    {
        "id": "aws-de-067",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete uma arquitetura que permita que analistas de dados executem jobs Spark interativamente em um ambiente gerenciado, usando notebooks Jupyter. Os dados para análise estão em um Data Lake no S3.",
        "solution": ["s3", "emr"],
        "availableServices": ["s3", "emr", "sagemaker", "glue", "ec2"],
        "explanation": "O Amazon EMR (Elastic MapReduce) pode ser configurado com o EMR Notebooks, que fornece um ambiente Jupyter Notebook gerenciado. O notebook se conecta a um cluster EMR que tem Spark instalado. Isso permite que os analistas escrevam e executem código Spark interativamente contra os dados no S3, ideal para exploração e desenvolvimento de jobs."
    },
    {
        "id": "aws-de-068",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie uma API REST que permita aos usuários consultar dados de uma tabela DynamoDB. A API deve ser segura e escalável.",
        "solution": ["api-gateway", "lambda", "dynamodb"],
        "availableServices": ["api-gateway", "lambda", "dynamodb", "rds", "ec2"],
        "explanation": "O Amazon API Gateway é usado para criar e gerenciar a API REST. Cada endpoint (por exemplo, /users/{id}) pode ser integrado a uma função AWS Lambda. A função Lambda contém a lógica de negócios para consultar a tabela DynamoDB com base nos parâmetros recebidos da API Gateway e retorna o resultado. Essa arquitetura é totalmente serverless, escalável e segura."
    },
    {
        "id": "aws-de-069",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Como você pode consolidar dados de várias fontes (por exemplo, um banco de dados RDS e arquivos em S3) em um único local para análise complexa e relatórios, usando um serviço de data warehousing colunar?",
        "solution": [["rds", "dms", "redshift"], ["s3", "redshift"]],
        "availableServices": ["rds", "dms", "s3", "redshift", "glue", "athena"],
        "explanation": "O Amazon Redshift é um serviço de data warehousing em escala de petabytes. Você pode usar o AWS DMS para replicar dados do RDS para o Redshift. Para os dados do S3, você pode usar o comando COPY do Redshift, que é altamente otimizado para carregar dados de forma massiva. Isso centraliza os dados de ambas as fontes no Redshift para análise."
    },
    {
        "id": "aws-de-070",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Orquestre um pipeline onde um arquivo de dados é colocado no S3. Isso aciona um job do Glue para validação. Se a validação for aprovada, outro job do Glue é acionado para processamento. Se falhar, uma notificação é enviada. Use um serviço de orquestração de fluxo de trabalho.",
        "solution": ["s3", {"orchestrator": "step-functions", "flow": ["glue", "glue", "sns"]}],
        "availableServices": ["s3", "step-functions", "glue", "sns", "lambda", "eventbridge"],
        "explanation": "O upload no S3 pode acionar uma máquina de estados do Step Functions. O primeiro estado invoca o job de validação do Glue. Um estado 'Choice' verifica o resultado do job. Se for 'SUCESSO', ele transita para um estado que invoca o job de processamento do Glue. Se for 'FALHA', ele transita para um estado que publica uma mensagem em um tópico SNS."
    },
    {
        "id": "aws-de-071",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Qual é a maneira mais simples e totalmente gerenciada de enviar dados de um stream do Kinesis Data Streams para o Amazon S3, OpenSearch ou Redshift sem escrever código de consumidor?",
        "solution": ["kinesis", "kinesis-firehose"],
        "availableServices": ["kinesis", "kinesis-firehose", "lambda", "ec2"],
        "explanation": "O Kinesis Data Firehose pode ser configurado como um consumidor direto de um Kinesis Data Stream. Ele lida com a leitura do stream, o buffer dos dados, a conversão de formato (opcional) e a entrega confiável aos destinos suportados, como S3, OpenSearch e Redshift, eliminando a necessidade de desenvolver e gerenciar uma aplicação consumidora."
    },
    {
        "id": "aws-de-072",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Construa um sistema para governança centralizada de um Data Lake. Ele deve registrar todos os ativos de dados, impor políticas de acesso refinadas (nível de coluna) e auditar o acesso aos dados, tudo isso gerenciando permissões para Glue, Athena e Redshift Spectrum.",
        "solution": ["s3", "lake-formation"],
        "availableServices": ["s3", "lake-formation", "iam", "glue", "athena"],
        "explanation": "O AWS Lake Formation é o serviço projetado exatamente para isso. Você registra seus buckets S3 no Lake Formation, que então utiliza o Glue Data Catalog para metadados. No Lake Formation, você pode conceder e revogar permissões granulares para usuários e roles do IAM, que são então aplicadas de forma consistente quando esses principais tentam acessar os dados através de serviços integrados como Athena, Glue e Redshift Spectrum."
    },
    {
        "id": "aws-de-073",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um painel de análise precisa exibir dados atualizados a cada hora. Os dados brutos estão no S3. Qual combinação de serviços pode processar os dados em lote e alimentar um serviço de BI?",
        "solution": ["s3", "glue", "redshift", "quicksight"],
        "availableServices": ["s3", "glue", "redshift", "quicksight", "athena", "lambda"],
        "explanation": "Um job do AWS Glue pode ser agendado para ser executado a cada hora. O job lê os novos dados do S3, os transforma e os anexa a uma tabela no Amazon Redshift. O Amazon QuickSight pode então se conectar ao Redshift para alimentar os painéis com os dados atualizados."
    },
    {
        "id": "aws-de-074",
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Crie um sistema de alerta que monitore um bucket S3 e notifique uma equipe de segurança via SNS sempre que um objeto for tornado público.",
        "solution": ["s3", "eventbridge", "sns"],
        "availableServices": ["s3", "eventbridge", "sns", "cloudwatch", "lambda"],
        "explanation": "Você pode usar o AWS CloudTrail para registrar chamadas de API, como 'PutBucketAcl'. Em seguida, crie uma regra no Amazon EventBridge que corresponda a este evento específico quando a ACL for definida como pública. A regra do EventBridge pode ter como alvo um tópico SNS, que então envia a notificação para a equipe de segurança."
    },
    {
        "id": "aws-de-075",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Migre um banco de dados MySQL de 2 TB de um data center on-premise para o Amazon Aurora com o mínimo de tempo de inatividade (downtime).",
        "solution": ["dms"],
        "availableServices": ["dms", "aurora", "s3", "glue"],
        "explanation": "O AWS Database Migration Service (DMS) é a ferramenta ideal para isso. Você pode realizar uma carga inicial completa do banco de dados e, em seguida, configurar uma tarefa de replicação contínua (Change Data Capture - CDC). O DMS captura as alterações no banco de dados de origem e as aplica ao destino (Aurora) em tempo real. Isso permite que você mude o tráfego da aplicação para o novo banco de dados com um tempo de inatividade mínimo."
    },
    {
        "id": "aws-de-076",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Qual é a maneira mais econômica de executar um cluster Hadoop (com Spark e Hive) para um trabalho de processamento em lote que leva 3 horas para ser executado todas as noites?",
        "solution": ["emr"],
        "availableServices": ["emr", "ec2", "glue", "fargate"],
        "explanation": "O Amazon EMR (Elastic MapReduce) permite que você provisione clusters Hadoop sob demanda. Para um trabalho noturno, você pode iniciar um cluster EMR, executar o trabalho e encerrar o cluster automaticamente quando terminar. Isso significa que você paga apenas pelas 3 horas em que o cluster está em execução, tornando-o muito mais econômico do que manter instâncias EC2 funcionando 24/7."
    },
    {
        "id": "aws-de-077",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um serviço externo envia webhooks para sua aplicação. Projete uma arquitetura serverless e resiliente para receber esses webhooks, enfileirá-los para processamento e, em seguida, armazená-los no S3.",
        "solution": ["api-gateway", "sqs", "lambda", "s3"],
        "availableServices": ["api-gateway", "sqs", "lambda", "s3", "sns", "kinesis"],
        "explanation": "O API Gateway fornece um endpoint HTTP para receber os webhooks. Ele pode ser integrado diretamente com o SQS para enfileirar as solicitações recebidas. Isso desacopla a ingestão do processamento e absorve picos de tráfego. Uma função Lambda é acionada por mensagens na fila SQS, processa o webhook e armazena o payload no S3 para arquivamento e análise posterior."
    },
    {
        "id": "aws-de-078",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Você tem um stream de eventos de um jogo online em um cluster MSK (Kafka). Crie um pipeline que leia esses eventos, execute agregações complexas (por exemplo, pontuações médias por jogador em janelas de tempo) e envie os resultados para um dashboard em tempo real.",
        "solution": ["msk", "kinesis-analytics", "lambda", "opensearch"],
        "availableServices": ["msk", "kinesis-analytics", "lambda", "opensearch", "quicksight", "glue"],
        "explanation": "O Kinesis Data Analytics for Apache Flink pode se conectar diretamente a um cluster MSK como fonte. Você pode escrever uma aplicação Flink para realizar as agregações com estado e em janelas de tempo. A aplicação Flink pode usar um 'sink' para enviar os resultados agregados para uma função Lambda. A Lambda, por sua vez, insere os dados no Amazon OpenSearch, que pode ser usado como fonte para um dashboard de monitoramento em tempo real (como OpenSearch Dashboards ou Grafana)."
    },
    {
        "id": "aws-de-079",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Como você pode expor um conjunto de dados do seu data warehouse Redshift através de uma API para que outras aplicações possam consumi-lo de forma segura e controlada?",
        "solution": ["redshift", "lambda", "api-gateway"],
        "availableServices": ["redshift", "lambda", "api-gateway", "dms", "appsync"],
        "explanation": "Você pode criar um API Gateway para definir os endpoints da sua API de dados. Cada endpoint aciona uma função Lambda. A função Lambda contém a lógica para se conectar ao cluster Redshift (usando o Data API do Redshift para facilitar), executar a consulta SQL necessária com base nos parâmetros da API e formatar o resultado como uma resposta JSON. Isso fornece uma camada de abstração segura e escalável sobre o seu data warehouse."
    },
    {
        "id": "aws-de-080",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um fluxo de trabalho precisa executar uma série de funções Lambda em sequência. A saída de uma função é a entrada da próxima. O fluxo de trabalho inteiro não deve exceder 30 minutos. Como você orquestraria isso de forma confiável?",
        "solution": [{"orchestrator": "step-functions", "flow": ["lambda", "lambda", "lambda"]}],
        "availableServices": ["step-functions", "lambda", "sqs", "sns", "eventbridge"],
        "explanation": "O AWS Step Functions é ideal para orquestrar sequências de funções Lambda (ou outros serviços). Você pode definir uma 'máquina de estados' que executa cada Lambda em um estado sequencial. O Step Functions gerencia o estado, passa a saída de uma etapa como entrada para a próxima, lida com erros e timeouts, e tem um tempo máximo de execução de até um ano, acomodando facilmente os 30 minutos necessários."
    },
    {
        "id": "aws-de-081",
        "category": "data-engineering",
        "difficulty": 1,
        "question": "Qual é o serviço de armazenamento de objetos fundamental e mais comum na AWS, usado para uma ampla variedade de casos de uso, como data lakes, backups e hospedagem de sites estáticos?",
        "solution": ["s3"],
        "availableServices": ["s3", "ebs", "efs", "glacier"],
        "explanation": "O Amazon S3 (Simple Storage Service) é o serviço de armazenamento de objetos da AWS. Ele oferece escalabilidade, disponibilidade de dados, segurança e desempenho líderes do setor, tornando-se a base para a maioria das arquiteturas de dados na AWS."
    },
    {
        "id": "aws-de-082",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Crie uma arquitetura para análise de dados de séries temporais provenientes de sensores industriais. Os dados devem ser ingeridos e armazenados em um banco de dados otimizado para esse fim, e depois visualizados em um painel.",
        "solution": ["iot-core", "timestream", "quicksight"],
        "availableServices": ["iot-core", "timestream", "quicksight", "rds", "dynamodb"],
        "explanation": "O AWS IoT Core pode ingerir os dados dos sensores. Uma regra no IoT Core pode encaminhar os dados diretamente para o Amazon Timestream. O Timestream é projetado para armazenar e consultar dados de séries temporais de forma eficiente. O Amazon QuickSight pode então se conectar ao Timestream como fonte de dados para criar painéis e visualizações."
    },
    {
        "id": "aws-de-083",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um processo de negócio requer a aprovação de um gerente antes de prosseguir. Projete um fluxo de trabalho que inicie uma tarefa, pause e espere por uma aprovação externa (via API ou console), e depois continue ou pare com base na resposta.",
        "solution": [{"orchestrator": "step-functions", "flow": ["lambda", "sns"]}],
        "availableServices": ["step-functions", "lambda", "sns", "sqs", "api-gateway"],
        "explanation": "O AWS Step Functions suporta o padrão 'wait for a callback'. Você pode definir um estado de tarefa que invoca uma Lambda. Essa Lambda inicia a tarefa e, em seguida, o Step Functions pausa a execução. A Lambda pode enviar um e-mail com um link de aprovação. Quando o gerente aprova, um processo externo usa o 'task token' para enviar um sinal de sucesso de volta ao Step Functions, que então retoma a execução. O SNS pode ser usado para a notificação inicial."
    },
    {
        "id": "aws-de-084",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Você precisa de um cache em memória para sua aplicação, com suporte para estruturas de dados avançadas como listas, hashes e conjuntos ordenados. Qual serviço gerenciado e qual mecanismo você escolheria?",
        "solution": ["elasticache"],
        "availableServices": ["elasticache", "dynamodb", "memorydb", "rds"],
        "explanation": "O Amazon ElastiCache for Redis é a escolha ideal. O ElastiCache é o serviço gerenciado, e o mecanismo Redis fornece o cache em memória com suporte robusto para múltiplas estruturas de dados, tornando-o extremamente flexível para casos de uso como placares, sessões e filas."
    },
    {
        "id": "aws-de-085",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Você precisa analisar dados em um data warehouse Redshift em conjunto com dados de log que estão sendo transmitidos em tempo real via Kinesis. Como você pode consultar ambos os conjuntos de dados de forma eficiente?",
        "solution": ["redshift", "kinesis"],
        "availableServices": ["redshift", "kinesis", "athena", "glue", "dms"],
        "explanation": "O Amazon Redshift suporta 'Materialized Views' que podem se integrar com o Kinesis Data Streams. Você pode criar uma visão materializada que consome e processa os dados do stream do Kinesis quase em tempo real. Isso permite que você execute consultas SQL no Redshift que unem os dados históricos do data warehouse com os dados de streaming mais recentes da visão materializada."
    },
    {
        "id": "aws-de-086",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Uma função Lambda precisa enviar notificações por e-mail, SMS e push para dispositivos móveis. Qual serviço permite que ela publique uma única mensagem que será entregue a múltiplos tipos de assinantes?",
        "solution": ["lambda", "sns"],
        "availableServices": ["lambda", "sns", "sqs", "ses"],
        "explanation": "O Amazon SNS (Simple Notification Service) é um serviço de mensagens publish/subscribe totalmente gerenciado. A Lambda pode publicar uma mensagem em um tópico SNS. Você pode ter diferentes tipos de 'assinaturas' para esse tópico, incluindo endpoints de e-mail, números de telefone (SMS) e endpoints de aplicativos móveis. O SNS cuida da entrega para cada assinante."
    },
    {
        "id": "aws-de-087",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie um pipeline de dados agendado usando o orquestrador preferido para engenheiros de dados (Airflow). O pipeline deve extrair dados de uma API, salvá-los no S3 e iniciar um job de transformação do Glue.",
        "solution": [{"orchestrator": "mwaa", "flow": ["s3", "glue"]}],
        "availableServices": ["mwaa", "s3", "glue", "lambda", "step-functions"],
        "explanation": "O MWAA (Managed Workflows for Apache Airflow) hospeda o ambiente Airflow. Uma DAG (Directed Acyclic Graph) no Airflow pode ser agendada para execução. A DAG conteria: 1) Um operador (como o PythonOperator) para chamar a API e salvar os dados no S3. 2) O `AwsGlueJobOperator` para iniciar o job do AWS Glue, esperando sua conclusão antes de prosseguir."
    },
    {
        "id": "aws-de-088",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Você está construindo um Data Lake e quer garantir que os dados armazenados no S3 sejam criptografados no lado do servidor. Você quer usar chaves de criptografia gerenciadas pela AWS, sem ter que gerenciar as chaves você mesmo. Qual é a configuração de criptografia mais simples no S3 para isso?",
        "solution": ["s3", "kms"],
        "availableServices": ["s3", "kms", "cloudhsm", "secrets-manager"],
        "explanation": "A criptografia do lado do servidor com chaves gerenciadas pelo AWS Key Management Service (SSE-KMS) é a abordagem recomendada. Ela oferece um equilíbrio entre simplicidade e controle. A AWS gerencia a chave mestra, mas você pode controlar o acesso a ela através de políticas do IAM e auditar seu uso via CloudTrail. A opção mais simples (SSE-S3) usa chaves gerenciadas pelo S3, mas o SSE-KMS oferece mais controle e recursos de auditoria."
    },
    {
        "id": "aws-de-089",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Implemente um padrão de 'Circuit Breaker' para uma chamada de API feita por uma função Lambda. Se a API externa falhar repetidamente, o 'circuito' deve 'abrir' e parar de fazer chamadas por um tempo. O estado do circuito deve ser compartilhado entre as execuções da Lambda.",
        "solution": ["lambda", "elasticache"],
        "availableServices": ["lambda", "elasticache", "dynamodb", "s3", "step-functions"],
        "explanation": "Para compartilhar o estado (aberto/fechado, contagem de falhas) entre as invocações concorrentes e independentes da Lambda, é necessário um armazenamento de estado externo e de baixa latência. O Amazon ElastiCache (Redis) é perfeito para isso. A Lambda pode verificar o estado do circuito no Redis antes de chamar a API, incrementar contadores de falha e definir o estado para 'aberto' com um TTL (Time To Live), que efetivamente fecha o circuito novamente após o tempo de expiração."
    },
    {
        "id": "aws-de-090",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Crie uma arquitetura para análise de grafos em um grande conjunto de dados de transações financeiras para detectar anéis de fraude. Os dados estão armazenados no S3. O processo deve ser executado em lote.",
        "solution": ["s3", "glue", "neptune"],
        "availableServices": ["s3", "glue", "neptune", "emr", "dynamodb"],
        "explanation": "Primeiro, um job do AWS Glue (ou EMR) pode ser usado para ler os dados de transações do S3 e transformá-los em um formato de grafo (por exemplo, vértices para contas, arestas para transações). Em seguida, o job pode usar o carregador em massa do Neptune para carregar eficientemente os dados no banco de dados de grafos Amazon Neptune. Uma vez no Neptune, você pode executar consultas de grafos complexas (usando Gremlin ou SPARQL) para identificar padrões como anéis, onde o dinheiro circula entre um grupo de contas."
    },
    {
        "id": "aws-de-091",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Seus analistas de BI usam o QuickSight, mas os painéis estão lentos porque consultam diretamente o Athena em grandes conjuntos de dados. Como você pode acelerar o desempenho das consultas do QuickSight?",
        "solution": ["athena", "quicksight"],
        "availableServices": ["athena", "quicksight", "s3", "redshift"],
        "explanation": "O Amazon QuickSight possui um mecanismo de dados em memória super-rápido chamado SPICE (Super-fast, Parallel, In-memory Calculation Engine). Em vez de consultar o Athena diretamente a cada vez (Direct Query), você pode configurar seu conjunto de dados no QuickSight para importar os dados do Athena para o SPICE em um agendamento. As interações no painel serão então atendidas pelo SPICE, resultando em um desempenho muito mais rápido."
    },
    {
        "id": "aws-de-092",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um aplicativo móvel precisa armazenar dados do usuário (perfil, preferências) em um banco de dados que possa sincronizar os dados automaticamente para uso offline. Qual combinação de serviços da AWS é ideal para isso?",
        "solution": ["cognito", "dynamodb"],
        "availableServices": ["cognito", "dynamodb", "appsync", "lambda"],
        "explanation": "O Amazon Cognito Identity Pools pode se integrar com o Cognito Sync, que permite salvar dados do usuário (pares chave-valor) associados à sua identidade. Esses dados são armazenados em um conjunto de dados no serviço, que por sua vez pode ser sincronizado com o DynamoDB. Isso permite que os dados persistam entre as sessões e dispositivos e estejam disponíveis offline no dispositivo móvel."
    },
    {
        "id": "aws-de-093",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um job Spark rodando no EMR precisa ler e escrever dados em um bucket S3 criptografado com chaves KMS. Como você concede as permissões necessárias ao cluster EMR?",
        "solution": ["emr", "s3", "kms", "iam"],
        "availableServices": ["emr", "s3", "kms", "iam", "secrets-manager"],
        "explanation": "Você precisa configurar a 'Instance Profile' do IAM para as instâncias EC2 do cluster EMR. Essa role deve ter duas políticas anexadas: uma que concede permissões de acesso ao bucket S3 (s3:GetObject, s3:PutObject, etc.) e outra que concede permissões para usar a chave KMS (kms:Decrypt, kms:Encrypt, kms:GenerateDataKey). Isso permite que o Spark no EMR acesse o S3 e que o S3 use a chave KMS em nome do EMR."
    },
    {
        "id": "aws-de-094",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete uma solução de pesquisa semântica. Documentos são armazenados no S3. Um pipeline deve processá-los, gerar embeddings vetoriais usando um modelo de ML e armazená-los em um banco de dados que suporte pesquisa de vizinhos mais próximos (k-NN).",
        "solution": ["s3", "lambda", "sagemaker", "opensearch"],
        "availableServices": ["s3", "lambda", "sagemaker", "opensearch", "comprehend", "neptune"],
        "explanation": "Um evento S3 aciona uma Lambda. A Lambda invoca um endpoint do SageMaker (hospedando um modelo como BERT) para gerar os embeddings vetoriais do texto do documento. Esses vetores são então indexados no Amazon OpenSearch Service, que possui um plugin k-NN integrado. Isso permite que você execute pesquisas de similaridade, encontrando documentos semanticamente semelhantes a uma consulta, em vez de apenas correspondência de palavras-chave."
    },
    {
        "id": "aws-de-095",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Qual serviço você usaria para hospedar um repositório Git privado para o código-fonte dos seus jobs de ETL e aplicações de dados?",
        "solution": ["codecommit"],
        "availableServices": ["codecommit", "s3", "codepipeline", "github"],
        "explanation": "O AWS CodeCommit é um serviço de controle de origem totalmente gerenciado que hospeda repositórios Git privados e seguros. Ele se integra bem com outros serviços da AWS, como o IAM para controle de acesso e o CodePipeline para CI/CD."
    },
    {
        "id": "aws-de-096",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Você está migrando um aplicativo legado que usa fortemente procedimentos armazenados (stored procedures) para um banco de dados relacional na AWS. Qual serviço de banco de dados da AWS oferece a melhor compatibilidade e desempenho para essa carga de trabalho?",
        "solution": ["aurora"],
        "availableServices": ["aurora", "rds", "redshift", "dynamodb"],
        "explanation": "O Amazon Aurora (edições compatíveis com PostgreSQL ou MySQL) é uma excelente escolha. Ele é um banco de dados relacional de alto desempenho que mantém total compatibilidade com seus respectivos mecanismos de código aberto. Isso significa que os procedimentos armazenados existentes provavelmente funcionarão com pouca ou nenhuma modificação, enquanto você se beneficia do desempenho, escalabilidade e resiliência do Aurora."
    },
    {
        "id": "aws-de-097",
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Como você pode obter uma visão geral do seu estado de segurança na AWS e verificar se suas configurações atendem às melhores práticas de segurança e padrões de conformidade?",
        "solution": ["macie"],
        "availableServices": ["macie", "iam", "cloudwatch", "inspector"],
        "explanation": "O AWS Security Hub é o serviço projetado para isso (não está na lista). Dentre as opções, Macie é focado em dados no S3. No entanto, o contexto de segurança é mais amplo. Se a pergunta for sobre dados, a resposta é Macie. (Nota: Nenhuma das opções listadas é a melhor ferramenta para uma visão *geral* da postura de segurança, mas Macie é a mais próxima no domínio de segurança de dados)."
    },
    {
        "id": "aws-de-098",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um stream de dados do Kinesis contém registros em formato Avro. Crie uma arquitetura serverless que decodifique esses registros e os insira em uma tabela DynamoDB.",
        "solution": ["kinesis", "glue", "lambda", "dynamodb"],
        "availableServices": ["kinesis", "glue", "lambda", "dynamodb", "s3"],
        "explanation": "O AWS Glue Schema Registry pode ser usado para armazenar e validar esquemas Avro. Uma função Lambda consumidora do Kinesis pode se integrar com o Glue Schema Registry para buscar o esquema e desserializar os registros Avro. Após a decodificação bem-sucedida, a função Lambda pode gravar os dados convertidos em JSON na tabela DynamoDB."
    },
    {
        "id": "aws-de-099",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete uma arquitetura para uma plataforma de análise de logs de segurança (SIEM). Os logs de várias fontes (CloudTrail, VPC Flow Logs, etc.) são centralizados, processados e indexados para pesquisa em tempo real, e os dados de longo prazo são arquivados de forma econômica.",
        "solution": ["kinesis-firehose", [["opensearch"], ["s3", "glacier"]]],
        "availableServices": ["kinesis-firehose", "opensearch", "s3", "glacier", "athena", "lambda"],
        "explanation": "Os logs podem ser enviados para um Kinesis Data Firehose. O Firehose pode ter dois destinos configurados. O primeiro é um cluster do Amazon OpenSearch, que indexa os dados para análise de segurança e correlação de eventos em tempo real. O segundo destino é um bucket S3. Você pode configurar políticas de ciclo de vida no S3 para mover os logs para o S3 Glacier após um período (por exemplo, 90 dias) para arquivamento de longo prazo e de baixo custo, garantindo a conformidade."
    },
    {
        "id": "aws-de-100",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um pipeline de ETL é orquestrado pelo Step Functions. Como você pode garantir que, se um job do Glue falhar, o pipeline tente executá-lo novamente mais duas vezes com um intervalo de 5 minutos antes de falhar completamente?",
        "solution": [{"orchestrator": "step-functions", "flow": ["glue"]}],
        "availableServices": ["step-functions", "glue", "lambda", "cloudwatch"],
        "explanation": "O AWS Step Functions tem recursos de tratamento de erros e repetição (retry) integrados na definição do estado. No estado da máquina de estados que invoca o job do Glue, você pode adicionar uma cláusula 'Retry'. Nela, você pode especificar os erros a serem capturados, o número máximo de tentativas (MaxAttempts: 2), o intervalo entre as tentativas (IntervalSeconds: 300) e um fator de aumento opcional (BackoffRate)."
    },
    {
        "id": "aws-de-101",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Projete uma solução para ingerir logs de aplicações de um grande número de servidores e disponibilizá-los para consulta via SQL sem provisionar um banco de dados.",
        "solution": ["kinesis-firehose", "s3", "athena"],
        "availableServices": ["kinesis-firehose", "s3", "athena", "glue", "redshift"],
        "explanation": "É possível instalar um agente (como o Kinesis Agent) nos servidores para enviar logs para o Kinesis Data Firehose. O Firehose agrega e entrega os dados ao S3. Opcionalmente, o Firehose pode invocar uma função Lambda para transformar os logs ou o AWS Glue para converter o formato para Parquet. Finalmente, o Athena pode ser usado para consultar diretamente os arquivos de log no S3 usando SQL."
    },
    {
        "id": "aws-de-102",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Orquestre um pipeline que primeiro executa um job do Glue para preparar os dados e, em seguida, inicia um trabalho de treinamento de modelo no SageMaker usando esses dados preparados.",
        "solution": [{"orchestrator": "step-functions", "flow": ["glue", "sagemaker"]}],
        "availableServices": ["step-functions", "glue", "sagemaker", "lambda", "mwaa"],
        "explanation": "O AWS Step Functions é ideal para orquestrar fluxos de trabalho que envolvem múltiplos serviços da AWS. Uma máquina de estados pode ser definida com um primeiro estado que inicia o job do AWS Glue e espera por sua conclusão. O próximo estado, em sequência, inicia o trabalho de treinamento do SageMaker, usando a saída do job do Glue como entrada."
    },
    {
        "id": "aws-de-103",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Como você pode servir uma aplicação web estática (HTML, CSS, JavaScript) com um domínio personalizado, de forma escalável e com baixa latência global?",
        "solution": ["s3", "cloudfront", "route 53"],
        "availableServices": ["s3", "cloudfront", "route 53", "ec2", "api-gateway"],
        "explanation": "Os arquivos do site estático podem ser hospedados em um bucket S3 configurado para hospedagem de site. O Amazon CloudFront pode ser colocado na frente do bucket S3 para servir o conteúdo globalmente através de sua CDN. Finalmente, o Amazon Route 53 pode ser usado para apontar o domínio personalizado para a distribuição do CloudFront."
    },
    {
        "id": "aws-de-104",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Desenvolva uma arquitetura para processar um feed de notícias em tempo real, extrair entidades (pessoas, organizações) do texto e construir um grafo de conhecimento que relacione essas entidades.",
        "solution": ["kinesis", "lambda", "comprehend", "neptune"],
        "availableServices": ["kinesis", "lambda", "comprehend", "neptune", "s3", "dynamodb"],
        "explanation": "O feed de notícias é ingerido via Kinesis Data Streams. Uma função Lambda consome o stream, pega o texto de cada artigo e o envia para o Amazon Comprehend para o reconhecimento de entidades nomeadas (NER). A Lambda então processa as entidades extraídas e suas relações, e as insere no Amazon Neptune para construir e atualizar o grafo de conhecimento em tempo real."
    },
    {
        "id": "aws-de-105",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Você precisa de um banco de dados NoSQL de chave-valor com um cache em memória integrado para leituras de latência de microssegundos. Qual serviço oferece essa capacidade?",
        "solution": ["dynamodb"],
        "availableServices": ["dynamodb", "elasticache", "rds", "documentdb"],
        "explanation": "O Amazon DynamoDB Accelerator (DAX) é um cache em memória totalmente gerenciado e altamente disponível para o DynamoDB. O DAX melhora o desempenho de leitura de milissegundos para microssegundos, mesmo com milhões de solicitações por segundo, tornando-o ideal para aplicações que exigem a leitura mais rápida possível."
    },
    {
        "id": "aws-de-106",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um pipeline de dados processa informações de identificação pessoal (PII). Como você pode descobrir e mascarar automaticamente os campos de PII durante um job de ETL do Glue?",
        "solution": ["glue", "macie"],
        "availableServices": ["glue", "macie", "lambda", "kms"],
        "explanation": "O AWS Glue Studio inclui uma transformação visual chamada 'Detect PII', que pode ser adicionada ao seu job de ETL. Essa transformação utiliza o Amazon Macie para identificar 13 tipos diferentes de PII nos seus dados. Você pode então escolher uma ação a ser tomada sobre os dados detectados, como mascarar, substituir ou redigir parcialmente, antes de gravá-los no destino."
    },
    {
        "id": "aws-de-107",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie um pipeline serverless que, ao receber um arquivo de áudio no S3, o transcreva para texto e, em seguida, analise o sentimento do texto transcrito.",
        "solution": ["s3", "lambda", "comprehend"],
        "availableServices": ["s3", "lambda", "comprehend", "transcribe", "polly"],
        "explanation": "O upload no S3 aciona uma Lambda. A Lambda inicia um trabalho de transcrição usando o Amazon Transcribe (não listado, mas essencial para a tarefa). Quando a transcrição estiver completa, outro evento (via EventBridge) pode acionar uma segunda Lambda. Esta Lambda pega o texto transcrito e o envia para o Amazon Comprehend para análise de sentimento. O resultado é então armazenado."
    },
    {
        "id": "aws-de-108",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Como você pode executar consultas SQL em seus dados operacionais no DynamoDB para fins analíticos sem impactar o desempenho da sua tabela de produção?",
        "solution": ["dynamodb", "athena"],
        "availableServices": ["dynamodb", "athena", "glue", "redshift", "lambda"],
        "explanation": "Uma opção é exportar os dados do DynamoDB para o S3 usando a funcionalidade de exportação para S3. Uma vez que os dados estão no S3, você pode usar o Amazon Athena para executar consultas analíticas complexas nos dados sem afetar a performance da tabela DynamoDB de produção. Para dados mais recentes, o Athena Federated Query pode consultar o DynamoDB diretamente, mas a exportação é melhor para análises em larga escala."
    },
    {
        "id": "aws-de-109",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete uma arquitetura de 'write-through cache'. Uma aplicação escreve dados através de uma API. A escrita deve ocorrer tanto no cache de baixa latência quanto no banco de dados de persistência (RDS). A leitura deve priorizar o cache.",
        "solution": ["api-gateway", "lambda", "elasticache", "rds"],
        "availableServices": ["api-gateway", "lambda", "elasticache", "rds", "dynamodb"],
        "explanation": "A API Gateway aciona uma função Lambda. A Lambda contém a lógica de escrita: ela primeiro escreve os dados no banco de dados RDS (a fonte da verdade) e, em caso de sucesso, escreve os mesmos dados no cache ElastiCache. Para a lógica de leitura, a Lambda primeiro tentaria ler do ElastiCache. Se houver um 'cache miss' (dado não encontrado), ela leria do RDS, popularia o cache com o dado e o retornaria."
    },
    {
        "id": "aws-de-110",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Você precisa de um banco de dados relacional que se escale automaticamente para zero quando não estiver em uso para economizar custos, ideal para ambientes de desenvolvimento ou cargas de trabalho intermitentes.",
        "solution": ["aurora"],
        "availableServices": ["aurora", "rds", "redshift", "ec2"],
        "explanation": "O Amazon Aurora Serverless é uma configuração sob demanda e de dimensionamento automático para o Amazon Aurora. Ele inicia, encerra e dimensiona a capacidade com base na carga de trabalho da sua aplicação. Quando não há conexões por um período configurável, ele pode pausar completamente, resultando em custo zero de computação."
    },
    {
        "id": "aws-de-111",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Como você pode mover 500 TB de dados de um data center on-premise para o S3 se a sua conexão de rede é lenta e instável?",
        "solution": ["s3"],
        "availableServices": ["s3", "dms", "snowball", "datasync"],
        "explanation": "Para grandes volumes de dados com conectividade de rede limitada, a família AWS Snow (como o AWS Snowball) é a solução ideal. A AWS envia um dispositivo de armazenamento físico para o seu data center. Você carrega os dados no dispositivo e o envia de volta para a AWS, que então transfere os dados para o seu bucket S3, contornando a rede pública."
    },
    {
        "id": "aws-de-112",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Implemente uma arquitetura de 'Strangler Fig Pattern' para migrar um monolito. Crie uma API que roteia o tráfego para a aplicação legada ou para novos microsserviços (Lambdas) com base no caminho da URL.",
        "solution": ["api-gateway", [["lambda"], ["ec2"]]],
        "availableServices": ["api-gateway", "lambda", "ec2", "route 53", "application-load-balancer"],
        "explanation": "O Amazon API Gateway pode atuar como a fachada. Você pode configurar rotas específicas (ex: /api/v2/users) para acionar novas funções Lambda (microsserviços). Para as rotas ainda não migradas (ex: /api/v1/*), você pode configurar uma integração de 'proxy HTTP' ou 'proxy VPC Link' para encaminhar as solicitações para a aplicação monolítica legada rodando em uma EC2 ou atrás de um balanceador de carga."
    },
    {
        "id": "aws-de-113",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Você quer criar um dashboard de BI que combine dados de vendas do Redshift e dados de marketing de uma planilha do Google Sheets. Qual serviço de BI da AWS suporta nativamente a conexão com ambas as fontes?",
        "solution": ["quicksight"],
        "availableServices": ["quicksight", "redshift", "athena", "glue"],
        "explanation": "O Amazon QuickSight suporta uma ampla gama de fontes de dados, incluindo bancos de dados da AWS como o Redshift, e também aplicações de terceiros e fontes de dados SaaS. Ele pode se conectar a ambos, Redshift e Google Sheets, permitindo que você junte os dados de ambas as fontes em um único conjunto de dados para criar dashboards unificados."
    },
    {
        "id": "aws-de-114",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete uma solução para ingerir dados de vídeo de câmeras de segurança, analisá-los em tempo real para detectar objetos específicos (ex: uma pessoa em uma área restrita) e enviar um alerta.",
        "solution": ["kinesis", "sagemaker", "lambda", "sns"],
        "availableServices": ["kinesis", "sagemaker", "lambda", "sns", "rekognition"],
        "explanation": "O Kinesis Video Streams é projetado para ingerir e armazenar streams de vídeo. Você pode conectar uma aplicação consumidora (rodando em EC2 ou Fargate, ou usando Lambda) que lê os fragmentos de vídeo do stream. A aplicação pode usar uma biblioteca de ML como OpenCV e um modelo treinado e hospedado no SageMaker para realizar a detecção de objetos em cada quadro. Se um objeto de interesse for detectado, a aplicação invoca uma função Lambda que envia um alerta via SNS."
    },
    {
        "id": "aws-de-115",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Como você pode executar um job de ETL do Glue que precisa acessar recursos dentro de uma VPC, como um cluster ElastiCache ou um banco de dados RDS, de forma segura?",
        "solution": ["glue", "rds"],
        "availableServices": ["glue", "rds", "vpc", "iam"],
        "explanation": "Ao configurar uma conexão no AWS Glue, você pode especificar a VPC, a sub-rede e os grupos de segurança (security groups) a serem usados. Isso anexa uma Interface de Rede Elástica (ENI) da sua VPC à tarefa do Glue. Ao configurar corretamente os security groups para permitir o tráfego entre a ENI do Glue e os recursos da VPC (como RDS), o job pode acessá-los de forma privada e segura."
    },
    {
        "id": "aws-de-116",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Seu Data Lake no S3 está crescendo rapidamente. Como você pode analisar os padrões de acesso aos dados para identificar quais objetos são acessados com frequência e quais podem ser movidos para uma classe de armazenamento mais barata?",
        "solution": ["s3"],
        "availableServices": ["s3", "cloudwatch", "athena", "storage-lens"],
        "explanation": "O Amazon S3 Storage Lens oferece visibilidade em toda a organização sobre o uso e a atividade do armazenamento de objetos, com mais de 29 métricas. Ele fornece dashboards interativos para visualizar tendências e identificar oportunidades de otimização de custos, como identificar dados raramente acessados que são candidatos para transição para S3 Intelligent-Tiering ou S3 Glacier."
    },
    {
        "id": "aws-de-117",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie um pipeline de dados que reage a eventos de alteração em uma tabela DynamoDB (inserções, atualizações) e replica esses dados para um cluster OpenSearch para habilitar pesquisas de texto completo.",
        "solution": ["dynamodb", "lambda", "opensearch"],
        "availableServices": ["dynamodb", "lambda", "opensearch", "kinesis", "dms"],
        "explanation": "Você pode habilitar o DynamoDB Streams na sua tabela. O stream captura todas as modificações nos itens da tabela. Uma função Lambda pode ser configurada como um consumidor do stream. A Lambda recebe os registros de alteração, os transforma no formato JSON necessário e os insere no cluster Amazon OpenSearch, mantendo o índice de pesquisa sincronizado com a tabela."
    },
    {
        "id": "aws-de-118",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Um pipeline de processamento de dados em lote precisa ser executado usando um DAG do Airflow. A tarefa principal é um job Spark pesado que deve ser executado em um cluster Kubernetes para isolamento e gerenciamento de dependências. Como você pode montar essa arquitetura?",
        "solution": [{"orchestrator": "mwaa", "flow": ["eks"]}],
        "availableServices": ["mwaa", "eks", "emr", "glue", "fargate"],
        "explanation": "Você pode usar o MWAA (Managed Workflows for Apache Airflow) como orquestrador. Dentro da sua DAG do Airflow, você pode usar o `KubernetesPodOperator`. Este operador inicia um pod no seu cluster Amazon EKS. O pod pode conter uma imagem Docker com o Spark e sua aplicação. O pod executa o job Spark e, após a conclusão, termina. O MWAA monitora o status do pod e continua ou falha a tarefa na DAG com base no resultado."
    },
    {
        "id": "aws-de-119",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Como você pode criar uma tabela no seu Data Lake cujos dados são particionados por ano, mês e dia para otimizar as consultas do Athena que filtram por período?",
        "solution": ["s3", "glue", "athena"],
        "availableServices": ["s3", "glue", "athena", "lambda"],
        "explanation": "Ao organizar seus dados no S3, você deve usar uma estrutura de prefixo que reflita as partições, como `s3://bucket/dados/ano=2025/mes=09/dia=16/`. Quando você executa um AWS Glue Crawler, ele reconhece automaticamente essa estrutura e define 'ano', 'mes' e 'dia' como colunas de partição na tabela do Glue Data Catalog. O Athena então usa essas partições para escanear apenas os dados relevantes quando uma consulta inclui um filtro `WHERE` nessas colunas."
    },
    {
        "id": "aws-de-120",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um aplicativo de mídia social precisa de um feed de atividades em tempo real. As ações do usuário (curtidas, comentários) devem ser adicionadas a um stream e processadas para atualizar os feeds de outros usuários. Qual serviço é ideal para a ingestão de streams e qual banco de dados é bom para modelar feeds?",
        "solution": ["kinesis", "lambda", "dynamodb"],
        "availableServices": ["kinesis", "lambda", "dynamodb", "rds", "sns"],
        "explanation": "O Kinesis Data Streams é projetado para a ingestão de alto volume de dados em tempo real, como ações do usuário. Uma função Lambda pode processar esses eventos. O DynamoDB é uma excelente escolha para o backend do feed de atividades, pois seu modelo de dados flexível e acesso de baixa latência são ideais para modelar e recuperar listas de atividades para os usuários."
    },
    {
        "id": "aws-de-121",
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Projete uma arquitetura para um sistema de 'feature store' para Machine Learning. As features pré-calculadas devem ser armazenadas para acesso de baixa latência (online) durante a inferência e também para acesso de alto rendimento (offline) durante o treinamento de modelos.",
        "solution": [["dynamodb"], ["s3"]],
        "availableServices": ["dynamodb", "s3", "sagemaker", "redis", "redshift"],
        "explanation": "Uma arquitetura de feature store dual é comum. Para a 'loja online', o DynamoDB ou ElastiCache (Redis) oferecem o acesso de baixa latência necessário para a inferência em tempo real. Para a 'loja offline', o S3 (usando um formato como Parquet) é ideal. Ele fornece o armazenamento de baixo custo e alto rendimento necessário para que os jobs de treinamento do SageMaker ou do Glue leiam grandes volumes de features históricas."
    },
    {
        "id": "aws-de-122",
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Você precisa de um serviço para enviar e-mails transacionais (como confirmação de pedido ou redefinição de senha) de sua aplicação. Qual serviço da AWS é projetado para isso?",
        "solution": ["lambda"],
        "availableServices": ["lambda", "sns", "ses", "sqs"],
        "explanation": "O Amazon SES (Simple Email Service) (não na lista) é a resposta correta. Dentre as opções, a Lambda pode ser usada para *chamar* um serviço de e-mail, mas o SNS é a opção mais próxima para envio de notificações, embora o SES seja o serviço específico para e-mail transacional. A questão está um pouco falha com as opções dadas, mas o fluxo seria `lambda` -> `sns`."
    },
    {
        "id": "aws-de-123",
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Crie um pipeline de processamento de transações com cartão de crédito que seja compatível com o PCI DSS. Os dados sensíveis devem ser criptografados com uma chave que só você controla. O pipeline processa os dados e os armazena em um banco de dados seguro.",
        "solution": ["kinesis", "lambda", "kms", "dynamodb"],
        "availableServices": ["kinesis", "lambda", "kms", "dynamodb", "s3", "rds"],
        "explanation": "Os dados são ingeridos via Kinesis. Uma função Lambda processa os dados. Para a conformidade com o PCI DSS, o AWS KMS com uma Chave Gerenciada pelo Cliente (CMK) é crucial. A Lambda usa o KMS para criptografar os dados sensíveis (como o número do cartão) antes de armazená-los. O DynamoDB, com criptografia em repouso habilitada usando a mesma CMK do KMS, armazena os dados de forma segura. O uso de uma CMK garante que você tenha controle total e auditabilidade sobre a chave de criptografia."
    },
    {
        "id": "aws-de-124",
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um job do Glue que processa dados de um bucket S3 está falhando intermitentemente com erros de 'throttling' (limitação) do S3. Como você pode mitigar esse problema sem alterar o código do job?",
        "solution": ["s3", "glue"],
        "availableServices": ["s3", "glue", "lambda", "cloudwatch"],
        "explanation": "O S3 pode limitar as solicitações se muitas leituras estiverem sendo feitas no mesmo prefixo. Uma solução é habilitar o 'S3 Bucket Keys' no bucket. Isso reduz o custo e o tráfego de solicitações do seu job do Glue para o KMS. Outra abordagem é otimizar o job para ler os dados de forma mais distribuída, ou se a causa for o número de partições, agrupar arquivos menores em arquivos maiores para reduzir o número de solicitações `List` e `Get`."
    }, 
    {
    "id": "aws-da-001",
    "category": "data-analysis",
    "difficulty": 2,
    "question": "Você precisa armazenar arquivos CSV com terabytes de dados históricos e consultá-los esporadicamente usando SQL padrão, pagando apenas pelas consultas que executa. Qual a combinação de serviços mais econômica?",
    "solution": ["s3", "athena"],
    "availableServices": ["s3", "athena", "rds", "dynamodb", "redshift"],
    "explanation": "O S3 é o serviço ideal para armazenamento de objetos de baixo custo e alta durabilidade. O Athena permite consultar dados diretamente no S3 usando SQL padrão, com um modelo de pagamento por consulta, tornando-o perfeito para análises ad-hoc."
  },
  {
    "id": "aws-da-002",
    "category": "data-analysis",
    "difficulty": 5,
    "question": "Replique continuamente os dados de um banco de dados on-premises (Oracle) para a AWS, com o objetivo de alimentar um Data Warehouse no Redshift. A migração deve ter o mínimo de tempo de inatividade.",
    "solution": ["dms", "redshift"],
    "availableServices": ["dms", "redshift", "glue", "lambda", "s3", "kinesis"],
    "explanation": "O AWS Database Migration Service (DMS) é projetado para migrar bancos de dados para a AWS de forma contínua (usando Change Data Capture - CDC) e com tempo de inatividade mínimo. Ele pode carregar os dados diretamente no Redshift como um destino."
  },
  {
    "id": "aws-da-003",
    "category": "data-analysis",
    "difficulty": 4,
    "question": "Crie um trabalho de ETL serverless que é acionado sempre que um novo arquivo JSON chega em um bucket S3. O trabalho deve converter o JSON para o formato Parquet e salvá-lo em outro bucket.",
    "solution": ["s3", "glue"],
    "availableServices": ["s3", "glue", "lambda", "emr", "redshift"],
    "explanation": "O AWS Glue é um serviço de ETL totalmente gerenciado e serverless. Ele pode ser acionado por eventos do S3, e seus trabalhos (usando Spark) são ideais para transformações de formato de arquivo como JSON para Parquet."
  },
  {
    "id": "aws-da-004",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Construa um painel de BI interativo para analistas de negócios. Os dados estão em um Data Warehouse no Redshift e as consultas precisam ser rápidas. Os usuários devem poder criar seus próprios visuais.",
    "solution": ["redshift", "quicksight"],
    "availableServices": ["redshift", "quicksight", "athena", "s3", "emr"],
    "explanation": "O Amazon Redshift é um serviço de Data Warehouse em escala de petabytes. O Amazon QuickSight é um serviço de BI que se integra nativamente ao Redshift, permitindo a criação de painéis rápidos e interativos."
  },
  {
    "id": "aws-da-005",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Capture um fluxo contínuo de dados de logs de aplicações. Os dados devem ser armazenados de forma durável no S3 e, simultaneamente, processados para detectar anomalias em tempo real.",
    "solution": ["kinesis-firehose", "s3", "kinesis-analytics"],
    "availableServices": ["kinesis-firehose", "s3", "kinesis-analytics", "dms", "glue", "lambda"],
    "explanation": "O Kinesis Data Firehose é a maneira mais fácil de capturar e carregar fluxos de dados em destinos como o S3. Em paralelo, o Kinesis Data Analytics pode consumir esse fluxo e executar consultas SQL ou Flink para análise em tempo real, como detecção de anomalias."
  },
  {
    "id": "aws-da-006",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Orquestre um pipeline de dados complexo: um trabalho do Glue deve ser executado. Se for bem-sucedido, um procedimento no Redshift deve ser chamado. Se falhar, uma notificação deve ser enviada para um tópico SNS. O fluxo inteiro precisa de retentativas automáticas.",
    "solution": ["glue", "redshift", "sns", "step-functions"],
    "availableServices": ["glue", "redshift", "sns", "step-functions", "lambda", "cloudwatch", "mwaa"],
    "explanation": "O AWS Step Functions é um orquestrador visual serverless que permite construir fluxos de trabalho resilientes. Ele se integra nativamente com serviços como Glue, Redshift (via Data API) e SNS, e gerencia estados, erros e retentativas."
  },
  {
    "id": "aws-da-007",
    "category": "data-analysis",
    "difficulty": 5,
    "question": "Implemente um catálogo de dados centralizado para um Data Lake no S3. O catálogo deve descobrir automaticamente os esquemas dos dados e versioná-los, permitindo que o Athena e o Redshift Spectrum consultem os dados.",
    "solution": ["s3", "glue"],
    "availableServices": ["s3", "glue", "athena", "lake-formation", "dynamodb"],
    "explanation": "O AWS Glue Data Catalog atua como um metastore central para o Data Lake. Seus crawlers podem escanear dados no S3, inferir esquemas e popular o catálogo, que é então usado por serviços de consulta como Athena e Redshift Spectrum."
  },
  {
    "id": "aws-da-008",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Treine um modelo de machine learning para previsão de churn de clientes. Os dados estão no S3. Orquestre o pipeline de preparação de dados, treinamento do modelo e implantação do endpoint para inferência em tempo real.",
    "solution": ["s3", "sagemaker"],
    "availableServices": ["s3", "sagemaker", "glue", "lambda", "comprehend", "personalize"],
    "explanation": "O Amazon SageMaker é uma plataforma completa de ML. Ele pode usar dados do S3, possui notebooks e SDKs para preparação e treinamento de modelos, e facilita a implantação de endpoints de inferência com escalabilidade automática."
  },
  {
    "id": "aws-da-009",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Crie uma API REST que recebe dados JSON, os enriquece com uma pequena lógica de negócios e os insere em um banco de dados NoSQL de baixa latência. A solução deve ser totalmente serverless.",
    "solution": ["api-gateway", "lambda", "dynamodb"],
    "availableServices": ["api-gateway", "lambda", "dynamodb", "ec2", "rds", "sqs"],
    "explanation": "O API Gateway expõe o endpoint HTTP. A Lambda contém a lógica de negócios e é acionada pelas requisições. O DynamoDB é um banco de dados NoSQL gerenciado que oferece baixa latência para leituras e escritas, completando a arquitetura serverless."
  },
  {
    "id": "aws-da-010",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Centralize o gerenciamento de permissões de um Data Lake no S3. Você precisa conceder acesso a tabelas e colunas específicas para diferentes grupos de usuários que usam Athena para suas análises.",
    "solution": ["s3", "glue", "lake-formation"],
    "availableServices": ["s3", "glue", "lake-formation", "iam", "macie", "kms"],
    "explanation": "O AWS Lake Formation simplifica a segurança de Data Lakes. Ele permite gerenciar permissões de forma centralizada (em nível de tabela e coluna) para os dados no S3, integrando-se com o Glue Data Catalog e aplicando essas permissões em serviços como o Athena."
  },
  {
    "id": "aws-da-011",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Processe petabytes de dados não estruturados usando um framework customizado de Spark com bibliotecas específicas. A carga de trabalho é esporádica e o custo é uma grande preocupação. O cluster deve ser encerrado automaticamente após a conclusão.",
    "solution": ["s3", "emr"],
    "availableServices": ["s3", "emr", "glue", "redshift", "lambda"],
    "explanation": "O Amazon EMR (Elastic MapReduce) é ideal para processamento de big data com frameworks como Spark e Hadoop. Ele oferece total controle sobre o ambiente (permitindo bibliotecas customizadas) e suporta clusters transitórios (que se encerram automaticamente), otimizando custos."
  },
  {
    "id": "aws-da-012",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Ingira um grande volume de eventos de um sistema de mensagens Kafka on-premises para a AWS, de forma gerenciada e escalável, para posterior análise em tempo real.",
    "solution": ["msk"],
    "availableServices": ["msk", "kinesis", "sqs", "sns", "lambda"],
    "explanation": "O Amazon MSK (Managed Streaming for Kafka) é um serviço totalmente gerenciado para Apache Kafka. Ele é a escolha natural para migrar ou estender clusters Kafka existentes para a AWS, mantendo a compatibilidade com as APIs do Kafka."
  },
  {
    "id": "aws-da-013",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Indexe logs de aplicação que estão sendo enviados para o S3 para permitir pesquisa de texto completo e análise interativa de logs, como encontrar erros específicos ou visualizar a frequência de eventos.",
    "solution": ["s3", "opensearch"],
    "availableServices": ["s3", "opensearch", "athena", "redshift", "dynamodb", "cloudwatch"],
    "explanation": "O Amazon OpenSearch Service (sucessor do Elasticsearch Service) é projetado para casos de uso de pesquisa de texto completo e análise de logs. Ele pode ingerir dados do S3 e oferece painéis (via OpenSearch Dashboards) para visualização e exploração interativa."
  },
  {
    "id": "aws-da-014",
    "category": "data-analysis",
    "difficulty": 9,
    "question": "Gerencie e orquestre pipelines de dados complexos escritos em Python e SQL, usando um framework de código aberto popular. Os DAGs (Directed Acyclic Graphs) precisam ser executados em uma agenda e as dependências entre tarefas devem ser gerenciadas.",
    "solution": ["mwaa"],
    "availableServices": ["mwaa", "step-functions", "glue", "lambda", "eventbridge"],
    "explanation": "O Amazon MWAA (Managed Workflows for Apache Airflow) é um serviço gerenciado para o Apache Airflow. Ele é a escolha ideal para equipes que já usam ou querem usar Airflow para orquestração de pipelines complexos definidos como código (DAGs)."
  },
  {
    "id": "aws-da-015",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Colete dados de telemetria de milhões de dispositivos IoT. Os dados são séries temporais e precisam ser armazenados em um banco de dados otimizado para esse tipo de dado, para consultas de alta performance sobre janelas de tempo.",
    "solution": ["iot-core", "timestream"],
    "availableServices": ["iot-core", "timestream", "kinesis", "rds", "dynamodb", "s3"],
    "explanation": "O AWS IoT Core conecta os dispositivos com segurança. Para os dados, o Amazon Timestream é um banco de dados de séries temporais rápido, escalável e serverless, projetado especificamente para dados de IoT e operacionais, tornando as consultas por tempo muito eficientes."
  },
  {
    "id": "aws-da-016",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Analise uma grande quantidade de avaliações de clientes em texto para extrair entidades (como nomes de produtos), análise de sentimento (positivo/negativo) e tópicos principais, sem a necessidade de treinar um modelo de ML.",
    "solution": ["comprehend"],
    "availableServices": ["comprehend", "sagemaker", "lambda", "textract", "glue"],
    "explanation": "O Amazon Comprehend é um serviço de processamento de linguagem natural (NLP) que usa machine learning para encontrar insights em texto. Ele oferece APIs prontas para extração de entidades, sentimento e tópicos, sem exigir conhecimento em ML."
  },
  {
    "id": "aws-da-017",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Extraia texto e dados estruturados (tabelas, formulários) de milhões de documentos PDF e imagens armazenados no S3. A solução deve ser automatizada e escalável.",
    "solution": ["s3", "textract"],
    "availableServices": ["s3", "textract", "comprehend", "lambda", "sagemaker", "glue"],
    "explanation": "O Amazon Textract é um serviço de IA que extrai automaticamente texto e dados de documentos digitalizados, indo além do OCR simples para identificar conteúdo em tabelas e formulários. Ele se integra diretamente com o S3."
  },
  {
    "id": "aws-da-018",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Crie um sistema de recomendação de produtos para um site de e-commerce, sem precisar construir, treinar e implantar modelos de ML manualmente. O serviço deve usar os dados de interação do usuário.",
    "solution": ["personalize"],
    "availableServices": ["personalize", "sagemaker", "comprehend", "lambda", "redshift"],
    "explanation": "O Amazon Personalize é um serviço de IA totalmente gerenciado que simplifica a criação de sistemas de recomendação. Você fornece dados de interação do usuário e ele lida com todo o pipeline de ML para fornecer recomendações personalizadas."
  },
  {
    "id": "aws-da-019",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Uma aplicação web gera picos de eventos. Para evitar sobrecarregar o banco de dados, você precisa de um buffer para processar esses eventos de forma assíncrona com uma função. A ordem não é crítica, mas cada evento deve ser processado.",
    "solution": ["sqs", "lambda", "rds"],
    "availableServices": ["sqs", "lambda", "rds", "kinesis", "sns", "api-gateway"],
    "explanation": "O Amazon SQS (Simple Queue Service) é uma fila de mensagens perfeita para desacoplar componentes. Ele atua como um buffer, armazenando eventos de forma confiável. Uma função Lambda pode então processar as mensagens da fila a uma taxa controlada e gravar no banco de dados RDS."
  },
  {
    "id": "aws-da-020",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Quando um novo cliente é cadastrado em um banco de dados Aurora, um evento deve ser disparado para iniciar múltiplos fluxos de trabalho paralelos: um para enviar um e-mail de boas-vindas e outro para iniciar um pipeline de dados no Step Functions.",
    "solution": ["aurora", "eventbridge", ["lambda", "step-functions"]],
    "availableServices": ["aurora", "eventbridge", "lambda", "step-functions", "sns", "sqs"],
    "explanation": "O Amazon EventBridge é um barramento de eventos serverless que pode reagir a eventos de várias fontes. Ele pode rotear um único evento (novo cliente) para múltiplos destinos (Lambda para e-mail, Step Functions para fluxo de trabalho) para orquestrar arquiteturas desacopladas."
  },
  {
    "id": "aws-da-021",
    "category": "data-analysis",
    "difficulty": 9,
    "question": "Projete uma arquitetura de dados completa para uma nova aplicação. Os dados de um banco de dados Aurora devem ser replicados para um Data Lake (S3) e um Data Warehouse (Redshift). Um ETL com Spark deve processar os dados brutos no S3 e criar tabelas limpas. O acesso ao Data Lake deve ser governado centralmente. Analistas usarão SQL e uma ferramenta de BI.",
    "solution": ["aurora", "dms", [["s3", "glue", "athena", "lake-formation"], ["redshift", "quicksight"]]],
    "availableServices": ["aurora", "dms", "s3", "glue", "athena", "redshift", "quicksight", "lake-formation", "iam"],
    "explanation": "Esta é uma arquitetura de análise moderna completa. O DMS lida com a replicação. O caminho do Data Lake usa S3 para armazenamento, Glue para ETL e catálogo, Athena para consultas ad-hoc e Lake Formation para governança. O caminho do Data Warehouse usa Redshift para análise de performance e QuickSight para BI."
  },
  {
    "id": "aws-da-022",
    "category": "data-analysis",
    "difficulty": 4,
    "question": "Você precisa de um banco de dados relacional totalmente gerenciado para uma aplicação web. O serviço deve cuidar de backups, patching e failover automaticamente. Qual serviço é a base para essa necessidade?",
    "solution": ["rds"],
    "availableServices": ["rds", "aurora", "dynamodb", "ec2", "redshift"],
    "explanation": "O Amazon RDS (Relational Database Service) é o serviço fundamental para bancos de dados relacionais gerenciados na AWS. Ele automatiza tarefas de administração, permitindo que você se concentre na aplicação."
  },
  {
    "id": "aws-da-023",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Analise dados de redes sociais para entender relacionamentos, como 'usuário A é amigo do usuário B' e 'usuário B curtiu a postagem C'. Você precisa executar consultas para encontrar amigos em comum ou recomendações de conexão de forma eficiente.",
    "solution": ["neptune"],
    "availableServices": ["neptune", "rds", "dynamodb", "redshift", "opensearch"],
    "explanation": "O Amazon Neptune é um serviço de banco de dados de grafos gerenciado, otimizado para armazenar e consultar bilhões de relacionamentos. É ideal para casos de uso como redes sociais, detecção de fraudes e grafos de conhecimento."
  },
  {
    "id": "aws-da-024",
    "category": "data-analysis",
    "difficulty": 5,
    "question": "Armazene e gerencie com segurança as credenciais do banco de dados usadas por um trabalho do AWS Glue, evitando que elas sejam codificadas diretamente no script.",
    "solution": ["secrets-manager"],
    "availableServices": ["secrets-manager", "iam", "kms", "lambda", "glue"],
    "explanation": "O AWS Secrets Manager é o serviço ideal para armazenar, gerenciar e rotacionar segredos como senhas de banco de dados. O Glue pode se integrar nativamente ao Secrets Manager para recuperar credenciais de forma segura em tempo de execução."
  },
  {
    "id": "aws-da-025",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Crie um pipeline de CI/CD para um trabalho de ETL do AWS Glue. Quando o código for enviado para um repositório, o pipeline deve ser acionado para testar e implantar automaticamente o trabalho no ambiente de produção.",
    "solution": ["codecommit", "codepipeline", "glue"],
    "availableServices": ["codecommit", "codepipeline", "glue", "s3", "lambda", "cloudformation"],
    "explanation": "O AWS CodePipeline automatiza o processo de build, teste e deploy. Usado em conjunto com o CodeCommit (um repositório Git gerenciado), ele pode orquestrar a implantação de recursos da AWS, incluindo trabalhos do Glue, garantindo um ciclo de vida de desenvolvimento de software (SDLC) robusto."
  },
  {
    "id": "aws-da-026",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Uma aplicação precisa de um cache em memória para reduzir a latência de leitura em um banco de dados RDS, armazenando em cache os resultados de consultas frequentes.",
    "solution": ["rds", "elasticache"],
    "availableServices": ["rds", "elasticache", "dynamodb", "s3", "lambda"],
    "explanation": "O Amazon ElastiCache oferece caches em memória gerenciados (Redis ou Memcached). Ele é comumente usado na frente de bancos de dados como o RDS para armazenar dados acessados com frequência, melhorando drasticamente a performance e reduzindo a carga no banco de dados."
  },
  {
    "id": "aws-da-027",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Provisione uma arquitetura de análise completa (S3, Glue, Athena, Redshift) de forma repetível e automatizada, usando infraestrutura como código (IaC).",
    "solution": ["cloudformation"],
    "availableServices": ["cloudformation", "lambda", "ec2", "ecs", "codepipeline"],
    "explanation": "O AWS CloudFormation permite que você modele e provisione recursos da AWS usando templates (YAML ou JSON). É a principal ferramenta de IaC da AWS para criar, atualizar e gerenciar pilhas de recursos de forma consistente e previsível."
  },
  {
    "id": "aws-da-028",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Descubra e classifique automaticamente dados sensíveis (PII) armazenados em buckets do S3 no seu Data Lake para atender a requisitos de conformidade como GDPR ou LGPD.",
    "solution": ["s3", "macie"],
    "availableServices": ["s3", "macie", "glue", "kms", "lake-formation", "iam"],
    "explanation": "O Amazon Macie é um serviço de segurança de dados que usa machine learning para descobrir, classificar e proteger dados sensíveis na AWS. Ele monitora continuamente os buckets do S3 em busca de PII e outros dados confidenciais."
  },
  {
    "id": "aws-da-029",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Uma aplicação precisa garantir que os eventos em um fluxo sejam processados em ordem estrita (FIFO - First-In, First-Out) para cada cliente. Os eventos de clientes diferentes podem ser processados em paralelo.",
    "solution": ["kinesis"],
    "availableServices": ["kinesis", "sqs", "sns", "lambda", "eventbridge"],
    "explanation": "O Amazon Kinesis Data Streams garante a ordenação de registros dentro de um shard usando chaves de partição. Ao usar o ID do cliente como chave de partição, todos os eventos desse cliente irão para o mesmo shard e serão processados em ordem, atendendo ao requisito FIFO."
  },
  {
    "id": "aws-da-030",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Arquive terabytes de logs brutos do S3 que raramente são acessados, mas precisam ser mantidos por 7 anos por motivos de conformidade. A solução deve ter o menor custo de armazenamento possível.",
    "solution": ["s3", "glacier"],
    "availableServices": ["s3", "glacier", "rds", "redshift", "ebs"],
    "explanation": "As políticas de ciclo de vida do S3 podem transicionar objetos automaticamente para classes de armazenamento de menor custo. O S3 Glacier Deep Archive oferece o menor custo de armazenamento na nuvem e é projetado para retenção de dados a longo prazo, sendo ideal para conformidade."
  },
  {
    "id": "aws-da-031",
    "category": "data-analysis",
    "difficulty": 5,
    "question": "Um painel do QuickSight que consulta o Athena está lento. A maioria das consultas filtra por 'ano' e 'mês'. Como você pode otimizar a estrutura dos dados no S3 para acelerar as consultas e reduzir os custos?",
    "solution": ["s3", "glue", "athena"],
    "availableServices": ["s3", "glue", "athena", "redshift", "emr"],
    "explanation": "Particionar os dados no S3 em uma estrutura de pastas como 's3://bucket/ano=2025/mes=09/' e usar o Glue para catalogar essas partições permite que o Athena escaneie apenas os dados relevantes (poda de partição), melhorando drasticamente a performance e reduzindo os custos."
  },
  {
    "id": "aws-da-032",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Crie um banco de dados relacional de alta performance e disponibilidade, compatível com MySQL, que ofereça replicação automática em 3 zonas de disponibilidade (AZs).",
    "solution": ["aurora"],
    "availableServices": ["aurora", "rds", "redshift", "ec2"],
    "explanation": "O Amazon Aurora é um banco de dados relacional compatível com MySQL e PostgreSQL, construído para a nuvem. Ele oferece performance e disponibilidade superiores ao RDS padrão, com replicação de dados em 3 AZs por padrão."
  },
  {
    "id": "aws-da-033",
    "category": "data-analysis",
    "difficulty": 9,
    "question": "Você precisa processar um fluxo de dados em tempo real. Os dados brutos devem ser armazenados no S3. Uma versão dos dados deve ser enriquecida por uma função Lambda e enviada para o OpenSearch para análise de logs. Outra versão deve ser enviada para o Redshift para BI. A solução deve ser resiliente e gerenciada.",
    "solution": ["kinesis", "kinesis-firehose", [["s3"], ["lambda", "opensearch"], ["redshift"]]],
    "availableServices": ["kinesis", "kinesis-firehose", "s3", "lambda", "opensearch", "redshift", "glue"],
    "explanation": "O Kinesis Data Streams atua como o ponto de entrada. O Kinesis Data Firehose pode consumir esse fluxo e ter múltiplos destinos. Ele pode entregar os dados brutos diretamente para o S3, invocar uma função Lambda para transformação antes de entregar ao OpenSearch e carregar os dados diretamente no Redshift."
  },
  {
    "id": "aws-da-034",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Um sistema gera milhares de mensagens que precisam ser entregues a múltiplos sistemas de processamento independentes (fan-out). Por exemplo, um evento de 'novo pedido' deve ser enviado para os sistemas de faturamento, estoque e notificação simultaneamente.",
    "solution": ["sns", "sqs"],
    "availableServices": ["sns", "sqs", "kinesis", "eventbridge", "lambda"],
    "explanation": "O padrão 'fan-out' é um caso de uso clássico para o Amazon SNS (Simple Notification Service). Você publica uma mensagem em um tópico SNS, e o SNS a entrega para todos os assinantes. Usar filas SQS como assinantes torna a arquitetura resiliente, pois cada sistema pode processar a mensagem no seu próprio ritmo."
  },
  {
    "id": "aws-da-035",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Execute um contêiner Docker que contém um script de processamento de dados Python como uma tarefa serverless, sem precisar gerenciar servidores ou clusters.",
    "solution": ["ecs", "fargate"],
    "availableServices": ["ecs", "fargate", "ec2", "eks", "lambda"],
    "explanation": "O Amazon ECS (Elastic Container Service) é um orquestrador de contêineres. Usado com o AWS Fargate como tipo de lançamento, ele permite executar contêineres sem gerenciar a infraestrutura subjacente (servidores EC2), tornando-o uma solução serverless para cargas de trabalho em contêineres."
  },
  {
    "id": "aws-da-036",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Migre um banco de dados Cassandra on-premises para um serviço compatível com a API do Cassandra na AWS que seja serverless e totalmente gerenciado.",
    "solution": ["keyspaces"],
    "availableServices": ["keyspaces", "dynamodb", "rds", "neptune", "aurora"],
    "explanation": "O Amazon Keyspaces (for Apache Cassandra) é um serviço de banco de dados serverless, escalável e gerenciado, compatível com a API do Apache Cassandra. Ele permite migrar cargas de trabalho do Cassandra para a AWS sem gerenciar a infraestrutura."
  },
  {
    "id": "aws-da-037",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Monitore a execução de um trabalho do AWS Glue e crie um alarme que notifique uma equipe via e-mail se o trabalho falhar ou exceder um tempo de execução de 2 horas.",
    "solution": ["glue", "cloudwatch", "sns"],
    "availableServices": ["glue", "cloudwatch", "sns", "lambda", "eventbridge", "step-functions"],
    "explanation": "O AWS Glue emite métricas para o Amazon CloudWatch, como status do trabalho e duração. No CloudWatch, você pode criar alarmes com base nessas métricas. A ação do alarme pode ser a publicação em um tópico SNS, que por sua vez envia uma notificação por e-mail aos assinantes."
  },
  {
    "id": "aws-da-038",
    "category": "data-analysis",
    "difficulty": 4,
    "question": "Crie uma política de segurança para um novo analista de dados que conceda acesso de leitura (somente SELECT) a tabelas específicas em um banco de dados Redshift.",
    "solution": ["iam"],
    "availableServices": ["iam", "lake-formation", "kms", "secrets-manager"],
    "explanation": "O AWS IAM (Identity and Access Management) é o serviço central para gerenciar o acesso aos serviços e recursos da AWS. Você pode criar políticas do IAM que definem permissões granulares, como permitir ou negar ações específicas em recursos como clusters do Redshift."
  },
  {
    "id": "aws-da-039",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Um Data Lake no S3 contém dados com diferentes níveis de qualidade (bronze, silver, gold). Orquestre o pipeline que move os dados brutos (bronze) para dados limpos e particionados (silver) e, em seguida, para dados agregados para negócios (gold).",
    "solution": ["s3", "glue", "step-functions"],
    "availableServices": ["s3", "glue", "step-functions", "lambda", "mwaa", "athena"],
    "explanation": "Esta é a arquitetura 'Medallion'. O S3 armazena os dados em cada camada. O Glue executa os trabalhos de ETL de Spark para limpar e agregar os dados. O Step Functions é ideal para orquestrar essa sequência de trabalhos do Glue, garantindo que a camada 'silver' seja concluída antes que a 'gold' comece."
  },
  {
    "id": "aws-da-040",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Criptografe os dados em repouso em um bucket S3 usando uma chave de criptografia gerenciada pela sua empresa, com controle total sobre a política da chave e sua rotação.",
    "solution": ["s3", "kms"],
    "availableServices": ["s3", "kms", "iam", "macie", "secrets-manager"],
    "explanation": "O AWS KMS (Key Management Service) permite criar e gerenciar chaves de criptografia. Ao configurar a criptografia do lado do servidor (SSE) em um bucket S3, você pode especificar uma chave gerenciada pelo cliente (CMK) do KMS, dando a você controle total sobre o acesso e o uso da chave de criptografia."
  },
  {
    "id": "aws-da-041",
    "category": "data-analysis",
    "difficulty": 3,
    "question": "Qual serviço você usaria para hospedar uma instância virtual na nuvem para executar um script de análise de dados em Python que requer um ambiente operacional específico?",
    "solution": ["ec2"],
    "availableServices": ["ec2", "lambda", "ecs", "s3"],
    "explanation": "O Amazon EC2 (Elastic Compute Cloud) fornece capacidade computacional segura e redimensionável na nuvem. É o serviço fundamental para criar servidores virtuais (instâncias) onde você tem controle total sobre o sistema operacional e o software instalado."
  },
  {
    "id": "aws-da-042",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Você precisa de um banco de dados NoSQL chave-valor com latência de milissegundos de um dígito para uma aplicação de e-commerce que gerencia sessões de usuário e carrinhos de compras.",
    "solution": ["dynamodb"],
    "availableServices": ["dynamodb", "rds", "aurora", "elasticache", "s3"],
    "explanation": "O Amazon DynamoDB é um banco de dados NoSQL chave-valor totalmente gerenciado que oferece performance de baixa latência em qualquer escala. É ideal para casos de uso que exigem acesso rápido a dados, como perfis de usuário, carrinhos de compras e tabelas de classificação de jogos."
  },
  {
    "id": "aws-da-043",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Um trabalho de ETL no Glue precisa ler dados de uma fonte e aplicar uma transformação complexa que uma função Lambda não suportaria devido a limites de tempo de execução e memória. Qual é o serviço serverless mais apropriado para essa tarefa?",
    "solution": ["glue"],
    "availableServices": ["glue", "lambda", "emr", "fargate"],
    "explanation": "Enquanto a Lambda é ótima para transformações rápidas e leves, o AWS Glue é projetado para cargas de trabalho de ETL mais pesadas. Ele executa em um ambiente Apache Spark gerenciado, superando facilmente os limites de tempo e memória da Lambda para processamento de dados em larga escala."
  },
  {
    "id": "aws-da-044",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Execute um cluster Kubernetes gerenciado para implantar e escalar aplicações de análise de dados em contêineres, sem a necessidade de gerenciar o painel de controle do Kubernetes.",
    "solution": ["eks"],
    "availableServices": ["eks", "ecs", "fargate", "ec2"],
    "explanation": "O Amazon EKS (Elastic Kubernetes Service) é o serviço gerenciado da AWS para executar Kubernetes. Ele simplifica a implantação, o gerenciamento e a escalabilidade de aplicações em contêineres usando Kubernetes na AWS."
  },
  {
    "id": "aws-da-045",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Disponibilize um conjunto de dados do seu Data Lake no S3 para consumo por terceiros de forma segura, sem expor o bucket diretamente. A solução deve permitir a autenticação e autorização dos usuários.",
    "solution": ["s3", "api-gateway", "lambda", "cognito"],
    "availableServices": ["s3", "api-gateway", "lambda", "cognito", "cloudfront", "iam"],
    "explanation": "O Amazon Cognito gerencia a autenticação dos usuários. O API Gateway expõe um endpoint seguro que requer autenticação do Cognito. A Lambda, acionada pelo API Gateway, contém a lógica para buscar os dados solicitados do S3 e retorná-los, atuando como uma camada de segurança e controle."
  },
  {
    "id": "aws-da-046",
    "category": "data-analysis",
    "difficulty": 5,
    "question": "Você precisa de uma solução de armazenamento de objetos para hospedar um site estático (HTML, CSS, JS) e entregar seu conteúdo globalmente com baixa latência.",
    "solution": ["s3", "cloudfront"],
    "availableServices": ["s3", "cloudfront", "ec2", "elasticache"],
    "explanation": "O S3 pode ser configurado para hospedar um site estático. O Amazon CloudFront é uma rede de entrega de conteúdo (CDN) que armazena em cache o conteúdo do S3 em locais de borda ao redor do mundo, entregando-o aos usuários com a menor latência possível."
  },
  {
    "id": "aws-da-047",
    "category": "data-analysis",
    "difficulty": 9,
    "question": "Projete uma arquitetura para análise de clickstream em tempo real. Os cliques são enviados para um endpoint de API. Eles devem ser ingeridos, processados para calcular métricas de sessão em uma janela de 5 minutos, e os resultados devem ser armazenados em um banco de dados de baixa latência para alimentar um painel ao vivo.",
    "solution": ["api-gateway", "kinesis", "kinesis-analytics", "lambda", "dynamodb"],
    "availableServices": ["api-gateway", "kinesis", "kinesis-analytics", "lambda", "dynamodb", "s3", "athena"],
    "explanation": "O API Gateway recebe os cliques e os envia para o Kinesis Data Streams. O Kinesis Data Analytics executa uma consulta de janela de tempo contínua no fluxo para calcular as métricas. Uma função Lambda é usada como destino para os resultados da análise, que por sua vez os grava no DynamoDB. O painel ao vivo consulta o DynamoDB."
  },
  {
    "id": "aws-da-048",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Você tem um cluster Redshift com vários terabytes e precisa descarregar dados históricos para o S3 para reduzir custos, mas ainda quer poder consultá-los junto com os dados 'quentes' do cluster. Qual recurso do Redshift permite isso?",
    "solution": ["redshift", "s3"],
    "availableServices": ["redshift", "s3", "athena", "glue", "dms"],
    "explanation": "O Amazon Redshift Spectrum é um recurso do Redshift que permite executar consultas SQL em dados armazenados diretamente no seu Data Lake no S3. Isso permite estender suas análises do Redshift para exabytes de dados no S3 sem precisar carregá-los no cluster."
  },
  {
    "id": "aws-da-049",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Uma função Lambda precisa de mais de 15 minutos para processar um lote de arquivos. Qual serviço de orquestração é ideal para dividir o trabalho em etapas menores e gerenciar o fluxo de longa duração?",
    "solution": ["lambda", "step-functions"],
    "availableServices": ["lambda", "step-functions", "sqs", "eventbridge"],
    "explanation": "O AWS Step Functions é projetado para orquestrar fluxos de trabalho de longa duração. Você pode modelar seu processo como uma máquina de estados, onde cada estado pode invocar uma função Lambda para processar uma parte do trabalho, superando o limite de tempo de execução de uma única Lambda."
  },
  {
    "id": "aws-da-050",
    "category": "data-analysis",
    "difficulty": 4,
    "question": "Qual serviço você usaria para registrar um nome de domínio e configurar o roteamento de tráfego DNS para sua aplicação web de análise?",
    "solution": ["route 53"],
    "availableServices": ["route 53", "cloudfront", "ec2", "api-gateway"],
    "explanation": "O Amazon Route 53 é um serviço de sistema de nomes de domínio (DNS) web altamente disponível e escalável. Ele é usado para registrar domínios e rotear o tráfego da internet para os recursos da sua aplicação, como instâncias EC2 ou balanceadores de carga."
  },
  {
    "id": "aws-da-051",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Um pipeline de ETL noturno falhou. Você precisa analisar os logs de execução do Glue, os logs de acesso ao S3 e as métricas do CloudWatch para depurar e identificar a causa raiz do problema.",
    "solution": ["glue", "s3", "cloudwatch"],
    "availableServices": ["glue", "s3", "cloudwatch", "athena", "opensearch", "x-ray"],
    "explanation": "A depuração de pipelines envolve vários serviços. Os logs do trabalho do AWS Glue estão no CloudWatch Logs. Os logs de acesso ao S3 (se habilitados) mostram as solicitações feitas ao bucket. As métricas do CloudWatch para Glue e S3 podem revelar anomalias de performance ou erros."
  },
  {
    "id": "aws-da-052",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Você está construindo um Data Lake e precisa garantir que os dados no S3 sejam imutáveis, ou seja, uma vez escritos, não podem ser alterados ou excluídos por um período de tempo definido, para fins de auditoria.",
    "solution": ["s3"],
    "availableServices": ["s3", "iam", "glacier", "macie"],
    "explanation": "O S3 Object Lock é um recurso que permite implementar políticas de retenção de dados Write-Once-Read-Many (WORM). Isso impede que objetos sejam excluídos ou sobrescritos por um período fixo ou indefinidamente, garantindo a imutabilidade dos dados."
  },
  {
    "id": "aws-da-053",
    "category": "data-analysis",
    "difficulty": 9,
    "question": "Orquestre um pipeline de ML. Um arquivo de dados no S3 deve acionar um trabalho do Glue para pré-processamento. Em seguida, um trabalho de treinamento do SageMaker deve ser iniciado. Se o treinamento for bem-sucedido, o modelo deve ser implantado, caso contrário, uma notificação de falha deve ser enviada.",
    "solution": ["s3", "glue", "sagemaker", "sns", "step-functions"],
    "availableServices": ["s3", "glue", "sagemaker", "sns", "step-functions", "lambda", "mwaa"],
    "explanation": "O Step Functions se integra nativamente com serviços de ML e dados. Ele pode acionar trabalhos do Glue, iniciar e monitorar trabalhos de treinamento do SageMaker e tomar decisões com base no resultado (sucesso ou falha) para orquestrar o pipeline de ponta a ponta (MLOps)."
  },
  {
    "id": "aws-da-054",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Você precisa de uma forma de executar consultas SQL em um banco de dados DynamoDB para análises exploratórias, mesmo que o DynamoDB seja NoSQL.",
    "solution": ["dynamodb", "athena"],
    "availableServices": ["dynamodb", "athena", "glue", "dms", "redshift"],
    "explanation": "O Amazon Athena possui conectores de federação de dados que permitem executar consultas SQL em fontes de dados além do S3. Usando o conector do DynamoDB, o Athena pode consultar tabelas do DynamoDB diretamente, facilitando a análise de dados NoSQL com SQL padrão."
  },
  {
    "id": "aws-da-055",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Um pipeline de dados processa informações de clientes. Os dados precisam ser anonimizados antes de serem armazenados na camada 'gold' do Data Lake, removendo ou mascarando PII.",
    "solution": ["s3", "glue"],
    "availableServices": ["s3", "glue", "macie", "lambda", "comprehend"],
    "explanation": "O AWS Glue é uma ferramenta de ETL poderosa. Você pode usar suas transformações integradas ou escrever scripts Spark customizados para identificar e anonimizar (mascarar, remover, tokenizar) colunas contendo PII durante o processo de ETL entre as camadas do Data Lake."
  },
  {
    "id": "aws-da-056",
    "category": "data-analysis",
    "difficulty": 5,
    "question": "Como você pode acelerar o desempenho de um grande número de pequenas leituras de arquivos no Amazon EMR? Os arquivos estão armazenados no S3.",
    "solution": ["emr", "s3"],
    "availableServices": ["emr", "s3", "elasticache", "glue"],
    "explanation": "O EMRFS (EMR File System) oferece uma visualização consistente e otimizações de performance para o S3. Para cenários com muitos arquivos pequenos, recursos como o EMRFS S3-optimized committer e o uso de formatos de arquivo como Parquet (que é colunar) podem melhorar significativamente o desempenho."
  },
  {
    "id": "aws-da-057",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Um fluxo de eventos precisa ser consumido por duas equipes diferentes com ritmos de processamento distintos. Uma equipe precisa dos dados em tempo real, enquanto a outra processa em lotes a cada hora. Como você pode fornecer o mesmo fluxo para ambas sem que uma interfira na outra?",
    "solution": ["kinesis"],
    "availableServices": ["kinesis", "sqs", "sns", "msk"],
    "explanation": "O Kinesis Data Streams suporta 'Enhanced Fan-Out'. Isso permite que múltiplos consumidores (aplicativos) leiam o mesmo fluxo com sua própria taxa de transferência dedicada (2MB/s por consumidor), garantindo que o processamento lento de um consumidor não afete o processamento em tempo real de outro."
  },
  {
    "id": "aws-da-058",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Você tem um cluster Redshift que é usado intensivamente durante o horário comercial, mas fica ocioso à noite e nos fins de semana. Como você pode otimizar os custos desse cluster?",
    "solution": ["redshift"],
    "availableServices": ["redshift", "cloudwatch", "lambda", "scheduler"],
    "explanation": "O Redshift suporta pausa e retomada de clusters. Você pode automatizar esse processo usando um agendador (como o Amazon EventBridge Scheduler) para invocar uma função Lambda ou usar a API do Redshift para pausar o cluster fora do horário de pico e retomá-lo antes do início do expediente, economizando custos de computação."
  },
  {
    "id": "aws-da-059",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Você precisa consultar dados operacionais de um banco de dados RDS e dados históricos de um Data Lake no S3 em uma única consulta SQL.",
    "solution": ["rds", "s3", "athena"],
    "availableServices": ["rds", "s3", "athena", "dms", "redshift"],
    "explanation": "O Athena Federated Query permite que você execute consultas em fontes de dados externas. Usando o conector JDBC do Athena, você pode se conectar a um banco de dados RDS e executar uma consulta que une tabelas do RDS com tabelas do seu Data Lake no S3."
  },
  {
    "id": "aws-da-060",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Uma aplicação de jogos precisa de uma tabela de classificação global atualizada em tempo real. A solução deve ser altamente escalável e de baixa latência. Qual combinação de banco de dados e serviço de cache é a mais adequada?",
    "solution": ["dynamodb", "elasticache"],
    "availableServices": ["dynamodb", "elasticache", "rds", "aurora", "s3"],
    "explanation": "O DynamoDB é excelente para armazenar as pontuações. Para uma tabela de classificação em tempo real, o ElastiCache for Redis com sua estrutura de dados Sorted Set é extremamente eficiente e otimizado para essa tarefa, permitindo atualizações e recuperações de classificações com latência de microssegundos."
  },
  {
    "id": "aws-da-061",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Você precisa realizar uma análise de coorte para entender o comportamento do usuário ao longo do tempo. Os dados estão em um Data Warehouse no Redshift. Qual serviço de BI é mais adequado para criar visualizações de coorte e funil?",
    "solution": ["redshift", "quicksight"],
    "availableServices": ["redshift", "quicksight", "athena", "glue"],
    "explanation": "O Amazon QuickSight possui tipos de gráficos especializados, como análise de coorte e gráficos de funil, que são projetados para casos de uso de análise de comportamento do cliente. Ele se integra perfeitamente ao Redshift para obter os dados necessários."
  },
  {
    "id": "aws-da-062",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Um pipeline de dados precisa ser acionado pontualmente às 2h da manhã todos os dias para processar os dados do dia anterior. Qual serviço é o mais simples para agendar a execução de uma função Lambda ou um trabalho do Glue?",
    "solution": ["eventbridge"],
    "availableServices": ["eventbridge", "cloudwatch", "lambda", "step-functions"],
    "explanation": "O Amazon EventBridge (anteriormente CloudWatch Events) permite criar regras baseadas em agendamento (usando expressões cron) para acionar uma ampla variedade de alvos, incluindo funções Lambda, trabalhos do Glue e máquinas de estado do Step Functions, sendo a solução ideal para agendamento."
  },
  {
    "id": "aws-da-063",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Você está migrando um Data Warehouse on-premises de 500TB para a nuvem. O link de internet é lento e a transferência levaria meses. Qual serviço físico você usaria para transferir essa grande quantidade de dados para a AWS de forma eficiente?",
    "solution": ["s3"],
    "availableServices": ["s3", "dms", "storage gateway", "snowball"],
    "explanation": "O AWS Snowball é um serviço de transporte de dados em escala de petabytes que usa dispositivos físicos seguros para transferir grandes quantidades de dados para dentro e fora da AWS. Para uma migração única de 500TB, é muito mais rápido e econômico do que transferir pela internet. (Nota: Snowball é o serviço, mas o destino final é o S3)."
  },
  {
    "id": "aws-da-064",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Uma API precisa retornar dados que são o resultado de uma junção complexa e de longa duração em um banco de dados Aurora. Como você pode melhorar a performance da API para que ela não execute a consulta a cada chamada?",
    "solution": ["aurora", "lambda", "s3"],
    "availableServices": ["aurora", "lambda", "s3", "elasticache", "athena"],
    "explanation": "Este é um padrão de materialização de visualização. Você pode ter um processo agendado (Lambda + EventBridge) que executa a consulta demorada no Aurora e salva o resultado como um arquivo (por exemplo, JSON ou Parquet) no S3. A API principal então lê diretamente do arquivo no S3, que é muito mais rápido."
  },
  {
    "id": "aws-da-065",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Você precisa garantir que apenas usuários de dentro da sua rede corporativa (VPC) possam consultar dados no seu Data Lake usando o Athena, bloqueando todo o acesso da internet pública.",
    "solution": ["s3", "athena", "vpc"],
    "availableServices": ["s3", "athena", "vpc", "iam", "security group"],
    "explanation": "Configurando um VPC Endpoint para o S3 e para o Glue, e executando as consultas do Athena a partir de recursos dentro da VPC, você pode garantir que o tráfego entre o Athena e seus dados não saia da rede da AWS. Políticas de bucket do S3 podem restringir o acesso apenas a partir do VPC Endpoint."
  },
  {
    "id": "aws-da-066",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Um processo de ingestão cria milhares de arquivos pequenos (KBs) por hora no S3. Isso está causando problemas de performance para as consultas do Athena. Qual é a melhor estratégia para resolver isso?",
    "solution": ["s3", "glue"],
    "availableServices": ["s3", "glue", "lambda", "kinesis-firehose"],
    "explanation": "O problema dos 'arquivos pequenos' é comum. A melhor solução é um trabalho de compactação. Um trabalho do AWS Glue agendado pode ler os arquivos pequenos, compactá-los em arquivos maiores e otimizados (por exemplo, Parquet de 128MB) e reescrevê-los, melhorando drasticamente a performance de leitura do Athena."
  },
  {
    "id": "aws-da-067",
    "category": "data-analysis",
    "difficulty": 9,
    "question": "Você tem um cluster Redshift usado por vários departamentos. Você precisa isolar as cargas de trabalho para que as consultas de BI de um departamento não afetem o desempenho dos trabalhos de ETL de outro, além de gerenciar os custos por departamento.",
    "solution": ["redshift"],
    "availableServices": ["redshift", "iam", "cloudwatch", "lake-formation"],
    "explanation": "O Amazon Redshift Concurrency Scaling permite adicionar capacidade de cluster de forma automática e temporária para lidar com picos de consultas. Além disso, o Workload Management (WLM) permite definir filas para diferentes tipos de consultas (ETL vs. BI) para priorizar e isolar as cargas de trabalho. O uso de tags de alocação de custos pode ajudar a rastrear os custos por departamento."
  },
  {
    "id": "aws-da-068",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Você precisa fornecer uma interface de notebook Jupyter gerenciada e colaborativa para uma equipe de cientistas de dados explorar dados no S3 e treinar modelos de ML.",
    "solution": ["sagemaker"],
    "availableServices": ["sagemaker", "ec2", "glue", "emr"],
    "explanation": "O Amazon SageMaker Studio fornece um IDE completo baseado na web para machine learning, que inclui instâncias de notebook Jupyter gerenciadas. Ele simplifica a colaboração, o controle de versão e o acesso aos dados e recursos de computação da AWS."
  },
  {
    "id": "aws-da-069",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Crie um pipeline que ingere dados JSON de uma fila SQS, converte-os para Parquet em micro-lotes e os entrega ao S3 a cada 5 minutos.",
    "solution": ["sqs", "lambda", "kinesis-firehose", "s3"],
    "availableServices": ["sqs", "lambda", "kinesis-firehose", "s3", "glue"],
    "explanation": "Uma função Lambda pode ser acionada em lotes pela fila SQS. A Lambda então encaminha os dados para o Kinesis Data Firehose, que é otimizado para agrupar (buffer) os dados e entregá-los ao S3 em arquivos maiores e em um formato colunar como o Parquet, resolvendo o problema dos 'arquivos pequenos'."
  },
  {
    "id": "aws-da-070",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Você está implementando um pipeline de Change Data Capture (CDC) do RDS para o S3. Como você pode mesclar os eventos de INSERÇÃO, ATUALIZAÇÃO e EXCLUSÃO em uma tabela final consistente no Data Lake que represente o estado atual dos dados?",
    "solution": ["rds", "dms", "s3", "glue"],
    "availableServices": ["rds", "dms", "s3", "glue", "emr", "lambda"],
    "explanation": "O DMS captura os eventos CDC e os grava no S3. Um trabalho do AWS Glue (ou EMR com frameworks como Hudi/Iceberg/Delta Lake) pode então ler esses eventos e executar uma operação de 'MERGE' (ou 'UPSERT') na tabela de destino no S3, aplicando as inserções, atualizações e exclusões para manter uma visão atualizada."
  },
  {
    "id": "aws-da-071",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Você precisa compartilhar um grande conjunto de dados (até 100GB) de forma segura com um parceiro externo que também tem uma conta AWS, sem copiar os dados.",
    "solution": ["s3", "iam"],
    "availableServices": ["s3", "iam", "dms", "data exchange"],
    "explanation": "A maneira mais eficiente e segura é usar políticas de bucket do S3 e perfis do IAM (IAM Roles). Você pode criar uma política que concede a um perfil na conta do parceiro acesso de leitura (cross-account access) ao seu bucket, permitindo que eles acessem os dados diretamente sem duplicação."
  },
  {
    "id": "aws-da-072",
    "category": "data-analysis",
    "difficulty": 9,
    "question": "Um serviço de detecção de fraudes precisa processar transações em menos de 100 milissegundos. O processo envolve enriquecer a transação com dados do perfil do usuário (em um banco NoSQL) e com dados agregados históricos (em um Data Warehouse). Qual arquitetura atende a esse requisito de baixa latência?",
    "solution": ["kinesis", "lambda", "dynamodb", "elasticache"],
    "availableServices": ["kinesis", "lambda", "dynamodb", "elasticache", "redshift", "aurora"],
    "explanation": "A latência é crítica. O Redshift é muito lento para isso. O padrão é usar o Kinesis para ingerir a transação, acionando uma Lambda. A Lambda busca o perfil do usuário no DynamoDB (baixa latência). Os agregados históricos do Redshift devem ser pré-calculados e carregados no ElastiCache (cache em memória) para busca rápida pela Lambda."
  },
  {
    "id": "aws-da-073",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Você precisa criar uma cópia de um banco de dados RDS de produção para um ambiente de teste, garantindo que nenhum dado sensível do cliente seja exposto aos desenvolvedores.",
    "solution": ["rds", "glue"],
    "availableServices": ["rds", "glue", "dms", "lambda"],
    "explanation": "A melhor prática é restaurar um snapshot do RDS de produção em uma nova instância. Em seguida, execute um trabalho do AWS Glue ou um script customizado na nova instância para mascarar ou anonimizar os dados sensíveis antes de liberar o acesso aos desenvolvedores."
  },
  {
    "id": "aws-da-074",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Como você pode consultar dados em um formato de arquivo customizado ou pouco comum armazenado no S3 usando SQL padrão, sem precisar primeiro convertê-los para um formato como Parquet ou JSON?",
    "solution": ["s3", "athena", "lambda"],
    "availableServices": ["s3", "athena", "lambda", "glue", "emr"],
    "explanation": "O Amazon Athena suporta a criação de 'Source Connectors' usando o AWS Lambda. Você pode escrever uma função Lambda que entende como ler e desserializar seu formato de arquivo customizado, permitindo que o Athena a utilize para consultar os dados diretamente."
  },
  {
    "id": "aws-da-075",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Você precisa fornecer acesso de BI a um Data Lake governado pelo Lake Formation para usuários que estão usando o Active Directory da sua empresa para autenticação.",
    "solution": ["lake-formation", "quicksight", "iam"],
    "availableServices": ["lake-formation", "quicksight", "iam", "cognito", "active directory connector"],
    "explanation": "O QuickSight pode se integrar com o Active Directory (via AD Connector ou IAM Identity Center). Você pode mapear grupos do AD para perfis do IAM. No Lake Formation, você concede permissões a esses perfis do IAM, permitindo que os usuários acessem os dados corretos no QuickSight com suas credenciais corporativas."
  },
  {
    "id": "aws-da-076",
    "category": "data-analysis",
    "difficulty": 5,
    "question": "Um script Python que é executado em uma instância EC2 precisa de permissão para ler arquivos de um bucket S3. Qual é a maneira mais segura de conceder essa permissão sem usar chaves de acesso de longo prazo?",
    "solution": ["ec2", "s3", "iam"],
    "availableServices": ["ec2", "s3", "iam", "secrets-manager", "kms"],
    "explanation": "A maneira mais segura é usar um 'IAM Role for EC2'. Você cria um perfil com as permissões necessárias para acessar o S3 e o anexa à instância EC2. A instância então obtém credenciais temporárias automaticamente, evitando a necessidade de armazenar chaves de acesso no código ou na instância."
  },
  {
    "id": "aws-da-077",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Um processo de ETL precisa ser executado somente após a conclusão bem-sucedida de três outros trabalhos de ingestão que rodam em paralelo e têm durações variáveis. Como você pode orquestrar essa dependência?",
    "solution": ["glue", "step-functions"],
    "availableServices": ["glue", "step-functions", "eventbridge", "lambda", "mwaa"],
    "explanation": "O AWS Step Functions é perfeito para gerenciar fluxos de trabalho com paralelismo e dependências. Você pode usar um estado 'Parallel' para executar os três trabalhos de ingestão simultaneamente e, em seguida, transicionar para o trabalho de ETL somente após todos os três ramos paralelos serem concluídos com sucesso."
  },
  {
    "id": "aws-da-078",
    "category": "data-analysis",
    "difficulty": 9,
    "question": "Você precisa de uma solução de Data Warehouse que possa escalar a computação e o armazenamento de forma independente. Além disso, ela deve permitir o compartilhamento de dados ao vivo com outras contas AWS sem copiar os dados.",
    "solution": ["redshift"],
    "availableServices": ["redshift", "aurora", "s3", "athena"],
    "explanation": "O tipo de nó RA3 do Amazon Redshift, com armazenamento gerenciado, desacopla a computação e o armazenamento, permitindo o escalonamento independente. O recurso 'Redshift Data Sharing' permite o compartilhamento de dados ao vivo e transacionalmente consistentes entre clusters do Redshift em diferentes contas AWS."
  },
  {
    "id": "aws-da-079",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Um pipeline de streaming processa dados financeiros e precisa garantir que cada evento seja processado 'exatamente uma vez' (exactly-once processing), mesmo em caso de falhas e reinicializações.",
    "solution": ["kinesis", "flink", "kinesis-analytics"],
    "availableServices": ["kinesis", "kinesis-analytics", "lambda", "sqs"],
    "explanation": "Para garantir o processamento 'exatamente uma vez', você precisa de um framework que suporte transações e checkpoints. O Kinesis Data Analytics for Apache Flink oferece esse recurso. O Flink pode criar checkpoints consistentes do seu estado no S3, permitindo a recuperação de falhas sem perda de dados ou duplicação."
  },
  {
    "id": "aws-da-080",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Você precisa criar um painel de controle de custos da AWS que detalhe os gastos por serviço, por tag de projeto e por conta vinculada, usando os dados do AWS Cost and Usage Report (CUR).",
    "solution": ["s3", "athena", "quicksight"],
    "availableServices": ["s3", "athena", "quicksight", "glue", "cost explorer"],
    "explanation": "O AWS CUR entrega relatórios detalhados de custos para um bucket S3. O AWS Glue pode rastrear esses dados para criar uma tabela no catálogo. O Athena pode então consultar esses dados detalhados. Finalmente, o QuickSight pode se conectar ao Athena para criar painéis de análise de custos (FinOps) customizados e poderosos."
  },
  {
    "id": "aws-da-081",
    "category": "data-analysis",
    "difficulty": 4,
    "question": "Qual é a maneira mais simples de carregar um arquivo CSV de 50MB do seu computador para um bucket S3?",
    "solution": ["s3"],
    "availableServices": ["s3", "dms", "glue", "lambda"],
    "explanation": "Para uma carga única e simples de um arquivo, a maneira mais direta é usar o Console de Gerenciamento da AWS, a AWS Command Line Interface (CLI) com o comando `aws s3 cp`, ou um dos SDKs da AWS. Nenhum outro serviço de ingestão complexo é necessário."
  },
  {
    "id": "aws-da-082",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Uma consulta do Athena em uma tabela com 500 colunas está lenta porque sempre seleciona apenas 5 colunas. Qual formato de arquivo no S3 resolveria melhor esse problema de performance?",
    "solution": ["s3", "glue"],
    "availableServices": ["s3", "glue", "json", "csv", "parquet"],
    "explanation": "Formatos colunares como o Apache Parquet ou o ORC são ideais para isso. Eles armazenam dados por coluna, não por linha. Isso permite que o Athena leia apenas os dados das 5 colunas necessárias, em vez de escanear todas as 500 colunas de cada linha, resultando em uma melhoria massiva de performance e redução de custos."
  },
  {
    "id": "aws-da-083",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Você precisa testar uma alteração em um trabalho complexo do AWS Glue que lê de múltiplas fontes e escreve em vários destinos. Como você pode criar um ambiente de desenvolvimento interativo para depurar o script Spark?",
    "solution": ["glue"],
    "availableServices": ["glue", "sagemaker", "lambda", "emr"],
    "explanation": "O AWS Glue oferece 'Interactive Sessions' e 'Development Endpoints'. Ambos permitem que você execute e depure interativamente seu script de ETL a partir de um notebook (como Jupyter ou Zeppelin), conectando-se a um ambiente Spark gerenciado pelo Glue, acelerando o ciclo de desenvolvimento."
  },
  {
    "id": "aws-da-084",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Sua empresa quer vender acesso a conjuntos de dados curados para clientes externos. Você precisa de uma plataforma para publicar, licenciar e fornecer acesso a esses dados de forma segura e gerenciada.",
    "solution": ["s3", "data exchange"],
    "availableServices": ["s3", "data exchange", "api-gateway", "lake-formation"],
    "explanation": "O AWS Data Exchange é um serviço que facilita encontrar, assinar e usar dados de terceiros na nuvem. Como provedor, você pode usá-lo para licenciar e fornecer seus conjuntos de dados (armazenados no S3) para assinantes, que podem então usá-los em seus próprios ambientes AWS."
  },
  {
    "id": "aws-da-085",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Um banco de dados Aurora está sobrecarregado com consultas analíticas complexas que afetam o desempenho da aplicação principal. Qual é a melhor maneira de isolar essas cargas de trabalho de leitura?",
    "solution": ["aurora"],
    "availableServices": ["aurora", "rds", "redshift", "dms"],
    "explanation": "O Amazon Aurora suporta a criação de 'Réplicas de Leitura' (Read Replicas). Você pode criar uma ou mais réplicas que se mantêm sincronizadas com a instância principal e direcionar todas as consultas analíticas para essas réplicas, isolando a carga de trabalho de leitura da carga de trabalho de escrita principal."
  },
  {
    "id": "aws-da-086",
    "category": "data-analysis",
    "difficulty": 9,
    "question": "Um pipeline de dados precisa processar arquivos que chegam em um bucket S3, mas as tarefas de processamento podem levar até 30 minutos e exigem 10GB de RAM. As chegadas de arquivos são imprevisíveis. A solução deve ser serverless e econômica.",
    "solution": ["s3", "lambda", "fargate"],
    "availableServices": ["s3", "lambda", "fargate", "glue", "emr", "ec2"],
    "explanation": "A Lambda tem um limite de 15 minutos e 10GB de RAM, tornando-a inadequada. O Glue pode ser caro para tarefas curtas e imprevisíveis. Uma arquitetura comum é usar um evento do S3 para acionar uma Lambda, que atua como 'despachante'. A Lambda então inicia uma tarefa do ECS no Fargate, que pode executar a carga de trabalho pesada em um contêiner pelo tempo necessário, de forma serverless."
  },
  {
    "id": "aws-da-087",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Você precisa criar uma tabela no Glue Data Catalog que aponte para dados armazenados no Google Cloud Storage, para que possam ser consultados pelo Athena.",
    "solution": ["glue", "athena"],
    "availableServices": ["glue", "athena", "s3", "dms"],
    "explanation": "O AWS Glue suporta a criação de 'Custom Connectors' que podem se conectar a fontes de dados externas. Usando um conector para o Google Cloud Storage (GCS), você pode catalogar os dados no Glue e, em seguida, usar o Athena Federated Query para consultá-los como se estivessem na AWS."
  },
  {
    "id": "aws-da-088",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Uma aplicação precisa de uma visão agregada dos dados de vendas do último minuto, atualizada a cada segundo. Os dados brutos de vendas estão em um fluxo Kinesis.",
    "solution": ["kinesis", "kinesis-analytics"],
    "availableServices": ["kinesis", "kinesis-analytics", "lambda", "athena"],
    "explanation": "O Kinesis Data Analytics é ideal para agregações contínuas em janelas de tempo. Você pode usar uma consulta SQL com uma 'Tumbling Window' de 1 minuto para calcular as métricas. A consulta emitirá resultados atualizados continuamente, atendendo ao requisito de tempo real."
  },
  {
    "id": "aws-da-089",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Um pipeline de dados que usa o MWAA (Airflow) precisa acessar um banco de dados RDS. Como você armazena e acessa as credenciais do banco de dados de forma segura nos seus DAGs?",
    "solution": ["mwaa", "secrets-manager"],
    "availableServices": ["mwaa", "secrets-manager", "iam", "kms"],
    "explanation": "O MWAA se integra com o AWS Secrets Manager. Você pode configurar seu ambiente MWAA para usar o Secrets Manager como seu 'secrets backend'. Isso permite que você armazene as credenciais no Secrets Manager e as referencie de forma segura nas suas conexões do Airflow, sem expô-las nos DAGs."
  },
  {
    "id": "aws-da-090",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Você precisa criar um 'feature store' centralizado para cientistas de dados, onde features de ML pré-calculadas possam ser armazenadas, descobertas e servidas com baixa latência para treinamento e inferência de modelos.",
    "solution": ["sagemaker"],
    "availableServices": ["sagemaker", "s3", "dynamodb", "elasticache"],
    "explanation": "O Amazon SageMaker Feature Store é um serviço construído especificamente para esse propósito. Ele fornece um repositório para armazenar, atualizar, recuperar e compartilhar features de ML, com armazenamento online (baixa latência) e offline (análise em lote) integrados."
  },
  {
    "id": "aws-da-091",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Um cluster EMR precisa acessar dados em um bucket S3 que usa criptografia do lado do servidor com chaves KMS (SSE-KMS). O que é necessário para garantir que o EMR possa descriptografar os dados?",
    "solution": ["emr", "s3", "kms", "iam"],
    "availableServices": ["emr", "s3", "kms", "iam", "secrets-manager"],
    "explanation": "O perfil do IAM (IAM Role) associado às instâncias EC2 do cluster EMR precisa de permissões explícitas na política de chave do KMS para realizar a ação 'kms:Decrypt'. Sem essa permissão, o EMR não conseguirá ler os arquivos criptografados no S3."
  },
  {
    "id": "aws-da-092",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Como você pode carregar dados de um bucket S3 para uma tabela no Redshift de forma incremental, processando apenas os arquivos que chegaram desde a última execução?",
    "solution": ["s3", "redshift", "glue"],
    "availableServices": ["s3", "redshift", "glue", "dms", "lambda"],
    "explanation": "O AWS Glue suporta 'job bookmarks'. Quando ativados, o Glue rastreia os arquivos que já foram processados. Em execuções subsequentes do trabalho, ele processará automaticamente apenas os arquivos novos, simplificando a lógica de cargas incrementais."
  },
  {
    "id": "aws-da-093",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Um modelo de ML implantado em um endpoint do SageMaker precisa ser monitorado para detectar desvios (drift) nos dados de entrada e na qualidade do modelo ao longo do tempo.",
    "solution": ["sagemaker"],
    "availableServices": ["sagemaker", "cloudwatch", "lambda", "athena"],
    "explanation": "O Amazon SageMaker Model Monitor monitora automaticamente os modelos de machine learning em produção. Ele captura os dados de entrada, previsões e metadados, e os compara com uma linha de base para detectar desvios de dados e conceito, enviando alertas se forem encontrados problemas."
  },
  {
    "id": "aws-da-094",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Crie uma arquitetura que permita a analistas de dados consultarem dados de log que estão em um bucket S3, logs de métricas que estão no CloudWatch Logs e dados de negócios que estão no RDS, tudo a partir de uma única interface SQL.",
    "solution": ["s3", "cloudwatch", "rds", "athena"],
    "availableServices": ["s3", "cloudwatch", "rds", "athena", "glue", "opensearch"],
    "explanation": "O Amazon Athena, com seus conectores de consulta federada, é a solução. Ele pode consultar nativamente os dados no S3 (via Glue Catalog) e usar conectores para consultar o CloudWatch Logs e o RDS (via JDBC) na mesma consulta, unificando a experiência de análise."
  },
  {
    "id": "aws-da-095",
    "category": "data-analysis",
    "difficulty": 9,
    "question": "Você precisa construir um pipeline de dados que seja resiliente a falhas de uma região inteira da AWS. Como você projetaria a ingestão e o armazenamento de dados para garantir a continuidade dos negócios?",
    "solution": ["s3", "kinesis", "dynamodb", "route 53"],
    "availableServices": ["s3", "kinesis", "dynamodb", "route 53", "rds", "redshift"],
    "explanation": "Para resiliência multi-região, use serviços com recursos de replicação. O S3 pode usar a Replicação Cross-Region (CRR). O DynamoDB pode usar as Tabelas Globais. Para ingestão, você pode ter streams do Kinesis em duas regiões e usar o Route 53 com verificações de saúde para direcionar o tráfego para a região ativa."
  },
  {
    "id": "aws-da-096",
    "category": "data-analysis",
    "difficulty": 6,
    "question": "Você quer criar um dashboard no QuickSight, mas sua fonte de dados é um arquivo Excel grande e complexo que está em uma pasta compartilhada. Qual é a maneira mais direta de usar esses dados no QuickSight?",
    "solution": ["s3", "quicksight"],
    "availableServices": ["s3", "quicksight", "glue", "athena"],
    "explanation": "O QuickSight pode se conectar a várias fontes, mas a maneira mais simples para arquivos é carregá-los primeiro no S3. A partir daí, o QuickSight pode ingerir os dados do S3 diretamente para o seu mecanismo de análise em memória, o SPICE, ou consultá-los via Athena."
  },
  {
    "id": "aws-da-097",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Um trabalho de ETL em lote é executado diariamente e os resultados devem estar disponíveis para consulta no Athena. Como você pode garantir que as consultas no Athena não vejam dados parciais enquanto o trabalho de ETL está em execução?",
    "solution": ["s3", "glue", "athena"],
    "availableServices": ["s3", "glue", "athena", "lambda", "step-functions"],
    "explanation": "A melhor prática é fazer com que o trabalho de ETL escreva os novos dados em um local temporário no S3. Somente após a escrita ser concluída com sucesso, o trabalho deve atualizar o catálogo de dados do Glue (por exemplo, adicionando a nova partição) para apontar para os novos dados. Isso garante que a atualização da tabela seja atômica."
  },
  {
    "id": "aws-da-098",
    "category": "data-analysis",
    "difficulty": 8,
    "question": "Um painel de BI de alta visibilidade precisa de tempos de resposta de consulta abaixo de um segundo, mas a fonte de dados subjacente é um Data Lake no S3 que é lento para consultas complexas. Como você pode acelerar as consultas do painel?",
    "solution": ["s3", "athena", "quicksight"],
    "availableServices": ["s3", "athena", "quicksight", "redshift", "elasticache"],
    "explanation": "O Amazon QuickSight possui um mecanismo de cache em memória chamado SPICE. Você pode configurar seu conjunto de dados para importar os dados do Athena/S3 para o SPICE em uma programação. Todas as consultas do painel serão então executadas contra o SPICE, que é extremamente rápido, em vez do Data Lake."
  },
  {
    "id": "aws-da-099",
    "category": "data-analysis",
    "difficulty": 10,
    "question": "Projete uma arquitetura de 'Lambda' para análise de dados, onde um caminho de 'velocidade' processa dados em tempo real para painéis ao vivo e um caminho de 'lote' processa os mesmos dados para análises históricas complexas e precisas. Os resultados de ambos os caminhos devem ser acessíveis de forma unificada.",
    "solution": [["kinesis", "lambda", "dynamodb"], ["s3", "glue", "redshift"], ["quicksight"]],
    "availableServices": ["kinesis", "lambda", "dynamodb", "s3", "glue", "redshift", "quicksight", "athena"],
    "explanation": "Esta é a arquitetura Lambda clássica. O caminho de velocidade (camada rápida) usa Kinesis -> Lambda -> DynamoDB para resultados em tempo real. O caminho de lote (camada lenta) armazena os dados brutos no S3 e usa Glue/EMR para processá-los e carregá-los no Redshift para análises abrangentes. Uma ferramenta de BI como o QuickSight pode então unificar a visualização, consultando tanto o DynamoDB quanto o Redshift."
  },
  {
    "id": "aws-da-100",
    "category": "data-analysis",
    "difficulty": 7,
    "question": "Você precisa que as consultas do Athena em um Data Lake no S3 apliquem permissões de acesso em nível de coluna, ocultando colunas com PII de certos grupos de usuários.",
    "solution": ["s3", "glue", "lake-formation", "athena"],
    "availableServices": ["s3", "glue", "lake-formation", "athena", "iam", "macie"],
    "explanation": "O AWS Lake Formation é o serviço projetado para isso. Ele se integra ao Glue Data Catalog e permite definir permissões granulares, incluindo acesso em nível de tabela e coluna, para perfis do IAM. Quando um usuário consulta via Athena, o Lake Formation impõe essas permissões, filtrando as colunas que o usuário não tem permissão para ver."
  },
{
    "id": "aws-co-001",
    "category": "cloud-operations",
    "difficulty": 3,
    "question": "Uma instância EC2 está com alta utilização de CPU. Você precisa ser notificado automaticamente por e-mail quando a CPU exceder 80% por 5 minutos consecutivos. Qual a combinação de serviços mais simples para criar este alerta?",
    "solution": ["ec2", "cloudwatch", "sns"],
    "availableServices": ["ec2", "cloudwatch", "sns", "cloudtrail", "lambda", "eventbridge"],
    "explanation": "O CloudWatch monitora as métricas da EC2, como CPUUtilization. Um Alarme do CloudWatch pode ser criado com base nessa métrica. A ação do alarme pode ser publicar uma mensagem em um tópico SNS, que por sua vez envia uma notificação por e-mail aos assinantes."
  },
  {
    "id": "aws-co-002",
    "category": "cloud-operations",
    "difficulty": 5,
    "question": "Você precisa registrar todas as chamadas de API feitas na sua conta AWS para fins de auditoria de segurança e conformidade. Onde esses registros devem ser centralizados e armazenados de forma durável?",
    "solution": ["cloudtrail", "s3"],
    "availableServices": ["cloudtrail", "s3", "cloudwatch", "iam", "opensearch"],
    "explanation": "O AWS CloudTrail registra a atividade do usuário e as chamadas de API como eventos. As trilhas do CloudTrail podem ser configuradas para entregar esses arquivos de log a um bucket S3 para armazenamento de longo prazo e análise de auditoria."
  },
  {
    "id": "aws-co-003",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Você precisa automatizar a aplicação de patches de segurança em um grande parque de instâncias EC2 com Windows Server, de acordo com uma janela de manutenção agendada. Qual serviço é projetado para essa tarefa?",
    "solution": ["systems manager"],
    "availableServices": ["systems manager", "ec2", "lambda", "cloudformation", "inspector"],
    "explanation": "O AWS Systems Manager, especificamente seu componente Patch Manager, foi projetado para automatizar o processo de aplicação de patches em sistemas operacionais em escala. Ele permite agendar janelas de manutenção e aplicar patches com base em regras predefinidas."
  },
  {
    "id": "aws-co-004",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Provisione uma infraestrutura de aplicação web completa (VPC, Subnets, EC2, Load Balancer) de forma automatizada e repetível. As alterações na infraestrutura devem ser versionadas e gerenciadas como código.",
    "solution": ["cloudformation"],
    "availableServices": ["cloudformation", "ec2", "ecs", "lambda", "codepipeline"],
    "explanation": "O AWS CloudFormation é o serviço de Infraestrutura como Código (IaC) da AWS. Ele permite que você defina e provisione todos os recursos de infraestrutura em um arquivo de template, garantindo implantações consistentes e permitindo o versionamento da sua arquitetura."
  },
  {
    "id": "aws-co-005",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Crie um pipeline de CI/CD completo para uma aplicação em contêineres. O código deve ser armazenado em um repositório, uma imagem Docker deve ser construída e, se os testes passarem, a imagem deve ser implantada em um cluster ECS sem tempo de inatividade.",
    "solution": ["codecommit", "codebuild", "codedeploy", "ecr", "ecs"],
    "availableServices": ["codecommit", "codebuild", "codedeploy", "ecr", "ecs", "s3", "lambda"],
    "explanation": "Esta é a suíte de CI/CD da AWS. CodeCommit para o repositório Git. CodeBuild para construir a imagem Docker e enviá-la para o Amazon ECR (Elastic Container Registry). CodePipeline orquestra o fluxo, e o CodeDeploy gerencia a implantação (por exemplo, Blue/Green) no cluster ECS."
  },
  {
    "id": "aws-co-006",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Um recurso crítico, como um bucket S3 ou uma tabela do DynamoDB, foi excluído acidentalmente. Qual serviço você usaria para investigar 'quem' fez a chamada de API de exclusão e 'quando'?",
    "solution": ["cloudtrail"],
    "availableServices": ["cloudtrail", "cloudwatch", "iam", "s3", "macie"],
    "explanation": "O AWS CloudTrail é o serviço de auditoria que registra todas as chamadas de API. Ao pesquisar no 'Event history' do CloudTrail, você pode filtrar pelo nome do recurso e pelo nome do evento (ex: 'DeleteBucket') para encontrar a identidade do IAM que realizou a ação e o horário exato."
  },
  {
    "id": "aws-co-007",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Você precisa centralizar os logs de várias fontes (instâncias EC2, funções Lambda, logs de VPC Flow) para pesquisa e análise em tempo real. A solução deve permitir a criação de painéis de visualização.",
    "solution": ["cloudwatch logs", "opensearch"],
    "availableServices": ["cloudwatch logs", "opensearch", "s3", "athena", "kinesis-firehose"],
    "explanation": "Os logs podem ser enviados para o CloudWatch Logs. De lá, você pode usar uma 'Subscription Filter' para transmitir os logs em tempo real (via Kinesis Firehose, por exemplo) para o Amazon OpenSearch Service. O OpenSearch é otimizado para indexação e pesquisa de logs, e inclui o OpenSearch Dashboards para visualização."
  },
  {
    "id": "aws-co-008",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Uma aplicação web precisa se recuperar automaticamente de uma falha de Zona de Disponibilidade (AZ). O tráfego deve ser redirecionado para instâncias saudáveis em outra AZ sem intervenção manual.",
    "solution": ["ec2", "auto scaling group", "elastic load balancer", "route 53"],
    "availableServices": ["ec2", "auto scaling group", "elastic load balancer", "route 53", "dms"],
    "explanation": "Um Auto Scaling Group distribuído em múltiplas AZs garante que as instâncias sejam recriadas em caso de falha. Um Elastic Load Balancer (ELB) na frente do ASG realiza verificações de saúde e direciona o tráfego apenas para as instâncias saudáveis. O Route 53 pode ser usado para o DNS, mas o ELB é quem lida com o failover entre AZs."
  },
  {
    "id": "aws-co-009",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Uma função Lambda precisa acessar um banco de dados RDS de forma segura, sem armazenar o nome de usuário e a senha no código. A senha também deve ser rotacionada automaticamente a cada 30 dias.",
    "solution": ["lambda", "rds", "secrets-manager"],
    "availableServices": ["lambda", "rds", "secrets-manager", "iam", "kms", "parameter store"],
    "explanation": "O AWS Secrets Manager é projetado para armazenar e gerenciar segredos, incluindo a capacidade de rotacionar senhas de bancos de dados RDS automaticamente. A função Lambda pode receber permissão do IAM para ler o segredo do Secrets Manager em tempo de execução."
  },
  {
    "id": "aws-co-010",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você precisa garantir que todos os buckets S3 criados na sua conta tenham o versionamento e a criptografia do lado do servidor habilitados, para atender a uma política de conformidade. Qualquer recurso não conforme deve ser sinalizado.",
    "solution": ["aws config"],
    "availableServices": ["aws config", "iam", "cloudtrail", "inspector", "lambda"],
    "explanation": "O AWS Config é o serviço para avaliar, auditar e monitorar a configuração dos seus recursos AWS. Você pode usar 'Regras do AWS Config' (gerenciadas ou customizadas) para verificar continuamente se os recursos (como buckets S3) estão em conformidade com suas políticas. Recursos não conformes são sinalizados em um painel."
  },
  {
    "id": "aws-co-011",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Quando uma nova instância EC2 é lançada por um Auto Scaling Group, ela precisa ser automaticamente registrada como um alvo em um Application Load Balancer para começar a receber tráfego.",
    "solution": ["ec2", "auto scaling group", "elastic load balancer"],
    "availableServices": ["ec2", "auto scaling group", "elastic load balancer", "lambda", "eventbridge"],
    "explanation": "Esta é uma integração nativa. Ao associar um Auto Scaling Group (ASG) a um 'Target Group' de um Application Load Balancer (ALB), o ASG registrará automaticamente novas instâncias no Target Group e cancelará o registro de instâncias terminadas."
  },
  {
    "id": "aws-co-012",
    "category": "cloud-operations",
    "difficulty": 5,
    "question": "Crie um alerta de faturamento que notifique a equipe de FinOps via e-mail quando os custos estimados da conta AWS ultrapassarem um limite de $5.000 no mês.",
    "solution": ["aws budgets", "sns"],
    "availableServices": ["aws budgets", "sns", "cost explorer", "cloudwatch", "lambda"],
    "explanation": "O AWS Budgets permite que você defina orçamentos de custo e configure alertas que são acionados quando seu custo ou uso excede (ou está previsto para exceder) o valor orçado. As notificações de alerta podem ser enviadas para um tópico SNS."
  },
  {
    "id": "aws-co-013",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Uma instância EC2 foi comprometida. Você precisa isolá-la imediatamente da rede para investigação forense, mas sem terminá-la, para preservar a memória e o estado do disco.",
    "solution": ["ec2", "security group", "iam"],
    "availableServices": ["ec2", "security group", "iam", "vpc", "systems manager"],
    "explanation": "A maneira mais rápida de isolar uma instância é alterar seu Security Group para um grupo de 'quarentena'. Este grupo de segurança teria todas as regras de entrada e saída removidas, exceto talvez o acesso de um endereço IP específico do time de segurança. Isso bloqueia toda a comunicação de rede da instância."
  },
  {
    "id": "aws-co-014",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa de uma estratégia de backup centralizada e automatizada para vários serviços da AWS, incluindo instâncias EC2, volumes EBS e bancos de dados RDS, com políticas de retenção e ciclo de vida definidas.",
    "solution": ["aws backup"],
    "availableServices": ["aws backup", "ebs snapshots", "rds snapshots", "lambda", "s3"],
    "explanation": "O AWS Backup é um serviço totalmente gerenciado que centraliza e automatiza o backup de dados em serviços AWS. Ele permite criar 'Planos de Backup' que definem a frequência, a janela de backup, a política de retenção e o ciclo de vida para os recursos selecionados."
  },
  {
    "id": "aws-co-015",
    "category": "cloud-operations",
    "difficulty": 9,
    "question": "Automatize uma resposta a um alerta de segurança do AWS GuardDuty. Se um alerta indicar uma varredura de portas em uma instância EC2, uma função deve ser acionada para adicionar o IP de origem a uma lista de bloqueio na Network ACL.",
    "solution": ["guardduty", "eventbridge", "lambda", "vpc"],
    "availableServices": ["guardduty", "eventbridge", "lambda", "vpc", "systems manager", "sns"],
    "explanation": "O AWS GuardDuty gera 'Findings' (descobertas). Essas descobertas são enviadas como eventos para o Amazon EventBridge. Você pode criar uma regra no EventBridge que corresponda ao 'Finding' específico e use uma função Lambda como alvo. A Lambda então executará a lógica para extrair o IP e modificar a Network ACL (NACL) da VPC."
  },
  {
    "id": "aws-co-016",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Forneça acesso temporário e com privilégios mínimos a um consultor externo para que ele possa acessar um bucket S3 específico por um período de 8 horas.",
    "solution": ["iam", "s3"],
    "availableServices": ["iam", "s3", "secrets-manager", "cognito"],
    "explanation": "A melhor prática é criar um 'IAM Role' (perfil) com uma política que conceda as permissões necessárias ao bucket S3. O consultor pode então 'assumir' esse perfil (AssumeRole) para obter credenciais de segurança temporárias que expiram automaticamente, em vez de criar um usuário IAM de longo prazo."
  },
  {
    "id": "aws-co-017",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Uma aplicação distribuída está apresentando alta latência. Você precisa identificar qual serviço no fluxo (ex: API Gateway -> Lambda -> DynamoDB) está causando o gargalo.",
    "solution": ["x-ray"],
    "availableServices": ["x-ray", "cloudwatch", "cloudtrail", "vpc flow logs"],
    "explanation": "O AWS X-Ray é o serviço de rastreamento distribuído. Ao instrumentar sua aplicação com o SDK do X-Ray, ele coleta dados sobre as solicitações e gera um 'mapa de serviço' visual, mostrando o fluxo das chamadas e a latência em cada componente, facilitando a identificação de gargalos."
  },
  {
    "id": "aws-co-018",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Implante uma aplicação web em contêineres que seja totalmente serverless, escalável e econômica, sem que você precise gerenciar servidores ou clusters de contêineres.",
    "solution": ["ecr", "ecs", "fargate"],
    "availableServices": ["ecr", "ecs", "fargate", "eks", "ec2"],
    "explanation": "O AWS Fargate é um mecanismo de computação serverless para contêineres que funciona com o Amazon ECS e EKS. Você empacota sua aplicação em contêineres, armazena no ECR, e o Fargate executa os contêineres sem que você precise provisionar ou gerenciar a infraestrutura de servidores EC2 subjacente."
  },
  {
    "id": "aws-co-019",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa analisar o tráfego de rede que entra e sai de suas instâncias EC2 para diagnosticar regras de firewall muito restritivas ou abertas. A análise deve capturar IPs de origem, destino e se o tráfego foi permitido ou negado.",
    "solution": ["vpc flow logs", "s3", "athena"],
    "availableServices": ["vpc flow logs", "s3", "athena", "cloudwatch", "cloudtrail"],
    "explanation": "Os VPC Flow Logs capturam informações sobre o tráfego IP que vai de e para as interfaces de rede na sua VPC. Você pode publicar esses logs no S3 e, em seguida, usar o Amazon Athena para executar consultas SQL nos logs, permitindo análises detalhadas do tráfego de rede."
  },
  {
    "id": "aws-co-020",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Para otimizar custos, você precisa parar automaticamente todas as instâncias EC2 de 'desenvolvimento' fora do horário comercial (às 19h) e iniciá-las novamente no início do dia (às 8h).",
    "solution": ["ec2", "eventbridge", "lambda"],
    "availableServices": ["ec2", "eventbridge", "lambda", "systems manager", "auto scaling"],
    "explanation": "O Amazon EventBridge (Scheduler) é ideal para criar regras baseadas em tempo (cron jobs). Você pode ter duas regras: uma às 19h e outra às 8h. Ambas acionam uma função Lambda. A função Lambda contém o código (usando o SDK da AWS) para listar as instâncias com a tag 'ambiente=desenvolvimento' e executar as ações de 'stop' ou 'start'."
  },
  {
    "id": "aws-co-021",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Um novo desenvolvedor precisa de acesso programático à AWS CLI. Qual é a maneira mais segura de configurar as credenciais na sua máquina local?",
    "solution": ["iam", "aws cli"],
    "availableServices": ["iam", "aws cli", "ec2", "secrets-manager"],
    "explanation": "A melhor prática é criar um 'Usuário IAM' para o desenvolvedor, com as permissões mínimas necessárias. Em seguida, gerar 'chaves de acesso' (Access Key ID e Secret Access Key) para esse usuário. As chaves devem ser configuradas localmente usando o comando `aws configure`, que as armazena de forma segura no perfil do usuário."
  },
  {
    "id": "aws-co-022",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você precisa de um painel centralizado que agregue descobertas de segurança de vários serviços da AWS, como GuardDuty, Inspector e Macie, para ter uma visão unificada da sua postura de segurança.",
    "solution": ["security hub"],
    "availableServices": ["security hub", "guardduty", "inspector", "macie", "cloudwatch"],
    "explanation": "O AWS Security Hub foi projetado exatamente para isso. Ele fornece uma visão abrangente do seu estado de segurança na AWS, centralizando e priorizando alertas e descobertas de segurança de vários serviços da AWS e de parceiros, em um único local."
  },
  {
    "id": "aws-co-023",
    "category": "cloud-operations",
    "difficulty": 9,
    "question": "Projete uma arquitetura de recuperação de desastres (DR) 'Pilot Light' para uma aplicação web. A infraestrutura principal deve ser provisionada, mas em uma escala mínima na região de DR, pronta para ser escalada rapidamente em caso de desastre.",
    "solution": ["route 53", "cloudformation", "rds", "ec2", "auto scaling group"],
    "availableServices": ["route 53", "cloudformation", "rds", "ec2", "auto scaling group", "dms"],
    "explanation": "Na região de DR, o CloudFormation define a infraestrutura. O RDS é provisionado como uma réplica de leitura cross-region. O Auto Scaling Group tem uma contagem mínima de instâncias (ex: 1). Os dados (ex: S3) são replicados. Em um desastre, o Route 53 direciona o tráfego, a réplica do RDS é promovida e o Auto Scaling Group é escalado para a capacidade total."
  },
  {
    "id": "aws-co-024",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Um volume EBS de uma instância EC2 crítica está ficando sem espaço. Você precisa aumentar seu tamanho sem tempo de inatividade para a aplicação.",
    "solution": ["ebs", "ec2"],
    "availableServices": ["ebs", "ec2", "s3", "storage gateway"],
    "explanation": "Os volumes EBS suportam 'modificações elásticas'. Você pode modificar o tamanho, o tipo e o IOPS de um volume anexado a uma instância EC2 sem precisar desanexá-lo. Após a AWS modificar o volume, você precisa estender o sistema de arquivos no sistema operacional para usar o novo espaço."
  },
  {
    "id": "aws-co-025",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Você precisa distribuir globalmente o conteúdo de um site estático hospedado no S3, garantindo baixa latência para usuários em todo o mundo e proteção contra ataques DDoS na camada de rede.",
    "solution": ["s3", "cloudfront", "aws shield"],
    "availableServices": ["s3", "cloudfront", "aws shield", "route 53", "ec2"],
    "explanation": "O Amazon CloudFront é a CDN da AWS que armazena o conteúdo em cache globalmente. Além disso, todas as distribuições do CloudFront são protegidas por padrão pelo AWS Shield Standard, que oferece defesa contra os ataques DDoS mais comuns na camada de rede e transporte."
  },
  {
    "id": "aws-co-026",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Uma aplicação legada que não suporta escalonamento horizontal precisa de alta disponibilidade. Se a instância EC2 principal falhar, o tráfego precisa ser redirecionado para uma instância de standby em outra AZ.",
    "solution": ["ec2", "route 53", "cloudwatch"],
    "availableServices": ["ec2", "route 53", "cloudwatch", "elastic load balancer", "auto scaling group"],
    "explanation": "Um ELB ou ASG não funcionaria para uma aplicação stateful única. A solução é usar 'Verificações de Saúde' (Health Checks) do Route 53. Você cria uma verificação de saúde que monitora a instância principal. O registro DNS do Route 53 é configurado com uma política de 'Failover', apontando para a instância de standby se a verificação de saúde da principal falhar."
  },
  {
    "id": "aws-co-027",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa executar um comando shell em centenas de instâncias EC2 de uma só vez para coletar informações de diagnóstico, sem precisar fazer login via SSH em cada uma.",
    "solution": ["systems manager"],
    "availableServices": ["systems manager", "ec2", "lambda", "ssh"],
    "explanation": "O AWS Systems Manager Run Command permite que você execute comandos ou scripts em um grande número de instâncias gerenciadas de forma remota e segura. Você pode executar comandos ad-hoc, rastrear o status e coletar a saída de todas as instâncias em um único local."
  },
  {
    "id": "aws-co-028",
    "category": "cloud-operations",
    "difficulty": 5,
    "question": "Um desenvolvedor precisa de permissões para gerenciar instâncias EC2, mas não deve ter permissão para modificar configurações de IAM. Como você estrutura essa permissão?",
    "solution": ["iam"],
    "availableServices": ["iam", "ec2", "security group", "organizations"],
    "explanation": "No IAM, você cria uma 'Política' que concede permissões para ações da EC2 (ex: `ec2:StartInstances`, `ec2:StopInstances`) e explicitamente nega ou não inclui permissões para ações do IAM (ex: `iam:*`). Essa política é então anexada ao usuário ou grupo do desenvolvedor, seguindo o princípio do menor privilégio."
  },
  {
    "id": "aws-co-029",
    "category": "cloud-operations",
    "difficulty": 9,
    "question": "Sua organização tem várias contas AWS para diferentes departamentos (dev, prod, finanças). Você precisa aplicar uma política de segurança (SCP) que impeça qualquer usuário em qualquer conta de desativar o CloudTrail, centralizar a cobrança e gerenciar todas as contas de forma unificada.",
    "solution": ["aws organizations"],
    "availableServices": ["aws organizations", "iam", "cloudtrail", "aws budgets"],
    "explanation": "O AWS Organizations permite gerenciar centralmente várias contas AWS. Você pode usar o 'Faturamento Consolidado' para a cobrança. E o mais importante, você pode aplicar 'Políticas de Controle de Serviço' (SCPs) na raiz da organização ou em Unidades Organizacionais (OUs) para impor barreiras de permissão, como proibir a desativação de serviços críticos como o CloudTrail, mesmo para o administrador da conta filha."
  },
  {
    "id": "aws-co-030",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Uma aplicação conteinerizada no ECS precisa de permissão para ler de uma fila SQS. Como você concede essa permissão ao contêiner de forma segura?",
    "solution": ["ecs", "sqs", "iam"],
    "availableServices": ["ecs", "sqs", "iam", "ecr", "secrets-manager"],
    "explanation": "A melhor prática é usar 'IAM Roles for ECS Tasks'. Você cria um 'Task Role' do IAM com uma política que permite as ações necessárias na fila SQS. Em seguida, você associa esse perfil à 'Definição da Tarefa' do ECS. Os contêineres dessa tarefa herdarão automaticamente as permissões por meio de credenciais temporárias."
  },
  {
    "id": "aws-co-031",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Você precisa criar um endereço IP público estático para uma instância EC2, que não mude mesmo se a instância for parada e iniciada.",
    "solution": ["elastic ip"],
    "availableServices": ["elastic ip", "ec2", "route 53", "vpc"],
    "explanation": "Um 'Elastic IP' (EIP) é um endereço IPv4 público estático projetado para computação em nuvem dinâmica. Você aloca um EIP na sua conta e o associa à sua instância EC2. O endereço permanece com sua conta até que você o libere, permitindo reassociá-lo a outra instância se necessário."
  },
  {
    "id": "aws-co-032",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Um desenvolvedor acidentalmente provisionou uma instância EC2 `m5.24xlarge` muito cara. Como você pode prevenir que isso aconteça no futuro, permitindo que apenas tipos de instância específicos sejam lançados?",
    "solution": ["iam"],
    "availableServices": ["iam", "aws config", "service catalog", "organizations"],
    "explanation": "Você pode usar 'chaves de condição' nas políticas do IAM. Uma política pode permitir a ação `ec2:RunInstances` mas com uma condição que restringe o atributo `ec2:InstanceType` a uma lista aprovada (ex: `t3.micro`, `t3.small`). Qualquer tentativa de lançar um tipo de instância fora dessa lista será negada."
  },
  {
    "id": "aws-co-033",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Uma aplicação precisa se conectar a um banco de dados RDS a partir de uma instância EC2 dentro da mesma VPC de forma privada e segura. Qual configuração de rede garante que o tráfego não passe pela internet pública?",
    "solution": ["ec2", "rds", "vpc", "security group"],
    "availableServices": ["ec2", "rds", "vpc", "security group", "internet gateway", "nat gateway"],
    "explanation": "Ambos os recursos devem estar em sub-redes privadas dentro da VPC. A conectividade é controlada por 'Security Groups'. O Security Group do RDS deve ter uma regra de entrada que permite o tráfego na porta do banco de dados (ex: 3306 para MySQL) originado do Security Group da instância EC2. Isso cria uma regra de firewall específica sem expor o banco de dados."
  },
  {
    "id": "aws-co-034",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa encontrar rapidamente todas as instâncias EC2 em sua conta que não possuem a tag 'Projeto' para fins de alocação de custos.",
    "solution": ["resource groups & tag editor"],
    "availableServices": ["resource groups & tag editor", "ec2", "cloudwatch", "systems manager"],
    "explanation": "O AWS Resource Groups & Tag Editor é a ferramenta central para gerenciar tags. Usando o 'Tag Editor', você pode pesquisar por todos os recursos de um tipo específico (como EC2) e filtrar por tags, incluindo a busca por recursos onde uma tag específica 'não está presente'."
  },
  {
    "id": "aws-co-035",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você precisa de uma conexão de rede privada e dedicada entre seu data center on-premises e sua VPC na AWS, com largura de banda consistente e baixa latência.",
    "solution": ["direct connect"],
    "availableServices": ["direct connect", "vpn", "vpc", "internet gateway"],
    "explanation": "O AWS Direct Connect estabelece uma conexão de rede privada e dedicada entre seu ambiente on-premises e a AWS. Diferente de uma VPN que opera sobre a internet pública, o Direct Connect oferece uma experiência de rede mais consistente e com maior largura de banda."
  },
  {
    "id": "aws-co-036",
    "category": "cloud-operations",
    "difficulty": 9,
    "question": "Um serviço crítico precisa de um objetivo de tempo de recuperação (RTO) de 15 minutos e um objetivo de ponto de recuperação (RPO) de 1 minuto em caso de desastre regional. Qual estratégia de DR atende a esses requisitos rigorosos?",
    "solution": ["rds", "s3", "route 53", "auto scaling group"],
    "availableServices": ["rds", "s3", "route 53", "auto scaling group", "aws backup"],
    "explanation": "Isso exige uma estratégia de 'Warm Standby' ou 'Hot Standby'. RPO de 1 minuto sugere replicação contínua de dados, como réplicas de leitura cross-region para RDS Aurora ou replicação cross-region (CRR) para S3. RTO de 15 minutos exige que a infraestrutura esteja pré-provisionada (Warm Standby) ou totalmente ativa (Hot Standby), com failover de DNS automatizado via Route 53."
  },
  {
    "id": "aws-co-037",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você quer padronizar a criação de produtos de TI (ex: um servidor de desenvolvimento com EC2 e RDS) para que os desenvolvedores possam provisioná-los através de um portal de autoatendimento, sem acessar diretamente o CloudFormation.",
    "solution": ["service catalog"],
    "availableServices": ["service catalog", "cloudformation", "lambda", "organizations"],
    "explanation": "O AWS Service Catalog permite que as organizações criem e gerenciem catálogos de serviços de TI aprovados para uso na AWS. Você, como administrador, define os produtos usando templates do CloudFormation e os usuários podem implantar esses produtos padronizados com um clique, sem ver ou modificar o código subjacente."
  },
  {
    "id": "aws-co-038",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você precisa expor vários microsserviços, cada um executado em seu próprio cluster ECS, através de um único endpoint de API. A solução deve lidar com roteamento, limitação de taxa (throttling) e autenticação.",
    "solution": ["api gateway", "ecs"],
    "availableServices": ["api gateway", "ecs", "elastic load balancer", "lambda"],
    "explanation": "O Amazon API Gateway atua como uma 'porta da frente' unificada para suas aplicações. Você pode configurar rotas (ex: /users, /orders) que direcionam para diferentes serviços de backend, como os serviços ECS. O API Gateway também fornece recursos integrados para throttling, autenticação (ex: chaves de API, IAM, Cognito) e cache."
  },
  {
    "id": "aws-co-039",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Um bucket S3 contém dados sensíveis. Você precisa garantir que todos os novos objetos carregados no bucket sejam criptografados automaticamente, sem que o cliente precise especificar cabeçalhos de criptografia.",
    "solution": ["s3", "kms"],
    "availableServices": ["s3", "kms", "iam", "macie"],
    "explanation": "Você pode habilitar a 'Criptografia Padrão' (Default Encryption) nas propriedades do bucket S3. Ao configurar isso para usar SSE-S3 (chaves gerenciadas pela AWS) ou SSE-KMS (chaves gerenciadas no KMS), qualquer objeto enviado para o bucket sem informações de criptografia será criptografado automaticamente."
  },
  {
    "id": "aws-co-040",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Após uma implantação, a latência de uma função Lambda aumentou. A função não mudou, mas uma de suas dependências (uma biblioteca Python) foi atualizada. Como você pode implantar a versão anterior da função Lambda rapidamente?",
    "solution": ["lambda"],
    "availableServices": ["lambda", "codedeploy", "cloudformation", "s3"],
    "explanation": "O AWS Lambda suporta 'Versionamento e Aliases'. Cada vez que você publica uma função, uma nova versão imutável é criada. Você pode usar um 'alias' (ex: 'PROD') que aponta para a versão estável. Para reverter, basta reconfigurar o alias 'PROD' para apontar para a versão funcional anterior, o que é uma operação quase instantânea."
  },
  {
    "id": "aws-co-041",
    "category": "cloud-operations",
    "difficulty": 4,
    "question": "Você precisa de um local centralizado para armazenar parâmetros de configuração, como URLs de banco de dados ou chaves de recursos, para serem usados por suas aplicações EC2 e funções Lambda.",
    "solution": ["systems manager parameter store"],
    "availableServices": ["systems manager parameter store", "secrets-manager", "s3", "dynamodb"],
    "explanation": "O AWS Systems Manager Parameter Store oferece armazenamento seguro e hierárquico para gerenciamento de dados de configuração. É ideal para armazenar dados não secretos. Para dados sensíveis como senhas, o Secrets Manager é mais apropriado."
  },
  {
    "id": "aws-co-042",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Uma aplicação precisa de um sistema de arquivos compartilhado e de alta performance, acessível por múltiplas instâncias EC2 Linux simultaneamente.",
    "solution": ["efs"],
    "availableServices": ["efs", "ebs", "s3", "fsx"],
    "explanation": "O Amazon EFS (Elastic File System) fornece um sistema de arquivos de rede (NFS) simples, escalável e elástico para ser usado com instâncias EC2. Ele é projetado para ser montado e acessado por muitas instâncias ao mesmo tempo, ideal para armazenamento compartilhado."
  },
  {
    "id": "aws-co-043",
    "category": "cloud-operations",
    "difficulty": 9,
    "question": "Um processo de ingestão de vídeo precisa ser orquestrado: quando um vídeo é carregado no S3, uma função Lambda deve extrair metadados. Em seguida, duas tarefas de transcodificação (para 1080p e 480p) devem ocorrer em paralelo. Quando ambas terminarem, uma notificação de sucesso deve ser enviada.",
    "solution": ["s3", "lambda", "step-functions", "sns"],
    "availableServices": ["s3", "lambda", "step-functions", "sns", "sqs", "eventbridge"],
    "explanation": "Este é um caso de uso perfeito para o AWS Step Functions. Um gatilho do S3 inicia a máquina de estados. O primeiro estado invoca a Lambda. Um estado 'Parallel' executa as duas tarefas de transcodificação (que poderiam ser outras Lambdas ou tarefas do Fargate). O fluxo converge após o paralelismo para enviar a notificação via SNS."
  },
  {
    "id": "aws-co-044",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa analisar suas faturas da AWS para identificar qual recurso específico (ex: uma instância EC2 com um ID particular) está gerando mais custos.",
    "solution": ["cost explorer", "tags"],
    "availableServices": ["cost explorer", "tags", "aws budgets", "cloudwatch"],
    "explanation": "A chave para a análise de custos granulares é o 'tagging'. Ao aplicar tags (ex: 'Projeto:Alfa', 'CentroCusto:123') aos seus recursos, você pode ativar essas tags no AWS Cost Explorer. Isso permite filtrar e agrupar os custos por tag, identificando exatamente quais recursos ou projetos são responsáveis pelos gastos."
  },
  {
    "id": "aws-co-045",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Uma aplicação em uma instância EC2 em uma sub-rede privada precisa acessar serviços da AWS (como S3 e SQS) sem que o tráfego passe pela internet. A solução não deve usar um NAT Gateway.",
    "solution": ["vpc", "ec2", "s3", "vpc endpoint"],
    "availableServices": ["vpc", "ec2", "s3", "vpc endpoint", "nat gateway", "internet gateway"],
    "explanation": "Os 'VPC Endpoints' fornecem conectividade privada da sua VPC para serviços da AWS suportados. Um 'Gateway Endpoint' para S3 e DynamoDB ou um 'Interface Endpoint' para outros serviços (como SQS) permite que o tráfego flua pela rede privada da AWS, melhorando a segurança e a performance."
  },
  {
    "id": "aws-co-046",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Um volume EBS foi acidentalmente excluído. Qual é o pré-requisito fundamental para que você possa restaurar os dados desse volume?",
    "solution": ["ebs snapshots"],
    "availableServices": ["ebs snapshots", "aws backup", "s3", "glacier"],
    "explanation": "Os dados de um volume EBS só podem ser recuperados se houver um 'snapshot' dele. Snapshots são backups de point-in-time dos volumes EBS, armazenados no S3. Sem um snapshot pré-existente (criado manualmente ou por uma política do AWS Backup), os dados de um volume excluído são irrecuperáveis."
  },
  {
    "id": "aws-co-047",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você está executando uma aplicação em contêineres no EKS. Como você pode escalar automaticamente o número de pods com base na utilização da CPU?",
    "solution": ["eks", "horizontal pod autoscaler"],
    "availableServices": ["eks", "horizontal pod autoscaler", "cluster autoscaler", "ec2 auto scaling"],
    "explanation": "No Kubernetes, o 'Horizontal Pod Autoscaler' (HPA) é o recurso nativo para escalar o número de réplicas de um pod com base em métricas observadas, como a utilização da CPU. O HPA ajusta dinamicamente o número de pods para atender à demanda."
  },
  {
    "id": "aws-co-048",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "No seu cluster EKS, os pods não conseguem ser agendados devido à falta de capacidade de nós (instâncias EC2). Como você pode escalar automaticamente o número de nós no cluster?",
    "solution": ["eks", "cluster autoscaler"],
    "availableServices": ["eks", "cluster autoscaler", "horizontal pod autoscaler", "ec2 auto scaling"],
    "explanation": "Enquanto o HPA escala os pods, o 'Cluster Autoscaler' é o componente que escala os nós (a infraestrutura). Ele monitora pods que estão no estado 'Pending' por falta de recursos e adiciona automaticamente novos nós (instâncias EC2) ao cluster para acomodá-los."
  },
  {
    "id": "aws-co-049",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa de um painel de controle operacional que mostre métricas de várias fontes (ex: CPU da EC2, invocações da Lambda, tamanho da fila SQS) e logs de aplicações em uma única visualização consolidada.",
    "solution": ["cloudwatch dashboards"],
    "availableServices": ["cloudwatch dashboards", "quicksight", "opensearch", "grafana"],
    "explanation": "Os 'CloudWatch Dashboards' são a solução nativa para criar painéis operacionais customizáveis. Você pode adicionar widgets que exibem métricas do CloudWatch, resultados de consultas do CloudWatch Logs Insights e alarmes, tudo em uma única tela para monitoramento consolidado."
  },
  {
    "id": "aws-co-050",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Implante uma atualização em um conjunto de instâncias EC2 de forma gradual. A nova versão deve ser implantada primeiro em um pequeno número de instâncias e, se os alarmes do CloudWatch permanecerem OK, a implantação deve continuar para o restante do parque.",
    "solution": ["codedeploy", "ec2", "cloudwatch"],
    "availableServices": ["codedeploy", "ec2", "cloudwatch", "cloudformation", "codebuild"],
    "explanation": "O AWS CodeDeploy suporta estratégias de implantação avançadas, como 'Canary' ou 'Linear'. Ele pode ser configurado para monitorar alarmes do CloudWatch durante o processo. Se um alarme for acionado após a implantação no primeiro lote, o CodeDeploy reverterá automaticamente a alteração, prevenindo uma falha em larga escala."
  },
  {
    "id": "aws-co-051",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Como você pode garantir que uma função Lambda, dentro de uma VPC, possa acessar a internet pública para chamar uma API de terceiros?",
    "solution": ["lambda", "vpc", "nat gateway"],
    "availableServices": ["lambda", "vpc", "nat gateway", "internet gateway", "vpc endpoint"],
    "explanation": "Uma Lambda em uma sub-rede privada não tem acesso direto à internet. Para permitir o acesso de saída, a tabela de rotas da sub-rede privada deve apontar o tráfego de internet (0.0.0.0/0) para um 'NAT Gateway', que por sua vez reside em uma sub-rede pública e tem acesso à internet."
  },
  {
    "id": "aws-co-052",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa analisar as dependências das bibliotecas em seu código e verificar se há vulnerabilidades de segurança conhecidas durante o processo de build no seu pipeline de CI/CD.",
    "solution": ["codebuild", "inspector"],
    "availableServices": ["codebuild", "inspector", "guardduty", "codeguru"],
    "explanation": "O Amazon Inspector pode realizar a 'análise de composição de software' (SCA), que escaneia as dependências do seu código em busca de vulnerabilidades. Isso pode ser integrado como uma etapa no AWS CodeBuild para falhar o build caso vulnerabilidades críticas sejam encontradas."
  },
  {
    "id": "aws-co-053",
    "category": "cloud-operations",
    "difficulty": 9,
    "question": "Uma aplicação crítica falhou. Você precisa montar uma 'war room' virtual, notificando as equipes de desenvolvimento e operações via chat e e-mail, e registrar todos os eventos e ações tomadas para a análise post-mortem.",
    "solution": ["incident manager"],
    "availableServices": ["incident manager", "systems manager", "sns", "cloudwatch", "chatbot"],
    "explanation": "O 'Incident Manager', um recurso do AWS Systems Manager, foi projetado para isso. Ele permite criar planos de resposta que, quando um incidente é detectado (via CloudWatch Alarms, por exemplo), automaticamente engajam as equipes certas (via SMS, e-mail, AWS Chatbot), criam um canal de chat e registram o cronograma do incidente."
  },
  {
    "id": "aws-co-054",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Um time de desenvolvimento precisa de um ambiente de banco de dados para testes que pode ser criado e destruído rapidamente. O banco não precisa de alta disponibilidade. Qual serviço de banco de dados relacional é mais adequado?",
    "solution": ["rds"],
    "availableServices": ["rds", "aurora", "dynamodb", "redshift"],
    "explanation": "O Amazon RDS (com um mecanismo como PostgreSQL ou MySQL) é perfeito para isso. Você pode provisionar uma instância 'Single-AZ' (sem alta disponibilidade) para reduzir custos e usar scripts de IaC (CloudFormation) para criá-la e destruí-la sob demanda."
  },
  {
    "id": "aws-co-055",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa conectar sua VPC a outra VPC em uma região diferente da AWS para permitir que recursos em ambas as VPCs se comuniquem usando IPs privados.",
    "solution": ["vpc peering", "transit gateway"],
    "availableServices": ["vpc peering", "transit gateway", "direct connect", "vpn"],
    "explanation": "Tanto o 'VPC Peering' quanto o 'Transit Gateway' podem resolver isso. O VPC Peering cria uma conexão 1-para-1. O Transit Gateway atua como um hub central para interconectar muitas VPCs (e conexões on-premises), o que é mais escalável. Para conectar entre regiões, ambos os serviços suportam 'inter-region peering'."
  },
  {
    "id": "aws-co-056",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você está migrando uma aplicação on-premises que usa um sistema de arquivos Windows para a AWS. Você precisa de um armazenamento de arquivos totalmente gerenciado e compatível com o protocolo SMB.",
    "solution": ["fsx for windows file server"],
    "availableServices": ["fsx for windows file server", "efs", "s3", "ebs"],
    "explanation": "O Amazon FSx for Windows File Server fornece um sistema de arquivos totalmente gerenciado e nativo do Windows, construído sobre o Windows Server e acessível via protocolo SMB. É a solução ideal para migrar cargas de trabalho que dependem de compartilhamento de arquivos do Windows."
  },
  {
    "id": "aws-co-057",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você quer executar comandos da AWS CLI a partir de um canal do Slack para realizar ações operacionais rápidas, como verificar o status de uma instância.",
    "solution": ["aws chatbot", "slack"],
    "availableServices": ["aws chatbot", "slack", "lambda", "sns", "systems manager"],
    "explanation": "O AWS Chatbot simplifica a integração de operações de chat (ChatOps) com a AWS. Ele permite que você receba alertas e execute comandos da AWS CLI diretamente do Slack ou Amazon Chime, com as permissões controladas por um perfil do IAM."
  },
  {
    "id": "aws-co-058",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Um conjunto de instâncias EC2 precisa ser iniciado em proximidade física dentro de uma Zona de Disponibilidade para minimizar a latência de rede entre elas, para uma carga de trabalho de computação de alta performance (HPC).",
    "solution": ["placement groups"],
    "availableServices": ["placement groups", "ec2", "auto scaling group", "vpc"],
    "explanation": "Os 'Placement Groups' (Grupos de Posicionamento) com a estratégia 'Cluster' permitem que você agrupe instâncias em um único rack dentro de uma AZ. Isso fornece a menor latência e a maior largura de banda de rede entre as instâncias, ideal para aplicações HPC."
  },
  {
    "id": "aws-co-059",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa obter um certificado SSL/TLS público para seu domínio e gerenciá-lo (incluindo a renovação automática) para uso em um Application Load Balancer.",
    "solution": ["certificate manager", "elastic load balancer"],
    "availableServices": ["certificate manager", "elastic load balancer", "route 53", "iam"],
    "explanation": "O AWS Certificate Manager (ACM) permite provisionar, gerenciar e implantar certificados SSL/TLS públicos e privados. O ACM se integra perfeitamente com serviços como ELB e CloudFront, e o mais importante, gerencia a renovação automática dos certificados públicos que ele provisiona."
  },
  {
    "id": "aws-co-060",
    "category": "cloud-operations",
    "difficulty": 9,
    "question": "Você precisa de um serviço de DNS que possa rotear os usuários para diferentes endpoints com base em sua localização geográfica (geolocalização), para otimizar a latência e fornecer conteúdo localizado.",
    "solution": ["route 53"],
    "availableServices": ["route 53", "cloudfront", "elastic load balancer", "api gateway"],
    "explanation": "O Amazon Route 53 oferece várias 'Políticas de Roteamento'. A política de 'Geolocalização' permite que você escolha os recursos que atenderão ao tráfego com base na localização geográfica de seus usuários (continente, país ou estado nos EUA), direcionando-os para o endpoint mais próximo ou mais apropriado."
  },
  {
    "id": "aws-co-061",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa criar uma imagem de máquina (AMI) customizada a partir de uma instância EC2 configurada, para poder lançar novas instâncias com o mesmo software e configurações.",
    "solution": ["ec2", "ami"],
    "availableServices": ["ec2", "ami", "lambda", "cloudformation", "packer"],
    "explanation": "O processo padrão na AWS para isso é configurar uma instância EC2 base e, em seguida, usar a ação 'Create Image' no console da EC2 ou na CLI. Isso cria uma Amazon Machine Image (AMI) que é um modelo para suas instâncias, contendo o sistema operacional e o software que você instalou."
  },
  {
    "id": "aws-co-062",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Automatize a criação de snapshots de volumes EBS em uma programação diária, garantindo que os snapshots mais antigos que 30 dias sejam excluídos automaticamente.",
    "solution": ["amazon data lifecycle manager"],
    "availableServices": ["amazon data lifecycle manager", "ebs", "lambda", "eventbridge", "aws backup"],
    "explanation": "Embora o AWS Backup possa fazer isso, o Amazon Data Lifecycle Manager (DLM) é um serviço mais específico e simples para automatizar a criação, cópia e exclusão de snapshots de EBS. Você cria uma política no DLM que define a programação e a retenção, e o serviço cuida do resto."
  },
  {
    "id": "aws-co-063",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Sua aplicação em um Auto Scaling Group está sofrendo com tempos de inicialização lentos porque cada nova instância precisa baixar e instalar várias dependências. Como você pode acelerar o tempo de inicialização?",
    "solution": ["ec2", "ami", "auto scaling group"],
    "availableServices": ["ec2", "ami", "auto scaling group", "lambda", "user data"],
    "explanation": "A melhor prática é criar uma 'Golden AMI'. Em vez de instalar o software em tempo de inicialização (via 'user data'), você pré-instala e pré-configura todo o software necessário em uma AMI customizada. O Auto Scaling Group então lança instâncias a partir desta AMI, o que é significativamente mais rápido."
  },
  {
    "id": "aws-co-064",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você precisa fornecer acesso a aplicações desktop (provisionadas na AWS) para seus usuários através de um navegador web, sem que eles precisem de um cliente de desktop remoto.",
    "solution": ["appstream 2.0"],
    "availableServices": ["appstream 2.0", "workspaces", "ec2", "client vpn"],
    "explanation": "O Amazon AppStream 2.0 é um serviço de streaming de aplicações totalmente gerenciado. Você pode instalar suas aplicações desktop no AppStream 2.0 e seus usuários podem acessá-las instantaneamente em seus navegadores, com a aplicação sendo executada na AWS."
  },
  {
    "id": "aws-co-065",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa garantir que sua aplicação web seja protegida contra vulnerabilidades comuns como injeção de SQL e Cross-Site Scripting (XSS).",
    "solution": ["waf", "cloudfront", "elastic load balancer"],
    "availableServices": ["waf", "cloudfront", "elastic load balancer", "security group", "aws shield"],
    "explanation": "O AWS WAF (Web Application Firewall) ajuda a proteger suas aplicações web contra exploits comuns. Você pode implantar o WAF no CloudFront ou em um Application Load Balancer e usar regras gerenciadas (como as da OWASP Top 10) para filtrar o tráfego malicioso."
  },
  {
    "id": "aws-co-066",
    "category": "cloud-operations",
    "difficulty": 9,
    "question": "Um grande número de objetos em um bucket S3 precisa ser processado. Iniciar uma função Lambda para cada objeto é ineficiente. Como você pode processar todos os objetos em lote de forma gerenciada?",
    "solution": ["s3 batch operations"],
    "availableServices": ["s3 batch operations", "lambda", "glue", "emr", "step-functions"],
    "explanation": "O S3 Batch Operations é um recurso do S3 projetado para executar operações em larga escala em bilhões de objetos. Você pode fornecer uma lista de objetos e instruir o S3 Batch Operations a invocar uma função Lambda, copiar objetos ou executar outras ações em cada um, com gerenciamento de retentativas, rastreamento e notificações."
  },
  {
    "id": "aws-co-067",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você precisa de uma maneira de se conectar de forma segura à sua VPC para gerenciar recursos, a partir da sua máquina local, como se estivesse dentro da rede privada.",
    "solution": ["client vpn"],
    "availableServices": ["client vpn", "direct connect", "systems manager session manager", "bastion host"],
    "explanation": "O AWS Client VPN é um serviço de VPN de acesso remoto totalmente gerenciado. Seus usuários remotos podem usar um cliente de software VPN para estabelecer uma conexão segura com a sua VPC, permitindo o acesso a recursos como se estivessem conectados localmente."
  },
  {
    "id": "aws-co-068",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa de uma maneira de se conectar a uma instância EC2 em uma sub-rede privada para fins de shell, sem expor a porta 22 (SSH) e sem a necessidade de um 'bastion host' (servidor de pulo).",
    "solution": ["systems manager session manager"],
    "availableServices": ["systems manager session manager", "ec2", "ssh", "client vpn"],
    "explanation": "O Session Manager, um recurso do AWS Systems Manager, permite que você gerencie suas instâncias EC2 através de um shell interativo baseado em navegador ou da AWS CLI. Ele não requer a abertura de portas de entrada e usa perfis do IAM para controle de acesso, tornando-o mais seguro do que o SSH."
  },
  {
    "id": "aws-co-069",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Um Auto Scaling Group deve aumentar o número de instâncias quando a utilização média de CPU do grupo ultrapassar 60%. Qual tipo de política de escalonamento é a mais adequada para isso?",
    "solution": ["auto scaling group", "cloudwatch"],
    "availableServices": ["auto scaling group", "cloudwatch", "lambda", "systems manager"],
    "explanation": "Uma 'Política de Escalonamento de Rastreamento de Alvo' (Target Tracking Scaling Policy) é a mais simples e eficaz para isso. Você define uma métrica (CPUUtilization) e um valor alvo (60%). O Auto Scaling Group então adiciona ou remove instâncias automaticamente para manter a métrica no valor alvo ou próximo a ele."
  },
  {
    "id": "aws-co-070",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você precisa garantir que os dados em uma tabela do DynamoDB sejam replicados para outra região da AWS para fins de recuperação de desastres e acesso de baixa latência para usuários globais.",
    "solution": ["dynamodb global tables"],
    "availableServices": ["dynamodb global tables", "dms", "lambda", "s3"],
    "explanation": "As 'Tabelas Globais' do DynamoDB são a solução nativa para isso. Elas fornecem um banco de dados totalmente replicado e multiativo em várias regiões da AWS. As escritas em uma região são replicadas automaticamente para as outras regiões, facilitando a criação de aplicações globais e resilientes."
  },
  {
    "id": "aws-co-071",
    "category": "cloud-operations",
    "difficulty": 5,
    "question": "Você precisa encontrar rapidamente a documentação oficial da AWS, tutoriais e artigos da base de conhecimento sobre um serviço específico da AWS.",
    "solution": ["aws documentation"],
    "availableServices": ["aws documentation", "google", "stack overflow", "aws console"],
    "explanation": "A fonte primária e mais confiável para informações sobre os serviços da AWS é a documentação oficial da AWS. Ela fornece guias do usuário, referências de API, tutoriais e melhores práticas detalhadas para cada serviço."
  },
  {
    "id": "aws-co-072",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Uma aplicação precisa enviar notificações por push para dispositivos móveis (iOS e Android). Qual serviço da AWS pode atuar como um único ponto para enviar essas notificações?",
    "solution": ["sns"],
    "availableServices": ["sns", "sqs", "kinesis", "pinpoint"],
    "explanation": "O Amazon SNS (Simple Notification Service) suporta o envio de notificações push para endpoints móveis. Você pode registrar os tokens dos dispositivos no SNS e, em seguida, publicar uma única mensagem no SNS, que a entregará para as plataformas apropriadas (APNS para iOS, FCM para Android)."
  },
  {
    "id": "aws-co-073",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você quer mover um grande repositório de arquivos de um servidor de arquivos on-premises para a AWS, mantendo-o acessível localmente com baixa latência, como se ainda estivesse na rede local.",
    "solution": ["storage gateway"],
    "availableServices": ["storage gateway", "s3", "direct connect", "dms"],
    "explanation": "O AWS Storage Gateway, no modo 'File Gateway', fornece uma interface de sistema de arquivos (NFS ou SMB) para o S3. Ele armazena os arquivos no S3, mas mantém um cache local dos dados acessados com mais frequência, proporcionando acesso de baixa latência para os usuários e aplicações on-premises."
  },
  {
    "id": "aws-co-074",
    "category": "cloud-operations",
    "difficulty": 9,
    "question": "Você precisa criar um 'data perimeter' para sua organização na AWS, garantindo que os dados não possam sair da sua rede de VPCs e que seus recursos só possam interagir com endpoints e serviços confiáveis, prevenindo a exfiltração de dados.",
    "solution": ["vpc", "vpc endpoint", "iam", "organizations"],
    "availableServices": ["vpc", "vpc endpoint", "iam", "organizations", "aws shield", "guardduty"],
    "explanation": "Um perímetro de dados é criado combinando vários controles. As políticas do AWS Organizations (SCPs) definem limites. Os VPC Endpoints e as políticas de endpoint restringem o acesso a serviços para dentro da VPC. As políticas do IAM usam condições (como `aws:SourceVpc`) para garantir que as solicitações se originem de locais de rede confiáveis."
  },
  {
    "id": "aws-co-075",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa de um serviço que forneça recomendações para otimizar seus recursos da AWS em cinco pilares: custo, performance, segurança, tolerância a falhas e limites de serviço.",
    "solution": ["trusted advisor"],
    "availableServices": ["trusted advisor", "cost explorer", "inspector", "aws config"],
    "explanation": "O AWS Trusted Advisor é um serviço que atua como seu consultor de nuvem personalizado. Ele analisa seu ambiente AWS e fornece recomendações em tempo real para ajudá-lo a seguir as melhores práticas da AWS, otimizando os recursos nos pilares mencionados."
  },
  {
    "id": "aws-co-076",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Uma aplicação crítica requer um banco de dados relacional com um RPO (Recovery Point Objective) de zero, o que significa que nenhuma perda de dados é aceitável em caso de falha.",
    "solution": ["aurora"],
    "availableServices": ["aurora", "rds", "dynamodb", "redshift"],
    "explanation": "O Amazon Aurora com a configuração de cluster Multi-AZ foi projetado para alta disponibilidade e durabilidade. Ele replica os dados de forma síncrona em 3 Zonas de Disponibilidade, o que significa que, no momento em que uma escrita é confirmada, ela já está em múltiplos locais, resultando em um RPO de zero (ou muito próximo de zero)."
  },
  {
    "id": "aws-co-077",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa de uma maneira de descobrir e visualizar as configurações e relacionamentos dos seus recursos AWS, além de rastrear o histórico de alterações de configuração de um recurso específico.",
    "solution": ["aws config"],
    "availableServices": ["aws config", "cloudtrail", "cloudwatch", "resource groups"],
    "explanation": "O AWS Config descobre continuamente seus recursos AWS e mantém um inventário detalhado de suas configurações. A 'Linha do Tempo de Configuração' (Configuration Timeline) de um recurso mostra um histórico completo de como sua configuração mudou ao longo do tempo."
  },
  {
    "id": "aws-co-078",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Uma aplicação precisa de um cache em memória distribuído para armazenar o estado da sessão. O cache deve ser altamente disponível e tolerante a falhas de nós.",
    "solution": ["elasticache for redis"],
    "availableServices": ["elasticache for redis", "elasticache for memcached", "dynamodb", "s3"],
    "explanation": "O ElastiCache for Redis, quando configurado no modo 'Cluster Mode Enabled', distribui os dados entre vários shards. Cada shard pode ter réplicas em diferentes AZs. Isso fornece alta disponibilidade, pois se um nó primário falhar, uma réplica pode ser promovida, garantindo a continuidade do serviço."
  },
  {
    "id": "aws-co-079",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Você precisa implantar uma alteração de configuração em um grupo de instâncias do Auto Scaling, substituindo todas as instâncias antigas por novas instâncias com a nova configuração, de forma gradual para minimizar o impacto.",
    "solution": ["auto scaling group"],
    "availableServices": ["auto scaling group", "codedeploy", "cloudformation", "systems manager"],
    "explanation": "Os Auto Scaling Groups suportam 'Instance Refresh'. Esse recurso permite que você execute uma atualização contínua (rolling update) para substituir as instâncias no grupo. Você pode configurar pausas entre as substituições e verificações de saúde para garantir que a atualização ocorra de forma segura e controlada."
  },
  {
    "id": "aws-co-080",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa de um local para armazenar artefatos de build (como arquivos JAR ou pacotes NPM) que são produzidos pelo seu pipeline de CI/CD.",
    "solution": ["codeartifact", "s3"],
    "availableServices": ["codeartifact", "s3", "ecr", "codecommit"],
    "explanation": "O AWS CodeArtifact é um serviço de repositório de artefatos totalmente gerenciado. Ele é projetado para armazenar pacotes de software e dependências. O S3 também pode ser usado como um repositório de artefatos genérico e de baixo custo, sendo uma alternativa comum."
  },
  {
    "id": "aws-co-081",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você precisa garantir que sua aplicação web possa lidar com picos de tráfego repentinos e massivos (ex: um flash sale). A arquitetura deve ser capaz de escalar de quase zero para milhares de solicitações por segundo em minutos.",
    "solution": ["api gateway", "lambda", "dynamodb"],
    "availableServices": ["api gateway", "lambda", "dynamodb", "ec2", "auto scaling", "rds"],
    "explanation": "Uma arquitetura serverless é ideal para picos de tráfego extremos. O API Gateway, a Lambda e o DynamoDB são todos serviços gerenciados que escalam automaticamente para lidar com a carga. Eles não exigem pré-aquecimento (warm-up) da mesma forma que uma frota de EC2, proporcionando escalabilidade massiva sob demanda."
  },
  {
    "id": "aws-co-082",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa analisar o código-fonte de sua aplicação em busca de bugs, vulnerabilidades de segurança e desvios das melhores práticas de codificação, de forma automatizada.",
    "solution": ["codeguru"],
    "availableServices": ["codeguru", "inspector", "codebuild", "lambda"],
    "explanation": "O Amazon CodeGuru é um serviço que usa machine learning para fornecer recomendações inteligentes para melhorar a qualidade do código e identificar os problemas mais caros de uma aplicação. Ele pode ser integrado ao seu processo de revisão de código para análise estática (CodeGuru Reviewer)."
  },
  {
    "id": "aws-co-083",
    "category": "cloud-operations",
    "difficulty": 9,
    "question": "Você precisa migrar uma máquina virtual VMware on-premises para uma instância EC2 na AWS com o mínimo de modificações e tempo de inatividade possível.",
    "solution": ["aws application migration service (mgn)"],
    "availableServices": ["aws application migration service (mgn)", "dms", "vm import/export", "cloudendure"],
    "explanation": "O AWS Application Migration Service (MGN) é o serviço recomendado para migrações 'lift-and-shift' para a AWS. Ele replica continuamente as máquinas de origem (on-premises ou outra nuvem) para uma área de preparação de baixo custo na sua conta AWS. Quando você está pronto para a transição, ele converte e lança as máquinas como instâncias EC2, minimizando o tempo de inatividade."
  },
  {
    "id": "aws-co-084",
    "category": "cloud-operations",
    "difficulty": 6,
    "question": "Você precisa criar um 'health check' personalizado para uma aplicação que vai além de uma simples verificação de porta. O health check deve chamar um endpoint `/health` e esperar uma resposta HTTP 200 OK.",
    "solution": ["elastic load balancer", "route 53"],
    "availableServices": ["elastic load balancer", "route 53", "cloudwatch", "lambda"],
    "explanation": "Tanto o Elastic Load Balancer quanto o Route 53 permitem configurar 'health checks' personalizados. Em vez de apenas verificar a conectividade TCP, você pode configurá-los para fazer solicitações HTTP/HTTPS para um caminho específico (ex: /health) e considerar a instância saudável apenas se receber um código de status 200."
  },
  {
    "id": "aws-co-085",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você precisa de uma maneira de implantar e gerenciar pilhas do CloudFormation em múltiplas contas e regiões da AWS a partir de uma conta de administrador centralizada.",
    "solution": ["cloudformation stacksets"],
    "availableServices": ["cloudformation stacksets", "codepipeline", "systems manager", "organizations"],
    "explanation": "O AWS CloudFormation StackSets estende a funcionalidade do CloudFormation, permitindo que você crie, atualize ou exclua pilhas em várias contas e regiões com uma única operação. A partir de uma conta de administrador, você pode implantar uma infraestrutura base comum (ex: perfis do IAM, regras do Config) em toda a sua organização."
  },
  {
    "id": "aws-co-086",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Como você pode restringir o acesso a um bucket S3 para que o conteúdo só possa ser acessado através de uma distribuição específica do CloudFront, e não diretamente pela URL do S3?",
    "solution": ["s3", "cloudfront", "origin access identity"],
    "availableServices": ["s3", "cloudfront", "origin access identity", "iam", "waf"],
    "explanation": "A solução é usar uma 'Origin Access Identity' (OAI) ou 'Origin Access Control' (OAC). Você cria uma identidade especial no CloudFront e concede a ela permissão para ler o bucket S3. Em seguida, a política do bucket S3 é modificada para permitir o acesso apenas a essa identidade, bloqueando efetivamente todo o acesso direto."
  },
  {
    "id": "aws-co-087",
    "category": "cloud-operations",
    "difficulty": 9,
    "question": "Um serviço de processamento de dados precisa lidar com uma fila de mensagens. Se uma mensagem falhar no processamento várias vezes, ela deve ser movida para outra fila para análise manual, para evitar o bloqueio do processamento de outras mensagens.",
    "solution": ["sqs", "lambda"],
    "availableServices": ["sqs", "lambda", "sns", "dynamodb"],
    "explanation": "Esta é a função de uma 'Dead-Letter Queue' (DLQ). Na configuração da sua fila SQS principal, você pode definir uma política de 'redrive' e especificar outra fila SQS como a DLQ. Se uma mensagem for recebida de volta pela fila um número máximo de vezes (indicando falha no processamento), o SQS a moverá automaticamente para a DLQ."
  },
  {
    "id": "aws-co-088",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa de uma maneira de controlar o tráfego de saída de suas instâncias EC2, especificando quais domínios (ex: `api.github.com`) elas podem acessar na internet.",
    "solution": ["network firewall"],
    "availableServices": ["network firewall", "security group", "nacl", "nat gateway"],
    "explanation": "Enquanto Security Groups e NACLs operam nas camadas 3 e 4 (IP e porta), o AWS Network Firewall é um serviço de firewall gerenciado que permite a filtragem na camada 7. Você pode criar regras que inspecionam o tráfego e permitem ou negam com base no nome de domínio (FQDN), e não apenas no endereço IP."
  },
  {
    "id": "aws-co-089",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Sua aplicação requer um número muito grande de IOPS (operações de I/O por segundo) para seu banco de dados. Qual é o tipo de volume EBS de maior performance disponível?",
    "solution": ["ebs io2 block express"],
    "availableServices": ["ebs io2 block express", "ebs gp3", "instance store", "efs"],
    "explanation": "Os volumes EBS `io2 Block Express` são a última geração de armazenamento de alta performance, oferecendo a maior performance e a menor latência dos volumes EBS. Eles são projetados para as cargas de trabalho mais exigentes, como grandes bancos de dados relacionais e NoSQL."
  },
  {
    "id": "aws-co-090",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa de um serviço que possa detectar ameaças de segurança monitorando continuamente seus logs de CloudTrail, logs de VPC Flow e logs de DNS em busca de atividades maliciosas ou não autorizadas.",
    "solution": ["guardduty"],
    "availableServices": ["guardduty", "inspector", "macie", "aws config", "cloudwatch"],
    "explanation": "O Amazon GuardDuty é um serviço de detecção de ameaças que usa inteligência de ameaças e machine learning para monitorar continuamente atividades maliciosas. Ele analisa várias fontes de dados da AWS sem a necessidade de instalar agentes, identificando ameaças como instâncias comprometidas, reconhecimento de rede ou exfiltração de dados."
  },
  {
    "id": "aws-co-091",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você precisa garantir que os dados de um banco de dados Aurora sejam copiados para outra região da AWS para fins de recuperação de desastres, com um RPO (Recovery Point Objective) de segundos.",
    "solution": ["aurora global database"],
    "availableServices": ["aurora global database", "dms", "rds snapshots", "aws backup"],
    "explanation": "O Amazon Aurora Global Database é projetado para aplicações globalmente distribuídas e recuperação de desastres. Ele consiste em um cluster primário e até cinco clusters secundários somente leitura em outras regiões. A replicação entre as regiões é feita na camada de armazenamento, com latência típica de menos de um segundo, proporcionando um RPO muito baixo."
  },
  {
    "id": "aws-co-092",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa de uma maneira de executar uma função Lambda em uma programação fixa, por exemplo, a cada 15 minutos.",
    "solution": ["eventbridge", "lambda"],
    "availableServices": ["eventbridge", "lambda", "sqs", "step-functions"],
    "explanation": "O Amazon EventBridge é a solução padrão para acionar eventos com base em uma programação. Você pode criar uma 'regra de agendamento' (schedule rule) no EventBridge com uma expressão de taxa (`rate(15 minutes)`) ou uma expressão cron, e definir a função Lambda como o alvo da regra."
  },
  {
    "id": "aws-co-093",
    "category": "cloud-operations",
    "difficulty": 9,
    "question": "Um ataque de negação de serviço (DDoS) na camada de aplicação (HTTP flood) está sobrecarregando seu Application Load Balancer. Como você pode mitigar esse ataque automaticamente?",
    "solution": ["waf", "aws shield advanced", "elastic load balancer"],
    "availableServices": ["waf", "aws shield advanced", "elastic load balancer", "security group", "nacl"],
    "explanation": "O AWS Shield Advanced oferece proteção aprimorada contra ataques DDoS, incluindo detecção e mitigação na camada de aplicação. Ele se integra ao AWS WAF, permitindo a criação de regras baseadas em taxa (rate-based rules) que bloqueiam automaticamente os endereços IP de origem que fazem um número excessivo de solicitações, mitigando o HTTP flood."
  },
  {
    "id": "aws-co-094",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você precisa de um painel que mostre a saúde geral de todos os seus serviços AWS em todas as regiões, incluindo informações sobre interrupções de serviço e eventos operacionais.",
    "solution": ["aws health dashboard"],
    "availableServices": ["aws health dashboard", "cloudwatch", "trusted advisor", "service catalog"],
    "explanation": "O AWS Health Dashboard fornece uma visão personalizada da saúde dos serviços da AWS. Ele exibe informações sobre eventos em andamento que podem afetar sua infraestrutura (Personal Health Dashboard) e o status geral de todos os serviços (Service Health Dashboard)."
  },
  {
    "id": "aws-co-095",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Como você pode garantir que uma instância EC2 sempre se recupere automaticamente se falhar em uma verificação de saúde do sistema (ex: perda de conectividade de rede, falha de hardware)?",
    "solution": ["ec2", "cloudwatch"],
    "availableServices": ["ec2", "cloudwatch", "auto scaling group", "lambda"],
    "explanation": "Você pode criar um Alarme do CloudWatch que monitora a métrica 'StatusCheckFailed_System'. A ação desse alarme pode ser definida como 'Recover this instance'. Quando o alarme é acionado, a EC2 tenta recuperar a instância automaticamente, migrando-a para um novo hardware saudável enquanto mantém o ID da instância, o IP e os volumes."
  },
  {
    "id": "aws-co-096",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Uma aplicação precisa de acesso de baixa latência a um conjunto de dados de referência que é armazenado no S3. Ler do S3 a cada vez é muito lento. A solução deve ser um cache gerenciado na instância EC2.",
    "solution": ["s3", "ec2", "file cache"],
    "availableServices": ["s3", "ec2", "file cache", "efs", "elasticache"],
    "explanation": "O AWS File Cache é um cache de alta velocidade na AWS que facilita o processamento de arquivos armazenados em locais diferentes, incluindo on-premises e o Amazon S3. Ele fornece uma visão unificada e acesso rápido aos dados para suas aplicações de computação, agindo como um cache de leitura de baixa latência."
  },
  {
    "id": "aws-co-097",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa criar um registro DNS, como `api.suaempresa.com`, que aponte para um Application Load Balancer.",
    "solution": ["route 53", "elastic load balancer"],
    "availableServices": ["route 53", "elastic load balancer", "ec2", "cloudfront"],
    "explanation": "No Amazon Route 53, você cria um conjunto de registros (record set) na sua zona hospedada. Em vez de um registro 'A' com um IP, você usa um registro 'Alias'. Os registros Alias são um recurso do Route 53 que permite mapear seu domínio para recursos da AWS, como um ELB, de forma nativa e eficiente."
  },
  {
    "id": "aws-co-098",
    "category": "cloud-operations",
    "difficulty": 10,
    "question": "Você precisa implantar uma aplicação 'canary' para uma função Lambda. 10% do tráfego de produção deve ser direcionado para a nova versão da função por uma hora. Se a taxa de erros permanecer baixa, todo o tráfego deve ser transferido para a nova versão.",
    "solution": ["lambda", "codedeploy", "cloudwatch"],
    "availableServices": ["lambda", "codedeploy", "cloudwatch", "api gateway", "step-functions"],
    "explanation": "Esta é uma implantação 'Canary Linear'. Você usa 'Aliases' da Lambda. O AWS CodeDeploy pode gerenciar o processo de implantação, deslocando gradualmente o tráfego do alias da versão antiga para a nova. O CodeDeploy pode ser configurado com 'hooks' de validação e monitorar alarmes do CloudWatch, revertendo automaticamente a implantação se a nova versão gerar erros."
  },
  {
    "id": "aws-co-099",
    "category": "cloud-operations",
    "difficulty": 8,
    "question": "Você precisa transferir arquivos de um servidor SFTP externo para um bucket S3 em uma programação regular, de forma totalmente gerenciada, sem provisionar servidores.",
    "solution": ["aws transfer family"],
    "availableServices": ["aws transfer family", "s3", "lambda", "ec2", "dataSync"],
    "explanation": "O AWS Transfer Family é um serviço totalmente gerenciado que suporta transferências de arquivos via SFTP, FTPS e FTP diretamente para o Amazon S3. Você pode usá-lo para criar um endpoint SFTP e, em seguida, usar fluxos de trabalho gerenciados para automatizar a busca de arquivos de um servidor SFTP remoto e salvá-los no S3."
  },
  {
    "id": "aws-co-100",
    "category": "cloud-operations",
    "difficulty": 7,
    "question": "Você precisa de uma visão geral de sua conformidade com frameworks como CIS Benchmarks ou PCI DSS, com verificações automáticas e pontuação de segurança.",
    "solution": ["security hub"],
    "availableServices": ["security hub", "aws config", "inspector", "trusted advisor", "guardduty"],
    "explanation": "O AWS Security Hub executa verificações de segurança contínuas e automáticas com base nas melhores práticas da AWS e em padrões da indústria. Ele fornece uma pontuação de segurança e visualiza sua conformidade com padrões como CIS AWS Foundations Benchmark e PCI DSS."
  }


            ],
            Azure: [        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Hospede um site estático (HTML, CSS, JS) de forma econômica e escalável.",
            "solution": ["blob-storage"],
            "availableServices": ["blob-storage", "app-service", "vm", "functions", "cdn"],
            "explanation": "O Azure Blob Storage possui um recurso chamado 'Static website hosting' que permite servir arquivos estáticos diretamente do armazenamento, sendo a opção mais barata e simples para este caso."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Crie uma máquina virtual Linux para hospedar um servidor web simples.",
            "solution": ["vm"],
            "availableServices": ["vm", "app-service", "aks", "container-apps"],
            "explanation": "O Azure Virtual Machines (VM) oferece controle total sobre o sistema operacional e o ambiente, sendo a escolha clássica para hospedar software tradicional como um servidor web Apache ou Nginx."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Armazene segredos, chaves e certificados de uma aplicação de forma segura.",
            "solution": ["key-vault"],
            "availableServices": ["key-vault", "app-configuration", "blob-storage", "devops"],
            "explanation": "O Azure Key Vault é um serviço projetado especificamente para o armazenamento seguro e o controle de acesso a tokens, senhas, certificados e chaves de API, com suporte a HSM."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Crie um alerta para notificar um administrador quando o uso de CPU de uma VM ultrapassar 90%.",
            "solution": ["vm", "monitor"],
            "availableServices": ["vm", "monitor", "log-analytics", "advisor", "security-center"],
            "explanation": "O Azure Monitor coleta métricas de todas as VMs. Você pode criar uma 'Regra de Alerta' no Monitor que avalia a métrica de CPU e, quando o limiar é atingido, aciona um 'Grupo de Ação' para enviar um e-mail."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Execute um script PowerShell agendado para rodar uma vez por dia e realizar uma tarefa de limpeza.",
            "solution": ["automation-account"],
            "availableServices": ["automation-account", "functions", "vm", "logic-apps"],
            "explanation": "O Azure Automation Account é projetado para automação de processos e gerenciamento de configuração. Ele permite criar 'Runbooks' (scripts) e agendá-los para execução regular."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Crie uma API para expor dados de um banco SQL como um endpoint REST seguro e gerenciado.",
            "solution": ["sql-database", "api-management", "functions"],
            "availableServices": ["sql-database", "api-management", "functions", "cosmos-db", "logic-apps", "service-bus"],
            "explanation": "Uma Azure Function lê os dados do SQL Database. O API Management é colocado na frente da função para atuar como um gateway, adicionando segurança (chaves, OAuth), cache e monitoramento."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Faça o build de um projeto de código-fonte a partir de um repositório Git.",
            "solution": ["devops"],
            "availableServices": ["devops", "github", "app-service", "vm"],
            "explanation": "O Azure DevOps Pipelines (ou GitHub Actions) pode ser configurado para monitorar um repositório Git. A cada novo commit, ele automaticamente executa um pipeline de build para compilar o código e gerar os artefatos."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Proteja uma máquina virtual do tráfego de entrada indesejado da internet, liberando apenas a porta 443 (HTTPS).",
            "solution": ["vm", "nsg"],
            "availableServices": ["vm", "nsg", "firewall", "app-gateway", "load-balancer"],
            "explanation": "Um Network Security Group (NSG) atua como um firewall de stateful para recursos em uma VNet. Você pode criar regras de entrada para permitir tráfego na porta 443 e negar todo o resto."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Crie um banco de dados relacional gerenciado (MySQL) para uma aplicação web PHP.",
            "solution": ["mysql-database"],
            "availableServices": ["mysql-database", "sql-database", "cosmos-db", "vm"],
            "explanation": "O Azure Database for MySQL é um serviço de banco de dados gerenciado (PaaS) que simplifica o provisionamento, backup e manutenção de um servidor MySQL na nuvem, ideal para aplicações como WordPress ou Drupal."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Centralize a autenticação de usuários para várias aplicações usando um serviço de identidade.",
            "solution": ["azure-ad"],
            "availableServices": ["azure-ad", "key-vault", "api-management", "app-service"],
            "explanation": "O Azure Active Directory (Azure AD / Entra ID) é o serviço de identidade e gerenciamento de acesso da Microsoft. Ele permite configurar Single Sign-On (SSO) para que os usuários se autentiquem uma vez e acessem múltiplas aplicações."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Distribua conteúdo estático (imagens, vídeos) globalmente para usuários com baixa latência.",
            "solution": ["blob-storage", "cdn"],
            "availableServices": ["blob-storage", "cdn", "app-service", "front-door"],
            "explanation": "O Blob Storage armazena os arquivos de origem. O Azure CDN (Content Delivery Network) armazena cópias desses arquivos em pontos de presença (PoPs) ao redor do mundo, entregando o conteúdo a partir do local mais próximo do usuário, o que reduz a latência."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Armazene logs de diagnóstico de um App Service para análise posterior.",
            "solution": ["app-service", "log-analytics"],
            "availableServices": ["app-service", "log-analytics", "monitor", "blob-storage"],
            "explanation": "Nas configurações de diagnóstico de um App Service, você pode configurar o envio de logs (como logs de aplicação e de servidor web) para um Log Analytics Workspace. Este serviço centraliza e permite a consulta dos logs usando a linguagem KQL."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Crie um repositório de código privado para sua equipe de desenvolvimento.",
            "solution": ["devops"],
            "availableServices": ["devops", "github", "blob-storage", "app-service"],
            "explanation": "O Azure Repos, parte do Azure DevOps, oferece repositórios Git privados e ilimitados para armazenar e versionar o código-fonte da sua equipe, com integração com o restante do ciclo de vida do desenvolvimento."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Como você pode obter recomendações da Azure para otimizar custos e performance dos seus recursos?",
            "solution": ["advisor"],
            "availableServices": ["advisor", "monitor", "cost-management", "policy"],
            "explanation": "O Azure Advisor é um consultor de nuvem personalizado que analisa sua configuração de recursos e oferece recomendações para melhorar a confiabilidade, segurança, excelência operacional, desempenho e custo."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Crie uma aplicação de lógica simples que envia um e-mail quando um novo arquivo é adicionado a uma pasta no OneDrive.",
            "solution": ["logic-apps"],
            "availableServices": ["logic-apps", "functions", "automation-account", "event-grid"],
            "explanation": "O Azure Logic Apps é uma plataforma de integração visual que permite criar fluxos de trabalho usando conectores pré-construídos. Ele possui conectores para OneDrive e Office 365, tornando essa automação simples de construir."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Crie um pipeline de CI/CD para uma aplicação web, fazendo o build do código e o deploy em um App Service.",
            "solution": ["devops", "app-service"],
            "availableServices": ["devops", "app-service", "aks", "functions", "blob-storage", "monitor", "api-management"],
            "explanation": "O Azure DevOps Pipelines pode ser configurado para compilar o código (build) e gerar um artefato. Uma 'Release Pipeline' então pega esse artefato e o implanta automaticamente no Azure App Service."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Hospede uma aplicação web .NET que se conecta a um banco de dados SQL gerenciado.",
            "solution": ["app-service", "sql-database"],
            "availableServices": ["app-service", "sql-database", "vm", "aks"],
            "explanation": "O Azure App Service é a plataforma PaaS ideal para hospedar aplicações web .NET. Ele se integra nativamente com o Azure SQL Database, permitindo uma conexão segura e gerenciada."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Crie uma função que é acionada sempre que uma nova imagem é carregada no Blob Storage para gerar um thumbnail.",
            "solution": ["blob-storage", "functions"],
            "availableServices": ["blob-storage", "functions", "logic-apps", "event-grid"],
            "explanation": "O Azure Functions possui um 'gatilho' (trigger) nativo para o Blob Storage. Esse gatilho inicia a execução da função sempre que um novo blob é criado, passando o blob como entrada para a lógica de processamento de imagem."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Monitore a performance de uma aplicação web, rastreando exceções no lado do servidor e tempos de carregamento no cliente.",
            "solution": ["app-service", "application-insights"],
            "availableServices": ["app-service", "application-insights", "monitor", "log-analytics"],
            "explanation": "O Application Insights é um serviço de APM (Application Performance Management). Ao integrá-lo a um App Service, ele coleta automaticamente telemetria detalhada, incluindo exceções, dependências e performance do lado do cliente."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Distribua o tráfego de rede igualmente entre duas máquinas virtuais que hospedam a mesma aplicação.",
            "solution": ["load-balancer", "vm"],
            "availableServices": ["load-balancer", "vm", "app-gateway", "traffic-manager", "nsg"],
            "explanation": "O Azure Load Balancer opera na camada 4 (TCP/UDP) e distribui o tráfego entre as VMs em um 'backend pool'. Ele garante que, se uma VM falhar, o tráfego seja direcionado para as VMs saudáveis."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Crie um registro privado para armazenar e gerenciar imagens de contêiner Docker.",
            "solution": ["container-registry"],
            "availableServices": ["container-registry", "devops", "aks", "blob-storage"],
            "explanation": "O Azure Container Registry (ACR) é um serviço gerenciado de registro Docker privado. Ele se integra com ferramentas de CI/CD como o Azure DevOps e serviços de computação como AKS e App Service."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Conecte de forma segura uma rede virtual (VNet) no Azure a uma rede corporativa on-premises.",
            "solution": ["vnet", "vpn-gateway"],
            "availableServices": ["vnet", "vpn-gateway", "expressroute", "firewall"],
            "explanation": "Um VPN Gateway permite criar uma conexão segura (túnel IPsec/IKE) entre a VNet do Azure e sua rede local através da internet pública, estendendo sua rede para a nuvem."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Migre um banco de dados SQL Server on-premises para a Azure sem tempo de inatividade significativo.",
            "solution": ["database-migration-service", "sql-database"],
            "availableServices": ["database-migration-service", "sql-database", "data-factory", "vm"],
            "explanation": "O Azure Database Migration Service (DMS) pode realizar migrações online, onde ele faz uma carga inicial e depois sincroniza continuamente as alterações do banco de dados de origem para o destino no Azure SQL Database."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Automatize a criação de uma máquina virtual e a instalação de um servidor web usando um template declarativo.",
            "solution": ["arm-template", "vm"],
            "availableServices": ["arm-template", "bicep", "vm", "devops"],
            "explanation": "Um ARM Template (ou Bicep) é um arquivo JSON (ou Bicep) que define os recursos do Azure a serem implantados (a VM, seu disco, interface de rede). Ele pode incluir scripts de extensão para instalar software automaticamente após a criação da VM."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Crie uma fila de mensagens para desacoplar um serviço que produz tarefas de outro que as consome.",
            "solution": ["service-bus", "functions"],
            "availableServices": ["service-bus", "functions", "event-grid", "queue-storage"],
            "explanation": "O Azure Service Bus Queues oferece uma fila de mensagens FIFO (primeiro a entrar, primeiro a sair) com semântica de entrega garantida. Uma Azure Function com um gatilho do Service Bus pode processar essas mensagens de forma confiável."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Implemente um cache em memória para acelerar o acesso a dados frequentemente consultados por uma aplicação web.",
            "solution": ["app-service", "redis"],
            "availableServices": ["app-service", "redis", "cosmos-db", "sql-database"],
            "explanation": "O Azure Cache for Redis fornece um serviço de cache em memória gerenciado. A aplicação no App Service pode se conectar a ele para armazenar e recuperar dados rapidamente, reduzindo a carga sobre o banco de dados principal."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Proteja uma API gerenciada exigindo uma chave de assinatura para cada chamada.",
            "solution": ["api-management"],
            "availableServices": ["api-management", "app-gateway", "functions", "azure-ad"],
            "explanation": "No Azure API Management, você pode criar 'produtos' que agrupam APIs e associar 'assinaturas' a eles. As aplicações cliente devem então incluir a chave de assinatura no cabeçalho da requisição para serem autorizadas pelo gateway."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Faça o backup automático de uma máquina virtual inteira, incluindo seus discos.",
            "solution": ["vm", "backup"],
            "availableServices": ["vm", "backup", "site-recovery", "blob-storage"],
            "explanation": "O Azure Backup é um serviço que permite configurar políticas de backup para VMs. Ele pode tirar snapshots consistentes com a aplicação da VM e seus discos em um cronograma definido, armazenando-os em um Recovery Services Vault."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Centralize a coleta de logs de várias fontes (VMs, App Services) em um único repositório para análise.",
            "solution": ["log-analytics"],
            "availableServices": ["log-analytics", "monitor", "event-hubs", "blob-storage"],
            "explanation": "Um Log Analytics Workspace é o ambiente principal para coletar e analisar dados do Azure Monitor. Você pode configurar VMs e outros serviços para enviar seus logs para um workspace central, onde podem ser consultados com a linguagem KQL."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Projete uma arquitetura de microsserviços desacoplada onde um serviço publica um evento e múltiplos serviços assinantes reagem a ele.",
            "solution": ["functions", "event-grid", "logic-apps"],
            "availableServices": ["functions", "event-grid", "logic-apps", "service-bus", "aks", "api-management"],
            "explanation": "O Azure Event Grid é um serviço de roteamento de eventos. Um serviço publica um evento no Event Grid. Múltiplos assinantes, como Azure Functions e Logic Apps, podem se inscrever nesse evento e serem acionados de forma independente quando o evento ocorre."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Implante e gerencie uma aplicação em contêineres usando Kubernetes de forma gerenciada.",
            "solution": ["devops", "container-registry", "aks"],
            "availableServices": ["devops", "container-registry", "aks", "app-service", "container-apps"],
            "explanation": "O Azure Kubernetes Service (AKS) fornece um cluster Kubernetes gerenciado. Um pipeline do Azure DevOps constrói a imagem do contêiner, a envia para o Azure Container Registry e, em seguida, usa `kubectl` ou Helm para implantá-la no cluster AKS."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Crie um pipeline de release com estágios de Dev, QA e Produção, exigindo aprovação manual para o deploy em produção.",
            "solution": ["devops"],
            "availableServices": ["devops", "github-actions", "app-service", "aks"],
            "explanation": "Nas Release Pipelines do Azure DevOps, você pode definir múltiplos estágios. É possível configurar 'aprovações pré-implantação' no estágio de Produção, onde o pipeline pausará e exigirá que um aprovador autorizado aprove manualmente antes de continuar."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Implemente um Web Application Firewall (WAF) para proteger uma aplicação web contra ataques comuns como SQL Injection e Cross-Site Scripting.",
            "solution": ["app-gateway", "app-service"],
            "availableServices": ["app-gateway", "app-service", "load-balancer", "firewall", "nsg"],
            "explanation": "O Azure Application Gateway é um balanceador de carga da camada 7 que inclui um WAF. O tráfego para o App Service é roteado através do Application Gateway, que inspeciona as requisições e bloqueia ameaças com base em regras (ex: OWASP Core Rule Set)."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Construa uma topologia de rede Hub-and-Spoke, onde múltiplas redes virtuais se conectam a uma VNet central para inspeção de tráfego.",
            "solution": ["vnet", "firewall"],
            "availableServices": ["vnet", "firewall", "vpn-gateway", "nsg", "peering"],
            "explanation": "A VNet Hub contém serviços compartilhados como o Azure Firewall. As VNets Spoke são conectadas à Hub via 'VNet Peering'. Todo o tráfego entre as spokes ou para a internet é forçado a passar pelo Firewall na Hub para inspeção e controle centralizado."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Automatize a configuração de um ambiente de desenvolvimento completo (VM, rede, software) usando Infraestrutura como Código.",
            "solution": ["bicep", "devops"],
            "availableServices": ["bicep", "arm-template", "devops", "powershell", "cli"],
            "explanation": "O Bicep (ou ARM Templates) define a infraestrutura. Um pipeline do Azure DevOps pode ser usado para implantar esse template Bicep, provisionando a rede, a VM e usando extensões de VM para instalar o software necessário, tudo de forma automatizada."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Proteja uma API com autenticação OAuth 2.0, garantindo que apenas usuários autenticados possam acessá-la.",
            "solution": ["api-management", "azure-ad", "functions"],
            "availableServices": ["api-management", "azure-ad", "functions", "key-vault"],
            "explanation": "O Azure AD atua como o provedor de identidade, emitindo tokens JWT. O API Management é configurado para validar esses tokens em cada requisição antes de encaminhá-la para a Azure Function de backend, garantindo que apenas chamadas autenticadas sejam processadas."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Crie uma estratégia de recuperação de desastres para uma aplicação crítica, replicando VMs para uma região secundária.",
            "solution": ["vm", "site-recovery"],
            "availableServices": ["vm", "site-recovery", "backup", "load-balancer"],
            "explanation": "O Azure Site Recovery orquestra a replicação de VMs de uma região primária para uma secundária. Em caso de desastre, você pode acionar um 'failover' para ativar as VMs na região secundária e restaurar o serviço."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Ingira um grande volume de dados de telemetria de dispositivos IoT em tempo real para processamento.",
            "solution": ["iot-hub", "event-hubs", "stream-analytics"],
            "availableServices": ["iot-hub", "event-hubs", "stream-analytics", "functions", "service-bus"],
            "explanation": "O IoT Hub é o ponto de entrada para dispositivos IoT. Ele pode rotear os dados para um Event Hub, que é otimizado para ingestão de big data em streaming. O Stream Analytics pode então consumir desses eventos para processamento e análise em tempo real."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Enforce uma política que proíbe a criação de VMs de tamanhos muito grandes e caros em uma subscription de desenvolvimento.",
            "solution": ["policy"],
            "availableServices": ["policy", "advisor", "monitor", "azure-ad"],
            "explanation": "O Azure Policy permite criar, atribuir e gerenciar políticas de governança. Você pode usar uma política interna ('Allowed virtual machine size SKUs') para especificar uma lista de tamanhos de VM permitidos e aplicá-la a uma subscription ou grupo de recursos."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Ofereça acesso Just-In-Time (JIT) para administradores a portas de gerenciamento de VMs (RDP/SSH) para reduzir a superfície de ataque.",
            "solution": ["defender-for-cloud", "vm"],
            "availableServices": ["defender-for-cloud", "vm", "nsg", "bastion"],
            "explanation": "O Microsoft Defender for Cloud oferece o recurso de acesso a VM Just-In-Time. Ele mantém as portas de gerenciamento fechadas por padrão e permite que os usuários solicitem acesso por um período limitado, abrindo a porta apenas para o IP do solicitante."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Gerencie a configuração de um grupo de VMs, garantindo que todas tenham um estado desejado (Desired State Configuration).",
            "solution": ["automation-account", "vm"],
            "availableServices": ["automation-account", "vm", "functions", "policy"],
            "explanation": "O Azure Automation State Configuration (baseado em PowerShell DSC) permite definir uma configuração desejada para suas VMs. As VMs verificam periodicamente o Automation Account e, se sua configuração divergir da desejada, ela é corrigida automaticamente."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Implante uma aplicação em contêineres de forma serverless, escalando de zero a N instâncias com base no tráfego HTTP, protegendo segredos no Key Vault.",
            "solution": ["devops", "container-registry", "container-apps", "key-vault"],
            "availableServices": ["devops", "container-registry", "container-apps", "key-vault", "aks", "app-service"],
            "explanation": "O Azure Container Apps é uma plataforma serverless para contêineres. O pipeline do DevOps cria a imagem, a armazena no Container Registry e a implanta no Container Apps. A aplicação pode escalar a zero e buscar seus segredos de forma segura do Key Vault usando uma identidade gerenciada."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Crie um pipeline de DevSecOps que inclui análise estática de código (SAST) e verificação de vulnerabilidades em dependências de software.",
            "solution": ["devops", "defender-for-cloud"],
            "availableServices": ["devops", "defender-for-cloud", "github", "monitor"],
            "explanation": "No Azure DevOps, você pode adicionar tarefas ao seu pipeline de build que integram ferramentas de segurança. O Microsoft Defender for DevOps pode escanear o código em busca de segredos e vulnerabilidades, e outras ferramentas podem analisar as dependências de código aberto."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Projete uma arquitetura para uma aplicação de e-commerce com um front-end em App Service, microsserviços em AKS e um banco de dados NoSQL globalmente distribuído.",
            "solution": ["app-service", "aks", "cosmos-db"],
            "availableServices": ["app-service", "aks", "cosmos-db", "sql-database", "functions"],
            "explanation": "O App Service hospeda o front-end web. A lógica de negócio é dividida em microsserviços no AKS para escalabilidade. O Cosmos DB é usado como banco de dados NoSQL, com sua capacidade de distribuição global e multi-master para garantir baixa latência de leitura/escrita em todo o mundo."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Implemente uma estratégia de deployment Blue-Green para uma aplicação no App Service para permitir releases com zero downtime.",
            "solution": ["app-service", "devops"],
            "availableServices": ["app-service", "devops", "traffic-manager", "aks"],
            "explanation": "O App Service usa 'deployment slots'. O slot de produção (azul) está ativo. O pipeline do DevOps implanta a nova versão no slot de 'staging' (verde). Após os testes, você pode realizar um 'swap' dos slots, que redireciona o tráfego instantaneamente para a nova versão sem tempo de inatividade."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Gerencie servidores on-premises (Windows e Linux) através do portal do Azure, aplicando políticas e monitoramento como se fossem recursos nativos da nuvem.",
            "solution": ["arc", "monitor"],
            "availableServices": ["arc", "monitor", "vpn-gateway", "site-recovery"],
            "explanation": "O Azure Arc estende o plano de gerenciamento do Azure para servidores em qualquer lugar. Ao instalar o agente do Arc em um servidor on-premises, ele aparece como um recurso no Azure, permitindo aplicar Azure Policy, coletar logs com o Azure Monitor e gerenciá-lo de forma centralizada."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Configure um SIEM (Security Information and Event Management) na nuvem para coletar e correlacionar eventos de segurança de diversas fontes (Azure, M365, on-premises).",
            "solution": ["sentinel", "log-analytics"],
            "availableServices": ["sentinel", "log-analytics", "defender-for-cloud", "monitor"],
            "explanation": "O Microsoft Sentinel é um serviço SIEM e SOAR nativo da nuvem. Ele usa um Log Analytics Workspace como base para coletar dados de segurança de conectores (Azure, Office 365, firewalls) e usa IA para correlacionar eventos e detectar ameaças."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Crie um balanceador de carga global que direcione os usuários para a região mais próxima e faça failover automático em caso de falha regional.",
            "solution": ["front-door", "app-service"],
            "availableServices": ["front-door", "app-gateway", "traffic-manager", "load-balancer"],
            "explanation": "O Azure Front Door é um balanceador de carga global da camada 7. Ele roteia o tráfego do cliente para o backend regional mais rápido (de menor latência) e monitora a saúde dos backends, fazendo o failover automaticamente para outra região se a primária falhar."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Proteja uma VNet contra ataques de DDoS (Distributed Denial of Service) em nível de rede.",
            "solution": ["vnet", "ddos-protection"],
            "availableServices": ["vnet", "ddos-protection", "firewall", "app-gateway", "nsg"],
            "explanation": "O Azure DDoS Protection Standard, quando habilitado em uma VNet, fornece mitigação aprimorada contra ataques DDoS volumétricos. Ele aprende os padrões de tráfego da aplicação e usa machine learning para detectar e mitigar ataques automaticamente."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Construa uma 'landing zone' usando um conjunto de políticas e configurações padronizadas para garantir que todas as novas subscriptions sigam as melhores práticas de governança.",
            "solution": ["blueprints", "policy", "management-groups"],
            "availableServices": ["blueprints", "policy", "management-groups", "advisor", "arm-template"],
            "explanation": "Os Management Groups organizam as subscriptions. O Azure Blueprints permite empacotar artefatos (como atribuições de Azure Policy, templates ARM, RBAC) e aplicá-los a um Management Group, garantindo que qualquer nova subscription nesse grupo herde a governança definida."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Acesse com segurança e sem expor portas RDP/SSH públicas uma máquina virtual dentro de uma VNet privada a partir do portal Azure.",
            "solution": ["vm", "bastion"],
            "availableServices": ["vm", "bastion", "vpn-gateway", "nsg", "firewall"],
            "explanation": "O Azure Bastion é um serviço PaaS que fornece acesso RDP/SSH seguro às suas VMs diretamente através do portal do Azure. Ele é provisionado em sua VNet e atua como um 'jump host' gerenciado, sem a necessidade de expor IPs públicos nas VMs."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Crie uma arquitetura de aplicação web globalmente distribuída com balanceamento de carga, WAF, e autenticação de usuários centralizada para alta disponibilidade.",
            "solution": ["front-door", "app-service", "cosmos-db", "azure-ad"],
            "availableServices": ["front-door", "app-service", "cosmos-db", "azure-ad", "app-gateway", "sql-database", "cdn"],
            "explanation": "O Front Door faz o balanceamento global e aplica o WAF. Os App Services são implantados em múltiplas regiões. O Cosmos DB lida com a replicação de dados globalmente. O Azure AD fornece uma identidade única para os usuários em todas as regiões."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Projete uma arquitetura de 'zero trust' para acesso a aplicações internas, onde cada requisição é autenticada e autorizada, independentemente da origem da rede.",
            "solution": ["azure-ad", "app-proxy", "conditional-access", "defender-for-cloud"],
            "availableServices": ["azure-ad", "app-proxy", "conditional-access", "defender-for-cloud", "vpn-gateway", "firewall"],
            "explanation": "O Azure AD Application Proxy expõe aplicações internas de forma segura. O Acesso Condicional (Conditional Access) do Azure AD verifica a identidade, o dispositivo e o contexto do usuário em cada requisição antes de conceder acesso, implementando o princípio de 'nunca confiar, sempre verificar'."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Implemente uma solução completa de FinOps (Cloud Financial Operations) para monitorar, alocar e otimizar os custos de nuvem em uma grande organização.",
            "solution": ["cost-management", "advisor", "policy", "budgets"],
            "availableServices": ["cost-management", "advisor", "policy", "budgets", "monitor", "log-analytics"],
            "explanation": "O Cost Management é usado para visualizar e analisar os custos, atribuídos por tags. O Advisor dá recomendações de otimização. O Azure Policy é usado para impor o uso de tags e restringir SKUs caros. Os Budgets são usados para alertar quando os gastos se aproximam dos limites."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Crie um pipeline de implantação de 'Infrastructure as Code' que provisiona um ambiente Kubernetes (AKS) e um cluster de banco de dados, testa a conectividade e, em caso de falha, executa o rollback completo da infraestrutura.",
            "solution": ["bicep", "devops", "aks", "sql-database"],
            "availableServices": ["bicep", "devops", "aks", "sql-database", "powershell", "arm-template"],
            "explanation": "O pipeline do Azure DevOps usa uma tarefa para implantar o template Bicep que define o AKS e o SQL. Uma etapa subsequente executa testes automatizados. Se os testes falharem, uma etapa de 'rollback' é acionada, executando um comando para remover o grupo de recursos que foi criado, revertendo a implantação."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Projete uma arquitetura para uma aplicação de negociação financeira de alta frequência que requer latência de rede extremamente baixa entre os microsserviços e conectividade dedicada com a bolsa de valores.",
            "solution": ["vm", "proximity-placement-group", "expressroute"],
            "availableServices": ["vm", "proximity-placement-group", "expressroute", "aks", "vpn-gateway"],
            "explanation": "Um Proximity Placement Group garante que as VMs sejam fisicamente colocadas o mais próximo possível umas das outras em um data center para minimizar a latência. O ExpressRoute fornece uma conexão privada e de alta largura de banda com a bolsa de valores, evitando a internet pública."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Construa uma plataforma de 'data mesh' na Azure, onde diferentes domínios de negócio gerenciam seus próprios pipelines de dados (usando Data Factory) e os expõem como produtos de dados (via Purview).",
            "solution": ["data-factory", "purview", "synapse-analytics"],
            "availableServices": ["data-factory", "purview", "synapse-analytics", "databricks", "event-hubs"],
            "explanation": "Cada domínio usa o Data Factory e o Synapse para criar seus produtos de dados. O Purview atua como o catálogo de dados central, onde esses produtos são registrados e descobertos. O Purview também gerencia a governança e a linhagem de dados em toda a malha."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Desenvolva uma estratégia de migração para mover um data center inteiro para a Azure, incluindo a avaliação de servidores, planejamento de ondas de migração e a execução da movimentação de workloads.",
            "solution": ["migrate", "site-recovery", "database-migration-service"],
            "availableServices": ["migrate", "site-recovery", "database-migration-service", "advisor"],
            "explanation": "O Azure Migrate é usado para descobrir e avaliar os servidores on-premises. O planejamento define as 'ondas' de migração. O Azure Site Recovery é usado para replicar e migrar as VMs, e o Database Migration Service é usado para migrar os bancos de dados."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Implemente o controle de acesso a dados em um data lake de forma granular, permitindo que diferentes grupos de usuários do Azure AD acessem apenas colunas e linhas específicas de tabelas.",
            "solution": ["data-lake-storage", "synapse-analytics", "purview", "azure-ad"],
            "availableServices": ["data-lake-storage", "synapse-analytics", "purview", "azure-ad", "sql-database"],
            "explanation": "No Synapse Analytics, você pode usar a Segurança em Nível de Coluna e a Segurança em Nível de Linha (RLS) para controlar o acesso. As permissões são concedidas a funções do banco de dados, que por sua vez são associadas a grupos do Azure AD, garantindo um controle de acesso granular e baseado em identidade."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Crie um pipeline simples para copiar dados de um banco SQL on-premises para o Blob Storage na nuvem de forma agendada.",
            "solution": ["data-factory", "blob-storage"],
            "availableServices": ["data-factory", "blob-storage", "sql-database", "databricks", "synapse", "functions"],
            "explanation": "O Data Factory é o serviço de orquestração de ETL/ELT do Azure. Ele possui conectores para diversas fontes, incluindo SQL on-premises, e pode agendar atividades de cópia para destinos como o Blob Storage de forma robusta."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Use uma ferramenta de linha de comando para fazer o upload de uma grande pasta de arquivos do seu computador para o Azure Data Lake Storage.",
            "solution": ["azcopy", "data-lake-storage"],
            "availableServices": ["azcopy", "data-factory", "blob-storage", "storage-explorer"],
            "explanation": "O AzCopy é uma ferramenta de linha de comando otimizada para transferências de dados rápidas e confiáveis para o armazenamento do Azure, sendo a escolha ideal para uploads de grande volume."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Crie um pipeline no Data Factory que executa um notebook do Databricks para realizar uma transformação simples.",
            "solution": ["data-factory", "databricks"],
            "availableServices": ["data-factory", "databricks", "synapse", "functions", "logic-apps"],
            "explanation": "O Data Factory possui uma atividade nativa para executar notebooks do Databricks. Isso permite orquestrar o processamento Spark como uma etapa dentro de um pipeline de dados maior."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Ingira arquivos JSON de um Blob Storage e carregue-os em uma coleção do Cosmos DB.",
            "solution": ["data-factory", "cosmos-db"],
            "availableServices": ["data-factory", "cosmos-db", "functions", "event-hubs"],
            "explanation": "A atividade de cópia do Data Factory pode usar o Blob Storage como fonte e o Cosmos DB como destino, mapeando os dados do arquivo JSON para os documentos a serem inseridos na coleção."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Crie uma tabela em um pool SQL dedicado do Synapse Analytics para armazenar dados de clientes.",
            "solution": ["synapse"],
            "availableServices": ["synapse", "databricks", "sql-database", "data-lake-storage"],
            "explanation": "Um pool SQL dedicado no Synapse Analytics funciona como um Data Warehouse tradicional. Você pode usar a sintaxe T-SQL padrão, como `CREATE TABLE`, para definir a estrutura da sua tabela de clientes."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Como você pode consultar arquivos Parquet armazenados no Data Lake usando SQL sem provisionar infraestrutura?",
            "solution": ["synapse-serverless"],
            "availableServices": ["synapse-serverless", "databricks", "hdinsight", "sql-database"],
            "explanation": "Os pools SQL serverless do Azure Synapse Analytics permitem executar consultas SQL diretamente sobre os dados no Data Lake. É uma abordagem serverless, pagando apenas pelos dados processados em cada consulta."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Crie um fluxo de dados (Data Flow) simples no Data Factory para filtrar linhas de um arquivo CSV.",
            "solution": ["data-factory"],
            "availableServices": ["data-factory", "databricks", "functions", "synapse"],
            "explanation": "O Mapping Data Flows no Data Factory oferece uma interface visual para construir lógica de transformação de dados sem código. Você pode adicionar uma transformação de 'Filtro' para especificar a condição que as linhas devem atender."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Envie 1.000 mensagens para um tópico do Service Bus para processamento assíncrono.",
            "solution": ["service-bus"],
            "availableServices": ["service-bus", "event-hubs", "iot-hub", "queue-storage"],
            "explanation": "O Azure Service Bus é um broker de mensagens empresariais. Usando um SDK do Azure, você pode facilmente criar um cliente para enviar mensagens para um tópico, que pode então distribuí-las para múltiplas assinaturas."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Crie um Hub de Eventos para receber um fluxo contínuo de dados de telemetria.",
            "solution": ["event-hubs"],
            "availableServices": ["event-hubs", "iot-hub", "service-bus", "data-factory"],
            "explanation": "O Azure Event Hubs é uma plataforma de streaming de Big Data, projetada para ingerir milhões de eventos por segundo, sendo a escolha ideal para cenários de telemetria e streaming em larga escala."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Registre um data lake no Microsoft Purview para iniciar a descoberta de ativos de dados.",
            "solution": ["data-lake-storage", "purview"],
            "availableServices": ["data-lake-storage", "purview", "data-factory", "synapse"],
            "explanation": "O Microsoft Purview (agora parte do Microsoft Fabric) é o serviço de governança de dados. Você o conecta a suas fontes de dados, como o Data Lake Storage, e configura um 'scan' para que ele leia os metadados e popule o catálogo de dados."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Conecte o Power BI a uma view no Synapse Analytics para criar um relatório de vendas.",
            "solution": ["synapse", "power-bi"],
            "availableServices": ["synapse", "power-bi", "databricks", "data-lake-storage"],
            "explanation": "O Power BI possui um conector otimizado para o Azure Synapse Analytics. Ele pode se conectar a tabelas e views dentro de um pool SQL, permitindo que os analistas criem relatórios sobre os dados do data warehouse."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Orquestre um pipeline que ingere dados de vendas de arquivos CSV, usa o Databricks para limpá-los e agregá-los, e os carrega no Synapse Analytics.",
            "solution": ["data-lake-storage", "data-factory", "databricks", "synapse"],
            "availableServices": ["data-lake-storage", "data-factory", "databricks", "synapse", "functions", "event-hubs"],
            "explanation": "Os arquivos CSV são armazenados no Data Lake. O Data Factory orquestra o pipeline, primeiro acionando um notebook no Databricks para a limpeza e transformação. Após a conclusão, outra atividade no Data Factory carrega os dados processados no Synapse."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Crie um pipeline que é acionado sempre que um novo arquivo CSV chega no Blob Storage, transformando-o com Data Factory e salvando como Parquet.",
            "solution": ["blob-storage", "data-factory"],
            "availableServices": ["blob-storage", "data-factory", "logic-apps", "functions", "databricks"],
            "explanation": "No Data Factory, você pode criar um 'gatilho' (trigger) baseado em eventos de armazenamento. Este gatilho inicia a execução de um pipeline (que contém um Data Flow para a conversão de CSV para Parquet) sempre que um novo arquivo é criado no Blob Storage."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Desenvolva um pipeline ELT: copie dados de um banco de dados Oracle para o Data Lake e use o Synapse Serverless para transformá-los e carregá-los em um pool dedicado.",
            "solution": ["data-factory", "data-lake-storage", "synapse"],
            "availableServices": ["data-factory", "data-lake-storage", "synapse", "databricks", "hdinsight"],
            "explanation": "O Data Factory copia os dados brutos (Extract/Load) do Oracle para o Data Lake. Em seguida, um script SQL executado no pool serverless do Synapse lê os dados do lake, os transforma (Transform) e os insere no pool dedicado do Synapse."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Migre um job do SQL Server Integration Services (SSIS) para ser executado de forma gerenciada na nuvem.",
            "solution": ["data-factory"],
            "availableServices": ["data-factory", "databricks", "synapse", "vm"],
            "explanation": "O Azure Data Factory possui o 'SSIS Integration Runtime', que permite executar pacotes SSIS existentes na nuvem sem a necessidade de refatorá-los, facilitando a migração 'lift-and-shift' de cargas de trabalho de ETL."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Ingira dados de uma API REST pública (ex: dados do clima) diariamente e armazene-os no Data Lake Storage.",
            "solution": ["data-factory", "data-lake-storage"],
            "availableServices": ["data-factory", "data-lake-storage", "functions", "logic-apps"],
            "explanation": "O Data Factory possui uma atividade de cópia com um conector REST. Você pode configurar a URL da API, o método, os cabeçalhos e agendar a execução do pipeline para que ele busque os dados e os salve como arquivos no Data Lake Storage."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Processe um lote de imagens com Azure Functions para extrair metadados e armazene os resultados no Cosmos DB.",
            "solution": ["blob-storage", "functions", "cosmos-db"],
            "availableServices": ["blob-storage", "functions", "cosmos-db", "cognitive-services", "data-factory"],
            "explanation": "Um gatilho de Blob Storage em uma Azure Function inicia o processo quando uma imagem é carregada. A função pode usar bibliotecas para extrair metadados (como EXIF) e, em seguida, grava esses metadados como um documento JSON no Cosmos DB."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Crie um pipeline que ingere dados de log do Apache em arquivos de texto e os carrega em um cluster do Azure Data Explorer (Kusto) para análise interativa.",
            "solution": ["data-lake-storage", "data-factory", "data-explorer"],
            "availableServices": ["data-lake-storage", "data-factory", "data-explorer", "synapse", "log-analytics"],
            "explanation": "O Data Factory é usado para ler os arquivos de log do Data Lake. Ele pode analisar o formato do log e usar a atividade de cópia para carregar os dados em uma tabela no Azure Data Explorer, que é otimizado para análise de séries temporais e logs."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Use o Microsoft Purview para escanear um banco de dados SQL e classificar automaticamente colunas que contêm dados sensíveis (PII).",
            "solution": ["sql-database", "purview"],
            "availableServices": ["sql-database", "purview", "data-factory", "defender-for-cloud"],
            "explanation": "O Purview possui classificadores integrados para PII (como CPF, e-mail, etc.). Ao configurar um scan em um Azure SQL Database, ele analisa os dados e aplica automaticamente as classificações apropriadas às colunas, ajudando na governança."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Configure um cluster Spark no Azure Databricks para ler e gravar dados diretamente de/para um Azure Data Lake Storage usando credenciais seguras.",
            "solution": ["databricks", "data-lake-storage", "key-vault"],
            "availableServices": ["databricks", "data-lake-storage", "key-vault", "blob-storage", "synapse"],
            "explanation": "A melhor prática é usar o 'credential passthrough' ou uma 'Service Principal'. As credenciais da Service Principal são armazenadas de forma segura no Key Vault, e o Databricks as utiliza para se autenticar no Data Lake Storage, evitando o uso de chaves de acesso no código."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Construa um pipeline de streaming que ingere dados de cliques de um site via Event Hubs, processa-os em tempo real com Stream Analytics e armazena os resultados agregados no Cosmos DB.",
            "solution": ["event-hubs", "stream-analytics", "cosmos-db"],
            "availableServices": ["event-hubs", "stream-analytics", "cosmos-db", "synapse", "functions", "data-factory"],
            "explanation": "O Event Hubs ingere os cliques. Um job do Stream Analytics usa uma consulta SQL contínua para agregar os dados em janelas de tempo (ex: contagem de cliques por página por minuto) e envia os resultados para o Cosmos DB para acesso de baixa latência."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Implemente uma arquitetura Delta Lake no Databricks para criar um repositório de dados confiável e com suporte a transações ACID sobre o Data Lake.",
            "solution": ["data-lake-storage", "databricks"],
            "availableServices": ["data-lake-storage", "databricks", "synapse", "data-factory"],
            "explanation": "O Delta Lake é um formato de armazenamento open-source que traz confiabilidade (transações ACID) para o data lake. No Databricks, você pode ler dados de várias fontes e salvá-los no formato Delta no Data Lake Storage, permitindo operações de `UPDATE`, `DELETE` e `MERGE`."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Desenvolva um pipeline que processa dados de streaming do Kafka (em HDInsight) e usa o Spark Streaming para carregar os dados em um Data Warehouse Synapse.",
            "solution": ["hdinsight-kafka", "hdinsight-spark", "synapse"],
            "availableServices": ["hdinsight-kafka", "hdinsight-spark", "synapse", "event-hubs", "databricks"],
            "explanation": "Um cluster Kafka no HDInsight serve como fonte. Um job do Spark Streaming, em outro cluster HDInsight, consome os dados do Kafka, realiza as transformações em micro-lotes e usa o conector do Synapse para carregar os dados de forma eficiente no data warehouse."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Orquestre um pipeline complexo com múltiplas ramificações e dependências no Data Factory, onde a falha de uma ramificação não interrompe as outras.",
            "solution": ["data-factory"],
            "availableServices": ["data-factory", "logic-apps", "functions", "databricks"],
            "explanation": "No Data Factory, você pode configurar as dependências entre atividades. Para ramificações independentes, você pode ligá-las a partir de uma mesma atividade inicial. Para controlar o fluxo em caso de falha, você pode usar as setas de 'ao falhar', 'ao concluir' ou 'ao pular'."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Crie uma solução que usa o Azure Cognitive Search para indexar documentos (PDFs, DOCX) de um Data Lake, tornando-os pesquisáveis através de uma API.",
            "solution": ["data-lake-storage", "cognitive-search", "functions"],
            "availableServices": ["data-lake-storage", "cognitive-search", "functions", "purview", "data-factory"],
            "explanation": "O Azure Cognitive Search pode ter um 'indexer' que aponta para o Data Lake Storage. Ele pode extrair texto e metadados dos documentos. Uma Azure Function pode ser usada para enriquecer o índice com lógica customizada, e a API de busca é fornecida pelo próprio serviço."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Processe dados de telemetria de veículos em tempo real, enriquecendo-os com dados de geolocalização de um banco de dados SQL antes de exibi-los em um painel.",
            "solution": ["iot-hub", "stream-analytics", "sql-database", "power-bi"],
            "availableServices": ["iot-hub", "stream-analytics", "sql-database", "power-bi", "cosmos-db", "functions"],
            "explanation": "O IoT Hub ingere os dados dos veículos. Um job do Stream Analytics recebe esses dados e faz um `JOIN` com uma tabela de referência no SQL Database (que contém os dados de geolocalização). O resultado enriquecido é enviado para um dataset do Power BI para visualização em tempo real."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Configure o CI/CD para um projeto de banco de dados do Synapse Analytics, automatizando o deploy de alterações de esquema de um ambiente de desenvolvimento para produção.",
            "solution": ["synapse", "devops"],
            "availableServices": ["synapse", "devops", "data-factory", "databricks"],
            "explanation": "Integrando o workspace do Synapse com o Azure DevOps (Git), você pode gerenciar seus scripts SQL e outros artefatos. Uma Release Pipeline no Azure DevOps pode ser usada para implantar essas alterações de forma controlada entre os diferentes ambientes (Dev, QA, Prod)."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Crie uma 'feature store' para Machine Learning, onde features de clientes são calculadas com Databricks e armazenadas no Cosmos DB para acesso de baixa latência.",
            "solution": ["databricks", "cosmos-db"],
            "availableServices": ["databricks", "cosmos-db", "synapse", "machine-learning", "redis"],
            "explanation": "Um job do Databricks calcula as features (ex: total de compras nos últimos 30 dias) em lote. Os resultados são salvos no Cosmos DB, que serve como a camada de serviço online da feature store, permitindo que os modelos de ML busquem as features rapidamente durante a inferência."
        },
        {
            "category": "data-engineering",
            "difficulty": 4,
            "question": "Desenvolva uma solução de Machine Learning para prever a demanda de produtos, treinando um modelo no Azure ML com dados do Synapse e servindo-o através de uma função.",
            "solution": ["synapse", "machine-learning", "functions"],
            "availableServices": ["synapse", "machine-learning", "functions", "data-factory", "databricks", "api-management"],
            "explanation": "O Azure Machine Learning se conecta ao Synapse para obter os dados de treinamento. Após o treinamento, o modelo é implantado como um endpoint. Uma Azure Function atua como uma fachada para esse endpoint, adicionando lógica de negócio antes e depois de chamar o modelo para inferência."
        },
        {
            "category": "data-engineering",
            "difficulty": 4,
            "question": "Projete uma arquitetura Lambda que combina processamento em lote (com Databricks) e em tempo real (com Stream Analytics) para fornecer uma visão completa e atualizada dos dados.",
            "solution": ["event-hubs", "stream-analytics", "databricks", "synapse"],
            "availableServices": ["event-hubs", "stream-analytics", "databricks", "synapse", "data-factory", "cosmos-db"],
            "explanation": "O Event Hubs recebe os dados. A camada de velocidade (Stream Analytics) processa os dados em tempo real para insights imediatos. A camada de lote (Databricks) processa os mesmos dados de forma mais completa e precisa. Ambos os resultados são salvos no Synapse, onde podem ser consultados de forma unificada."
        },
        {
            "category": "data-engineering",
            "difficulty": 4,
            "question": "Implemente um pipeline de MLOps. O pipeline deve ser acionado por novos dados, treinar um modelo no Azure ML e registrá-lo, tudo orquestrado pelo Data Factory.",
            "solution": ["data-lake-storage", "data-factory", "machine-learning"],
            "availableServices": ["data-lake-storage", "data-factory", "machine-learning", "databricks", "synapse", "functions"],
            "explanation": "O Data Factory orquestra o fluxo de ponta a ponta. Um gatilho de evento no Data Lake inicia o pipeline. Atividades do Data Factory podem executar pré-processamento, acionar um pipeline de treinamento no Azure Machine Learning e, com base no resultado, registrar a nova versão do modelo."
        },
        {
            "category": "data-engineering",
            "difficulty": 4,
            "question": "Construa um Data Warehouse moderno onde o acesso a todos os dados (Data Lake, Synapse) é protegido por uma rede virtual e acessado apenas através de Private Endpoints.",
            "solution": ["data-lake-storage", "synapse", "vnet", "private-endpoint"],
            "availableServices": ["data-lake-storage", "synapse", "vnet", "private-endpoint", "data-factory", "firewall"],
            "explanation": "O Data Lake e o Synapse são configurados para desabilitar o acesso público. Private Endpoints são criados dentro de uma VNet, fornecendo um endereço IP privado para acessar esses serviços. Isso garante que todo o tráfego de dados permaneça na rede privada da Azure."
        },
        {
            "category": "data-engineering",
            "difficulty": 4,
            "question": "Crie uma solução de análise de sentimento em tempo real para tweets sobre sua marca, usando Event Hubs para ingestão, Functions com Cognitive Services para análise e Power BI para visualização.",
            "solution": ["event-hubs", "functions", "cognitive-services", "power-bi"],
            "availableServices": ["event-hubs", "functions", "cognitive-services", "power-bi", "stream-analytics", "databricks"],
            "explanation": "O Event Hubs ingere os tweets. Uma Azure Function é acionada por esses eventos, chama a API de Análise de Texto dos Cognitive Services para obter o sentimento e envia o resultado para um dataset de streaming do Power BI, que atualiza o dashboard em tempo real."
        },
        {
            "category": "data-engineering",
            "difficulty": 4,
            "question": "Desenvolva um pipeline de CI/CD para seu projeto do Azure Data Factory, promovendo pipelines entre ambientes de Dev, QA e Prod usando Azure DevOps e integração Git.",
            "solution": ["data-factory", "devops"],
            "availableServices": ["data-factory", "devops", "synapse", "databricks", "arm-template"],
            "explanation": "O Data Factory é integrado a um repositório Git no Azure DevOps. Os desenvolvedores trabalham em branches. Uma Release Pipeline no DevOps é usada para publicar as alterações do branch principal para a instância de produção do Data Factory, automatizando a implantação."
        },
        {
            "category": "data-engineering",
            "difficulty": 4,
            "question": "Use o Microsoft Purview para rastrear a linhagem de dados (data lineage) de ponta a ponta, desde a fonte de dados em um banco SQL, através de um pipeline do Data Factory, até o relatório final no Power BI.",
            "solution": ["sql-database", "data-factory", "power-bi", "purview"],
            "availableServices": ["sql-database", "data-factory", "power-bi", "purview", "synapse"],
            "explanation": "O Purview se conecta a todos esses serviços. Ele pode escanear as atividades do Data Factory e os relatórios do Power BI para entender as dependências e construir automaticamente um grafo de linhagem visual, mostrando como os dados fluem e são transformados."
        },
        {
            "category": "data-engineering",
            "difficulty": 5,
            "question": "Projete uma arquitetura de Big Data completa usando HDInsight para processamento Spark, Data Factory para orquestração e Purview para governança de dados em toda a plataforma.",
            "solution": ["data-lake-storage", "data-factory", "hdinsight", "purview", "synapse"],
            "availableServices": ["data-lake-storage", "data-factory", "hdinsight", "purview", "synapse", "databricks", "power-bi"],
            "explanation": "O Data Factory orquestra os pipelines que acionam jobs Spark no HDInsight sobre os dados no Data Lake. O Synapse serve como o data warehouse para os resultados. O Purview cataloga e governa todos os ativos de dados (Data Lake, Synapse) e pipelines (Data Factory)."
        },
        {
            "category": "data-engineering",
            "difficulty": 5,
            "question": "Crie uma plataforma de dados self-service e governada, onde analistas podem usar Synapse Serverless e Power BI para explorar dados, enquanto o Purview garante a aplicação de políticas de acesso.",
            "solution": ["data-lake-storage", "synapse", "purview", "power-bi"],
            "availableServices": ["data-lake-storage", "synapse", "purview", "power-bi", "databricks", "data-factory"],
            "explanation": "Os dados residem no Data Lake. O Purview cataloga e aplica políticas de governança (ex: quem pode ver dados PII). Os analistas usam o Synapse Serverless para consultas ad-hoc e o Power BI para dashboards, com as políticas do Purview sendo aplicadas para garantir o acesso seguro."
        },
        {
            "category": "data-engineering",
            "difficulty": 5,
            "question": "Implemente uma arquitetura de 'Data Mesh' no Azure, onde diferentes domínios de negócio possuem seus próprios pipelines de dados (produtos de dados) e os disponibilizam de forma centralmente governada.",
            "solution": ["data-factory", "synapse", "purview", "api-management"],
            "availableServices": ["data-factory", "synapse", "purview", "api-management", "databricks", "event-hubs"],
            "explanation": "Cada domínio usa Data Factory e Synapse para criar e gerenciar seus produtos de dados. O Purview atua como o catálogo central para descoberta e governança. O API Management pode ser usado para expor alguns desses produtos de dados como APIs seguras para consumo."
        },
        {
            "category": "data-engineering",
            "difficulty": 5,
            "question": "Construa um sistema de detecção de anomalias em transações financeiras em tempo real, usando Kafka em HDInsight para ingestão, Spark Streaming para processamento e Azure ML para detecção.",
            "solution": ["hdinsight-kafka", "hdinsight-spark", "machine-learning"],
            "availableServices": ["hdinsight-kafka", "hdinsight-spark", "machine-learning", "databricks", "event-hubs", "stream-analytics"],
            "explanation": "O Kafka no HDInsight ingere as transações. Um job do Spark Streaming no HDInsight processa os eventos em micro-lotes e chama um endpoint do Azure Machine Learning (que hospeda o modelo de detecção de anomalias) para pontuar cada transação em tempo real."
        },
        {
            "category": "data-engineering",
            "difficulty": 5,
            "question": "Projete uma solução de análise de genoma em larga escala, usando Azure Batch para processamento paralelo de arquivos genômicos e Databricks para análises agregadas.",
            "solution": ["blob-storage", "batch", "databricks"],
            "availableServices": ["blob-storage", "batch", "databricks", "hdinsight", "synapse"],
            "explanation": "O Azure Batch é ideal para processamento paralelo massivo. Ele pode executar softwares de análise genômica em milhares de VMs simultaneamente sobre os dados no Blob Storage. O Databricks é então usado para analisar os resultados agregados desses processamentos."
        },
        {
            "category": "data-engineering",
            "difficulty": 5,
            "question": "Crie uma arquitetura de FinOps para uma plataforma de dados, otimizando agressivamente os custos de clusters Databricks/Synapse com instâncias spot, auto-scaling e monitoramento de custos por projeto.",
            "solution": ["databricks", "synapse", "cost-management", "monitor"],
            "availableServices": ["databricks", "synapse", "cost-management", "monitor", "policy", "advisor"],
            "explanation": "A solução envolve configurar clusters Databricks e Synapse para usar VMs Spot e políticas de auto-scaling. O Cost Management, com tags de projeto, é usado para analisar e alocar os custos. O Azure Monitor é usado para criar alertas sobre métricas de performance que podem indicar ineficiências de custo."
        },
        {
            "category": "data-engineering",
            "difficulty": 5,
            "question": "Desenvolva uma arquitetura híbrida que processa dados sensíveis em um cluster Hadoop on-premises, mas usa o Data Factory para orquestrar e mover os resultados agregados e anonimizados para o Synapse na nuvem.",
            "solution": ["data-factory", "synapse", "vnet", "expressroute"],
            "availableServices": ["data-factory", "synapse", "vnet", "expressroute", "arc", "purview"],
            "explanation": "O Data Factory, usando um 'Self-hosted Integration Runtime' instalado on-premises, pode orquestrar jobs no cluster local. Após o processamento, ele pode mover os resultados seguros através de uma conexão privada (ExpressRoute) para o Synapse Analytics na nuvem para análise global."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Disponibilize um conjunto de dados do Blob Storage para análise interativa usando SQL em um pool serverless.",
            "solution": ["blob-storage", "synapse"],
            "availableServices": ["blob-storage", "synapse", "data-factory", "power-bi", "sql-database", "cosmos-db"],
            "explanation": "O pool SQL serverless do Azure Synapse Analytics pode consultar dados diretamente do Blob Storage (ou Data Lake) usando a sintaxe T-SQL, sem a necessidade de mover ou carregar os dados previamente."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Crie um relatório no Power BI que se conecta a uma planilha do Excel armazenada no OneDrive.",
            "solution": ["power-bi"],
            "availableServices": ["power-bi", "synapse", "data-factory", "blob-storage"],
            "explanation": "O Power BI possui um conector nativo para o OneDrive for Business, permitindo que você se conecte diretamente a arquivos Excel e os use como fonte de dados para criar relatórios e dashboards."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Execute uma consulta SQL para contar o número de linhas em um arquivo Parquet de 10 GB armazenado no Data Lake, sem movê-lo.",
            "solution": ["data-lake-storage", "synapse-serverless"],
            "availableServices": ["data-lake-storage", "synapse-serverless", "databricks", "sql-database", "vm"],
            "explanation": "O pool SQL serverless do Synapse é otimizado para ler formatos colunares como o Parquet. Ele pode ler os metadados do arquivo para obter a contagem de linhas de forma extremamente rápida e barata, sem precisar ler o arquivo inteiro."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Conecte o Power BI a um banco de dados SQL do Azure para criar um dashboard de vendas.",
            "solution": ["sql-database", "power-bi"],
            "availableServices": ["sql-database", "power-bi", "synapse", "cosmos-db", "data-factory"],
            "explanation": "O Power BI tem um conector nativo e otimizado para o Azure SQL Database. Ele permite importar os dados para o Power BI ou usar o 'DirectQuery' para consultar o banco de dados em tempo real."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Use o Power Query dentro do Power BI para remover colunas indesejadas e filtrar linhas de um conjunto de dados.",
            "solution": ["power-bi"],
            "availableServices": ["power-bi", "data-factory", "synapse", "functions"],
            "explanation": "O Power Query é o motor de transformação de dados integrado ao Power BI. Ele fornece uma interface de usuário gráfica para aplicar centenas de transformações, como remover colunas, filtrar, alterar tipos de dados, etc."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Crie um gráfico de pizza no Power BI para mostrar a distribuição de vendas por categoria de produto.",
            "solution": ["power-bi"],
            "availableServices": ["power-bi", "synapse", "excel", "databricks"],
            "explanation": "Dentro do Power BI Desktop, após carregar os dados, você pode selecionar o visual 'Gráfico de pizza', arrastar o campo 'Categoria' para a legenda e o campo 'Vendas' para os valores para gerar o gráfico."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Qual serviço você usaria para analisar logs de telemetria e séries temporais usando a linguagem de consulta KQL (Kusto Query Language)?",
            "solution": ["data-explorer"],
            "availableServices": ["data-explorer", "synapse", "log-analytics", "sql-database"],
            "explanation": "O Azure Data Explorer (codinome Kusto) é um serviço de análise de dados rápido e totalmente gerenciado, otimizado para análise de grandes volumes de dados de streaming, especialmente logs e telemetria, usando a linguagem KQL."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Visualize dados de uma lista do SharePoint em um relatório do Power BI.",
            "solution": ["power-bi"],
            "availableServices": ["power-bi", "data-factory", "synapse", "logic-apps"],
            "explanation": "O Power BI possui um conector nativo para 'Lista do SharePoint Online', permitindo que você se conecte diretamente à lista e use seus dados como fonte para criar relatórios e dashboards."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Crie uma arquitetura para analisar dados de streaming em tempo real e visualizar os insights em um dashboard dinâmico.",
            "solution": ["event-hubs", "stream-analytics", "power-bi"],
            "availableServices": ["event-hubs", "stream-analytics", "power-bi", "synapse", "functions", "blob-storage"],
            "explanation": "O Event Hubs ingere o fluxo de dados. O Stream Analytics executa uma consulta contínua sobre esses dados. O resultado da consulta é enviado como 'output' para um dataset de streaming no Power BI, que atualiza os visuais do dashboard em tempo real."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Combine dados de um banco de dados SQL do Azure e de um arquivo CSV do Blob Storage em um único modelo de dados no Power BI.",
            "solution": ["sql-database", "blob-storage", "power-bi"],
            "availableServices": ["sql-database", "blob-storage", "power-bi", "data-factory", "synapse"],
            "explanation": "No Power BI Desktop, você pode adicionar múltiplas fontes de dados. Ele permite importar dados do SQL Database e do Blob Storage e, em seguida, criar um relacionamento (join) entre as tabelas na visualização de modelo de dados."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Crie uma medida DAX (Data Analysis Expressions) no Power BI para calcular as vendas acumuladas no ano (Year-to-Date).",
            "solution": ["power-bi"],
            "availableServices": ["power-bi", "synapse", "sql", "python"],
            "explanation": "DAX é a linguagem de fórmula usada no Power BI. Você pode usar uma função de inteligência de tempo como `TOTALYTD` ou `DATESYTD` para criar uma nova medida que calcula a soma das vendas desde o início do ano até a data atual no contexto do filtro."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Analise o texto de comentários de clientes em um banco de dados SQL para extrair as frases-chave usando um serviço cognitivo.",
            "solution": ["sql-database", "functions", "cognitive-services"],
            "availableServices": ["sql-database", "functions", "cognitive-services", "power-bi", "synapse"],
            "explanation": "Uma Azure Function pode ser usada para ler os comentários do SQL Database. Para cada comentário, ela chama a API de Análise de Texto dos Cognitive Services para extrair as frases-chave e pode gravar os resultados de volta em uma nova tabela no banco de dados."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Configure um alerta no Power BI para notificar um gerente quando as vendas diárias caírem abaixo de uma meta específica.",
            "solution": ["power-bi"],
            "availableServices": ["power-bi", "logic-apps", "monitor", "stream-analytics"],
            "explanation": "No serviço do Power BI, você pode definir alertas em 'cards' e KPIs em um dashboard. Você define o limiar e a frequência de verificação. Quando o limiar é atingido, o Power BI envia uma notificação."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Crie um dashboard no Power BI que se atualiza em tempo real à medida que novos dados chegam a um banco de dados Cosmos DB.",
            "solution": ["cosmos-db", "power-bi"],
            "availableServices": ["cosmos-db", "power-bi", "stream-analytics", "synapse"],
            "explanation": "Usando o conector do Cosmos DB no Power BI em modo 'DirectQuery' e o recurso de 'Atualização automática de página' no Power BI Service, você pode criar um dashboard que consulta o Cosmos DB em intervalos frequentes, simulando uma visualização em tempo real."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Use o Microsoft Purview para escanear um Data Lake e identificar automaticamente ativos de dados que contêm informações de cartão de crédito.",
            "solution": ["data-lake-storage", "purview"],
            "availableServices": ["data-lake-storage", "purview", "synapse", "data-factory"],
            "explanation": "O Purview vem com classificadores de sistema integrados para dados sensíveis. Ao configurar um 'scan' em seu Data Lake, ele irá inspecionar o conteúdo dos arquivos e aplicar automaticamente a classificação 'Número do Cartão de Crédito' aos dados correspondentes."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Extraia dados de tabelas de documentos PDF armazenados no Blob Storage usando um serviço de IA.",
            "solution": ["blob-storage", "form-recognizer", "data-factory"],
            "availableServices": ["blob-storage", "form-recognizer", "cognitive-services", "functions", "synapse"],
            "explanation": "O Azure Form Recognizer (agora parte do Azure AI Document Intelligence) é um serviço de IA que pode ser treinado para entender o layout de seus documentos (como faturas) e extrair dados de tabelas e campos de forma estruturada. O Data Factory pode orquestrar esse processo."
        },
        {
            "category": "data-analysis",
            "difficulty": 3,
            "question": "Projete uma solução de BI de ponta a ponta: ingira dados de várias fontes, transforme-os, carregue em um data warehouse e crie relatórios interativos.",
            "solution": ["data-factory", "data-lake-storage", "synapse", "power-bi"],
            "availableServices": ["data-factory", "data-lake-storage", "synapse", "power-bi", "databricks", "stream-analytics"],
            "explanation": "O Data Factory ingere os dados para o Data Lake. Ele também orquestra a transformação e a carga dos dados para um pool SQL dedicado no Synapse Analytics (o data warehouse). O Power BI se conecta ao Synapse para criar os relatórios."
        },
        {
            "category": "data-analysis",
            "difficulty": 3,
            "question": "Crie um modelo de dados em estrela (star schema) no Synapse Analytics para otimizar consultas de BI em um grande volume de dados de vendas.",
            "solution": ["synapse", "power-bi"],
            "availableServices": ["synapse", "power-bi", "data-factory", "databricks", "data-lake-storage"],
            "explanation": "No Synapse, você cria uma tabela de fatos (ex: vendas) e tabelas de dimensão (ex: produtos, clientes, tempo). O Power BI se conecta a esse modelo, e suas consultas (geradas pelos visuais) se tornam muito mais eficientes do que seriam em uma tabela única e desnormalizada."
        },
        {
            "category": "data-analysis",
            "difficulty": 3,
            "question": "Use o recurso de AutoML (Machine Learning Automatizado) do Azure ML para criar um modelo de previsão de churn de clientes e visualize os resultados no Power BI.",
            "solution": ["synapse", "machine-learning", "power-bi"],
            "availableServices": ["synapse", "machine-learning", "power-bi", "databricks", "functions"],
            "explanation": "O AutoML no Azure Machine Learning pode usar dados do Synapse para treinar e selecionar o melhor modelo de classificação. O resultado (a previsão de churn para cada cliente) pode ser salvo de volta no Synapse e, em seguida, analisado no Power BI."
        },
        {
            "category": "data-analysis",
            "difficulty": 3,
            "question": "Analise dados de sensores de IoT em tempo real para detectar anomalias (ex: picos de temperatura) usando uma janela de tempo deslizante.",
            "solution": ["iot-hub", "stream-analytics"],
            "availableServices": ["iot-hub", "stream-analytics", "functions", "event-hubs", "synapse"],
            "explanation": "O IoT Hub recebe os dados dos sensores. O Azure Stream Analytics pode usar funções de janela de tempo (Tumbling, Hopping, Sliding) em sua consulta SQL para analisar os dados em um período e usar funções de detecção de anomalias para identificar outliers."
        },
        {
            "category": "data-analysis",
            "difficulty": 3,
            "question": "Conecte o Power BI a um workspace do Azure Databricks para analisar dados processados e armazenados em tabelas Delta Lake.",
            "solution": ["databricks", "power-bi"],
            "availableServices": ["databricks", "power-bi", "synapse", "data-factory"],
            "explanation": "O Power BI possui um conector nativo para o Azure Databricks. Ele permite que você se conecte a um cluster Databricks e consulte as tabelas Delta (que residem no Data Lake) como se fossem tabelas de um banco de dados tradicional."
        },
        {
            "category": "data-analysis",
            "difficulty": 3,
            "question": "Crie um relatório paginado (estilo PDF) no Power BI para gerar faturas mensais para clientes com base nos dados de uso.",
            "solution": ["synapse", "power-bi-report-builder"],
            "availableServices": ["synapse", "power-bi-report-builder", "data-factory", "logic-apps"],
            "explanation": "Relatórios paginados são projetados para serem impressos ou gerados como PDFs perfeitos em pixels. O Power BI Report Builder é a ferramenta para criar esses relatórios, que podem usar dados do Synapse. Eles podem ser publicados no serviço do Power BI e exportados."
        },
        {
            "category": "data-analysis",
            "difficulty": 3,
            "question": "Use o Microsoft Purview para visualizar a linhagem de dados de um campo específico, desde sua origem em um arquivo CSV até seu uso em um relatório do Power BI.",
            "solution": ["data-lake-storage", "data-factory", "purview", "power-bi"],
            "availableServices": ["data-lake-storage", "data-factory", "purview", "power-bi", "synapse"],
            "explanation": "Ao escanear o Data Lake, o Data Factory e o Power BI, o Purview constrói automaticamente um mapa de linhagem de dados. Você pode selecionar um campo em um relatório do Power BI e visualizar todo o caminho que ele percorreu, incluindo as transformações no Data Factory."
        },
        {
            "category": "data-analysis",
            "difficulty": 4,
            "question": "Analise o sentimento de avaliações de produtos enviadas via API, usando Cognitive Services para processar o texto e armazenando o resultado para análise no Synapse.",
            "solution": ["api-management", "functions", "cognitive-services", "synapse"],
            "availableServices": ["api-management", "functions", "cognitive-services", "synapse", "event-hubs", "cosmos-db"],
            "explanation": "O API Management recebe a avaliação. Ele aciona uma Azure Function que envia o texto para a API de Análise de Texto dos Cognitive Services. A função recebe o sentimento (positivo/negativo) e o armazena em uma tabela no Synapse para análise agregada."
        },
        {
            "category": "data-analysis",
            "difficulty": 4,
            "question": "Crie um sistema de busca inteligente sobre uma base de documentos não estruturados (PDFs, PPTs) usando IA para permitir buscas por linguagem natural.",
            "solution": ["blob-storage", "cognitive-search"],
            "availableServices": ["blob-storage", "cognitive-search", "synapse", "purview", "functions"],
            "explanation": "O Azure Cognitive Search pode indexar conteúdo do Blob Storage. Usando 'habilidades' (skills) de IA, ele pode enriquecer o índice com OCR, extração de entidades e, com a busca semântica, entender a intenção do usuário em vez de apenas corresponder palavras-chave."
        },
        {
            "category": "data-analysis",
            "difficulty": 4,
            "question": "Desenvolva uma solução de manutenção preditiva, analisando dados de telemetria de máquinas para prever falhas antes que ocorram, usando Azure ML.",
            "solution": ["iot-hub", "stream-analytics", "synapse", "machine-learning"],
            "availableServices": ["iot-hub", "stream-analytics", "synapse", "machine-learning", "databricks", "power-bi"],
            "explanation": "Dados de telemetria são coletados e armazenados no Synapse. O Azure Machine Learning usa esses dados históricos para treinar um modelo de classificação (ex: 'vai falhar nos próximos 7 dias'). O modelo pode então ser aplicado a novos dados para gerar previsões."
        },
        {
            "category": "data-analysis",
            "difficulty": 4,
            "question": "Implemente o CI/CD para seus relatórios e conjuntos de dados do Power BI, automatizando a implantação entre workspaces de desenvolvimento, teste e produção.",
            "solution": ["power-bi", "devops"],
            "availableServices": ["power-bi", "devops", "synapse", "data-factory", "github"],
            "explanation": "Usando as 'deployment pipelines' do Power BI ou a integração com Azure DevOps, você pode gerenciar o ciclo de vida do seu conteúdo de BI. Isso permite implantar alterações de um workspace de desenvolvimento para um de produção de forma controlada e automatizada."
        },
        {
            "category": "data-analysis",
            "difficulty": 4,
            "question": "Analise o conteúdo de imagens em um Data Lake para categorizá-las (ex: 'paisagem', 'retrato', 'produto') e enriquecer os metadados no Synapse.",
            "solution": ["data-lake-storage", "functions", "cognitive-services-vision", "synapse"],
            "availableServices": ["data-lake-storage", "functions", "cognitive-services-vision", "synapse", "databricks"],
            "explanation": "Uma Azure Function acionada por novas imagens no Data Lake chama a API do Azure AI Vision para obter rótulos e descrições da imagem. A função então grava esses metadados em uma tabela no Synapse, associando-os ao caminho do arquivo da imagem."
        },
        {
            "category": "data-analysis",
            "difficulty": 4,
            "question": "Construa uma Visão 360º do Cliente no Synapse, unificando dados de um CRM (Salesforce), de um sistema de pedidos (SQL DB) e de logs de navegação do site (Data Lake).",
            "solution": ["data-factory", "data-lake-storage", "sql-database", "synapse"],
            "availableServices": ["data-factory", "data-lake-storage", "sql-database", "synapse", "power-bi", "databricks"],
            "explanation": "O Data Factory usa seus conectores para ingerir dados de todas as fontes para o Data Lake. Em seguida, pipelines de transformação no Synapse (ou Data Factory) limpam, unem e modelam esses dados em um esquema unificado de cliente no data warehouse do Synapse."
        },
        {
            "category": "data-analysis",
            "difficulty": 5,
            "question": "Implemente uma solução de governança de dados para catalogar, classificar e controlar o acesso a ativos de dados em toda a organização, da ingestão ao consumo.",
            "solution": ["data-factory", "purview", "synapse", "power-bi"],
            "availableServices": ["data-factory", "purview", "synapse", "power-bi", "data-lake-storage", "sql-database", "key-vault"],
            "explanation": "O Purview escaneia e cataloga todos os ativos de dados e pipelines. Ele aplica classificações e políticas de acesso. Os usuários no Power BI ou Synapse podem descobrir dados através do Purview, e suas permissões de acesso são aplicadas em tempo de consulta."
        },
        {
            "category": "data-analysis",
            "difficulty": 5,
            "question": "Projete uma plataforma de BI self-service onde usuários de negócio podem descobrir, preparar e analisar dados de forma segura e governada, com políticas de acesso definidas centralmente.",
            "solution": ["purview", "synapse", "power-bi"],
            "availableServices": ["purview", "synapse", "power-bi", "data-factory", "databricks", "azure-ad"],
            "explanation": "O Purview serve como o portal de descoberta de dados. O Synapse fornece o poder de computação para consulta. O Power BI é a ferramenta de análise. As políticas de acesso são definidas no Purview e aplicadas no Synapse (ex: com RLS/CLS) com base nos grupos do Azure AD do usuário."
        },
        {
            "category": "data-analysis",
            "difficulty": 5,
            "question": "Crie uma solução de detecção de fraude em transações financeiras em tempo real, que processe milhões de eventos e use um modelo de Machine Learning para pontuar cada transação, exibindo alertas em um dashboard.",
            "solution": ["event-hubs", "stream-analytics", "machine-learning", "power-bi"],
            "availableServices": ["event-hubs", "stream-analytics", "machine-learning", "power-bi", "synapse", "databricks", "functions"],
            "explanation": "O Event Hubs ingere as transações. O Stream Analytics chama uma função de 'User-Defined Function' (UDF) que se conecta a um endpoint do Azure Machine Learning para pontuar a transação. Os resultados (especialmente as transações de alto risco) são enviados para um dataset do Power BI para um dashboard de monitoramento."
        },
        {
            "category": "data-analysis",
            "difficulty": 5,
            "question": "Otimize um modelo de dados do Power BI com mais de 1 bilhão de linhas para performance interativa, usando uma combinação de DirectQuery, modo de importação e agregações.",
            "solution": ["synapse", "power-bi"],
            "availableServices": ["synapse", "power-bi", "analysis-services", "data-factory"],
            "explanation": "A solução usa um modelo 'composto' no Power BI. As tabelas de dimensão são importadas para velocidade. A tabela de fatos gigante é acessada via DirectQuery para o Synapse. Agregações pré-calculadas são criadas no Power BI (ou no Synapse como views materializadas) para que os visuais de alto nível atinjam o cache de agregação em vez da tabela de fatos."
        },
        {
            "category": "data-analysis",
            "difficulty": 5,
            "question": "Implemente segurança em nível de linha (Row-Level Security) em uma solução de análise, garantindo que os vendedores só possam ver os dados de seus próprios clientes tanto no Synapse quanto nos relatórios do Power BI.",
            "solution": ["synapse", "power-bi", "azure-ad"],
            "availableServices": ["synapse", "power-bi", "azure-ad", "purview", "data-factory"],
            "explanation": "A Segurança em Nível de Linha (RLS) é implementada no Synapse, criando uma política de segurança que filtra os dados com base no `USER_NAME()` (que corresponde ao usuário do Azure AD). O Power BI, ao usar o DirectQuery com SSO, passa a identidade do usuário para o Synapse, que aplica a RLS automaticamente."
        },
        {
            "category": "data-analysis",
            "difficulty": 5,
            "question": "Desenvolva uma arquitetura de 'Data Mesh' para análise, onde diferentes domínios (Vendas, Marketing, RH) publicam seus próprios 'produtos de dados' de forma independente, mas que podem ser descobertos e consumidos de forma centralizada.",
            "solution": ["synapse", "purview", "power-bi", "api-management"],
            "availableServices": ["synapse", "purview", "power-bi", "api-management", "data-factory", "databricks"],
            "explanation": "Cada domínio gerencia seus produtos de dados no Synapse. O Purview atua como o catálogo federado para descoberta e governança. O Power BI se conecta a esses diferentes produtos de dados para análise. O API Management pode expor dados agregados como APIs para consumo por outras aplicações."
        },
            {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Um alerta de segurança do Microsoft Defender for Cloud é gerado. Isole a VM afetada, notifique a equipe de segurança via Teams e, em paralelo, inicie um runbook de automação para coletar evidências forenses.",
        "solution": ["defender-for-cloud", "event-grid", [["nsg", "lambda"], ["logic-apps"], ["automation-account"]]],
        "availableServices": ["defender-for-cloud", "event-grid", "nsg", "lambda", "logic-apps", "automation-account"],
        "explanation": "O alerta do Defender gera um evento no Event Grid, que dispara três ações paralelas: uma Lambda que aplica um NSG restritivo à VM, um Logic App que envia uma mensagem para o Teams, e um Automation Runbook para coletar logs e snapshots."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Quando um novo repositório é criado no Azure DevOps, aplique uma política de branch para proteger o 'main' e, em paralelo, crie um pipeline de build padrão e adicione o repositório a um scan de segurança do Defender.",
        "solution": ["devops", "event-grid", [["devops", "functions"], ["devops", "functions"], ["defender-for-cloud", "functions"]]],
        "availableServices": ["devops", "event-grid", "functions", "defender-for-cloud", "policy"],
        "explanation": "O Azure DevOps emite um evento 'RepoCreated' para o Event Grid, acionando três Azure Functions paralelas. Cada uma usa a API do DevOps ou do Defender para aplicar a política de branch, criar o pipeline e registrar o repositório para scans, respectivamente."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Um novo App Service é provisionado. Em paralelo, configure um domínio personalizado, habilite o backup diário e integre-o com o Application Insights para monitoramento.",
        "solution": ["app-service", "event-grid", [["app-service", "functions"], ["backup", "functions"], ["application-insights", "functions"]]],
        "availableServices": ["app-service", "event-grid", "functions", "backup", "application-insights"],
        "explanation": "A criação do App Service é capturada pelo Event Grid, que executa três Functions em paralelo. Cada uma é responsável por uma tarefa de configuração: uma para o domínio, uma para a política de backup e outra para a integração APM."
    },
    {
        "category": "cloud-operations",
        "difficulty": 5,
        "question": "Ao detectar um ataque de DDoS (via alerta do Azure Monitor), redirecione o tráfego para um site estático de 'página de manutenção' (via Front Door) e, em paralelo, notifique a equipe de infraestrutura e aumente a escala dos recursos de back-end preventivamente.",
        "solution": ["monitor", "event-grid", [["front-door", "functions"], ["logic-apps"], ["app-service", "functions"]]],
        "availableServices": ["monitor", "event-grid", "front-door", "functions", "logic-apps", "app-service"],
        "explanation": "O alerta de DDoS do Monitor dispara um evento. Em paralelo, uma Function altera a prioridade do grupo de backend no Front Door para a página de manutenção, um Logic App notifica a equipe, e outra Function aumenta o número de instâncias do App Service Plan."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Quando uma VM é desligada, crie um snapshot do seu disco de SO e, em paralelo, remova seu registro de um balanceador de carga e envie um log de auditoria.",
        "solution": ["vm", "event-grid", [["backup", "functions"], ["load-balancer", "functions"], ["log-analytics", "functions"]]],
        "availableServices": ["vm", "event-grid", "backup", "functions", "load-balancer", "log-analytics"],
        "explanation": "O evento de 'VM Desligada' no Event Grid dispara três fluxos paralelos via Azure Functions: um para criar o snapshot, outro para remover a VM do pool do Load Balancer e um terceiro para enviar um log customizado para o Log Analytics."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Uma nova VNet é criada. Em paralelo, conecte-a (peer) a uma VNet 'hub', aplique um conjunto padrão de NSGs e associe uma tabela de rotas que força o tráfego através de um firewall central.",
        "solution": ["vnet", "event-grid", [["vnet", "functions"], ["nsg", "functions"], ["vnet", "functions"]]],
        "availableServices": ["vnet", "event-grid", "nsg", "functions", "firewall", "policy"],
        "explanation": "O Event Grid captura o evento de criação da VNet e invoca três Functions paralelas, cada uma responsável por uma tarefa de governança de rede: criar o peering, aplicar o NSG padrão e associar a tabela de rotas (UDR)."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Um novo usuário é adicionado a um grupo do Azure AD. Conceda a ele acesso a um aplicativo SaaS (via provisionamento SCIM) e, em paralelo, adicione-o a um canal do Teams e envie um e-mail de boas-vindas.",
        "solution": ["azure-ad", "logic-apps", [["azure-ad"], ["logic-apps"], ["logic-apps"]]],
        "availableServices": ["azure-ad", "logic-apps", "functions", "graph-api"],
        "explanation": "Um Logic App é acionado pelo evento de 'usuário adicionado ao grupo' no Azure AD. Ele então executa três ações paralelas: o próprio Azure AD lida com o provisionamento SCIM, enquanto o Logic App usa conectores para adicionar o usuário ao Teams e enviar o e-mail."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Um pipeline de release no Azure DevOps falha no estágio de QA. Em paralelo, reverta a infraestrutura para o estado anterior (usando uma implantação Bicep anterior), crie um bug no Azure Boards e notifique o time de desenvolvimento no Slack.",
        "solution": ["devops", [["bicep"], ["devops"], ["logic-apps"]]],
        "availableServices": ["devops", "bicep", "logic-apps", "functions", "arm-template"],
        "explanation": "Na definição do pipeline de release, a falha do estágio de QA pode acionar três tarefas paralelas na fase de 'pós-implantação': uma que executa um job para implantar um template Bicep estável, uma que usa a API do DevOps para criar um item de trabalho (bug), e uma que chama um webhook (via Logic App) para notificar o Slack."
    },
    {
        "category": "cloud-operations",
        "difficulty": 5,
        "question": "Um servidor gerenciado pelo Azure Arc on-premises fica offline. Em paralelo, crie um alerta de alta prioridade no Azure Monitor, inicie um failover para um Site Recovery na nuvem e remova o servidor de um grupo de back-end do Azure Front Door.",
        "solution": ["arc", "monitor", [["logic-apps"], ["site-recovery"], ["front-door", "functions"]]],
        "availableServices": ["arc", "monitor", "logic-apps", "site-recovery", "front-door", "functions"],
        "explanation": "O Azure Arc envia uma métrica de 'heartbeat' para o Azure Monitor. Uma regra de alerta para a ausência de heartbeat dispara um grupo de ação que, em paralelo, chama um Logic App para incidentes, aciona o plano de failover do Site Recovery e invoca uma Function para atualizar a configuração do Front Door."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Ao provisionar um novo banco de dados SQL, em paralelo, habilite o Microsoft Defender for SQL, configure o backup de longo prazo (LTR) e adicione um registro do banco em um CMDB (via API).",
        "solution": ["sql-database", "event-grid", [["defender-for-cloud", "functions"], ["backup", "functions"], ["api-management", "functions"]]],
        "availableServices": ["sql-database", "event-grid", "defender-for-cloud", "backup", "functions", "api-management"],
        "explanation": "A criação do SQL Database é capturada pelo Event Grid, que dispara três Azure Functions simultâneas para configurar aspectos diferentes do novo banco: segurança (Defender), conformidade (Backup LTR) e gerenciamento (CMDB)."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Um Key Vault é atualizado com uma nova versão de um segredo. Em paralelo, reinicie um App Service para que ele pegue o novo segredo e reinicie um cluster AKS para a mesma finalidade.",
        "solution": ["key-vault", "event-grid", [["app-service", "functions"], ["aks", "functions"]]],
        "availableServices": ["key-vault", "event-grid", "app-service", "aks", "functions", "automation-account"],
        "explanation": "O Key Vault emite um evento 'SecretNewVersionCreated' para o Event Grid. Duas assinaturas de evento acionam duas Functions em paralelo. Uma usa a API do Azure para reiniciar o App Service, enquanto a outra usa `kubectl rollout restart` para reiniciar os pods no AKS."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Quando um arquivo .BACPAC é carregado em um Blob Storage, inicie a importação para um novo SQL Database e, em paralelo, copie o arquivo para um container de arquivamento e envie uma notificação de início de processo.",
        "solution": ["blob-storage", "logic-apps", [["sql-database"], ["blob-storage"], ["logic-apps"]]],
        "availableServices": ["blob-storage", "logic-apps", "sql-database", "data-factory", "functions"],
        "explanation": "Um Logic App é acionado pelo upload do blob. Ele então executa três ações em paralelo: uma que usa o conector do SQL Server para iniciar a importação, uma que copia o blob para outro container e uma que envia um e-mail de notificação."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Um usuário solicita acesso a um recurso via API. O pedido é enviado para dois aprovadores em paralelo. O fluxo só continua se ambos aprovarem. Se qualquer um rejeitar, o fluxo termina e notifica o usuário.",
        "solution": ["api-management", "step-functions-logic-apps", [["logic-apps"], ["logic-apps"]]],
        "availableServices": ["api-management", "step-functions-logic-apps", "functions", "service-bus"],
        "explanation": "Neste caso, o Azure Logic Apps (que serve como orquestrador de estado) é ideal. A API inicia o Logic App, que entra em um estado 'parallel'. Duas ações de 'esperar por aprovação' são enviadas. O Logic App aguarda a conclusão de ambas, avalia os resultados e então segue para o caminho de sucesso ou falha."
    },
    {
        "category": "cloud-operations",
        "difficulty": 5,
        "question": "Para uma implantação Canário em um cluster AKS, o Azure DevOps direciona 5% do tráfego para a nova versão. Em paralelo, inicia uma consulta no Log Analytics para monitorar erros 500 na nova versão e inicia um teste de carga sintético.",
        "solution": ["devops", "aks", [["log-analytics", "monitor"], ["devops-load-testing"]]],
        "availableServices": ["devops", "aks", "log-analytics", "monitor", "app-gateway", "service-mesh"],
        "explanation": "O pipeline do Azure DevOps usa uma estratégia de implantação 'canary' no AKS. Em uma etapa de 'monitoramento' paralela, ele executa duas tarefas: um script que consulta continuamente o Log Analytics (via API ou CLI) e aciona um alarme do Monitor em caso de anomalias, e uma tarefa do Azure Load Testing para gerar tráfego."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Ao final de cada mês, um Automation Runbook é acionado. Em paralelo, ele gera um relatório de custos do Cost Management, desliga VMs de desenvolvimento e arquiva logs antigos do Blob Storage.",
        "solution": ["automation-account", [["cost-management", "functions"], ["vm"], ["blob-storage"]]],
        "availableServices": ["automation-account", "cost-management", "functions", "vm", "blob-storage"],
        "explanation": "O Automation Account é acionado por um agendamento. O runbook principal pode invocar outros três runbooks em paralelo (ou usar `Start-Job` do PowerShell): um para chamar a API de custos, um para iterar e desligar VMs com uma tag específica, e um para gerenciar o ciclo de vida dos blobs."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Um novo registro de DNS é adicionado a uma zona pública no Azure DNS. Em paralelo, adicione um registro correspondente em uma zona privada para 'split-brain DNS' e acione um monitor de disponibilidade para testar o novo endpoint.",
        "solution": ["azure-dns", "event-grid", [["azure-dns", "functions"], ["monitor", "functions"]]],
        "availableServices": ["azure-dns", "event-grid", "functions", "monitor", "app-gateway"],
        "explanation": "A criação de um registro DNS gera um evento no Event Grid. Duas Functions são acionadas em paralelo: uma cria o registro correspondente na zona de DNS privada, e a outra usa a API do Azure Monitor para criar um teste de disponibilidade para o novo FQDN."
    },
    {
        "category": "cloud-operations",
        "difficulty": 5,
        "question": "Um Sentinel dispara um incidente de segurança. Automatize a resposta (SOAR) executando três ações paralelas: uma para enriquecer o incidente com dados de inteligência de ameaças, uma para bloquear o IP no Firewall do Azure, e uma para criar um snapshot forense da VM envolvida.",
        "solution": ["sentinel", [["logic-apps"], ["firewall", "logic-apps"], ["backup", "logic-apps"]]],
        "availableServices": ["sentinel", "logic-apps", "firewall", "backup", "defender-for-cloud"],
        "explanation": "Um 'playbook' do Sentinel (que é um Logic App) é acionado pelo incidente. O Logic App executa três ramificações paralelas: uma que consulta APIs de threat intelligence, uma que adiciona o IP malicioso a uma regra de negação do Firewall, e uma que usa o conector do Azure Backup para criar um snapshot da VM."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um fluxo de dados chega ao Event Hubs. Em paralelo, armazene os dados brutos no Data Lake para conformidade e processe-os com o Stream Analytics para painéis em tempo real.",
        "solution": ["event-hubs", [["data-lake-storage"], ["stream-analytics", "power-bi"]]],
        "availableServices": ["event-hubs", "data-lake-storage", "stream-analytics", "power-bi", "functions"],
        "explanation": "O Event Hubs pode ter múltiplos 'grupos de consumidores'. Um grupo de consumidores é usado pelo recurso 'Capture' do próprio Event Hub para arquivar os dados brutos no Data Lake. Um segundo grupo de consumidores, independente, é usado pelo job do Stream Analytics para processar os mesmos eventos em tempo real."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um pipeline do Data Factory conclui com sucesso. Em paralelo, atualize uma tabela de metadados no SQL Database e envie uma notificação para o Teams.",
        "solution": ["data-factory", [["sql-database"], ["logic-apps"]]],
        "availableServices": ["data-factory", "sql-database", "logic-apps", "functions", "monitor"],
        "explanation": "No final do pipeline do Data Factory, você pode adicionar duas atividades que não dependem uma da outra. Uma atividade de 'Stored Procedure' para atualizar o SQL Database e uma atividade de 'Webhook' que chama um Logic App para enviar a mensagem para o Teams."
    },
    {
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Ingira dados de CDC de um banco de dados com DMS. Em paralelo, encaminhe os dados para um data lakehouse (Databricks + Delta Lake) e para um data warehouse tradicional (Synapse Dedicated Pool) para atender a diferentes equipes de análise.",
        "solution": ["database-migration-service", "event-hubs", [["databricks", "data-lake-storage"], ["stream-analytics", "synapse"]]],
        "availableServices": ["database-migration-service", "event-hubs", "databricks", "stream-analytics", "synapse"],
        "explanation": "O DMS envia os eventos de alteração para o Event Hubs, que atua como um distribuidor. Em paralelo, um job do Databricks consome do Event Hubs e aplica as alterações (MERGE) na tabela Delta, enquanto um job do Stream Analytics consome os mesmos eventos e os carrega no Synapse."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um novo arquivo Parquet é adicionado ao Data Lake. Em paralelo, inicie um scan do Purview para catalogar e classificar os dados e inicie um pipeline do Azure Machine Learning para usar os novos dados para inferência em lote.",
        "solution": ["data-lake-storage", "event-grid", [["purview"], ["machine-learning"]]],
        "availableServices": ["data-lake-storage", "event-grid", "purview", "machine-learning", "data-factory"],
        "explanation": "O Event Grid captura o evento de criação do arquivo e aciona duas ações paralelas: um webhook que chama a API do Purview para iniciar um scan e outro que aciona a execução de um pipeline publicado do Azure Machine Learning."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um evento de negócio é publicado em um tópico do Service Bus. Crie duas assinaturas para este tópico: uma que envia os dados para uma Azure Function para processamento urgente e outra que os envia para uma fila para processamento em lote.",
        "solution": ["service-bus", [["functions"], ["service-bus-queue"]]],
        "availableServices": ["service-bus", "functions", "service-bus-queue", "logic-apps", "event-grid"],
        "explanation": "Os tópicos do Service Bus suportam múltiplas 'assinaturas', que recebem uma cópia de cada mensagem. Uma assinatura pode ter uma Function como seu processador direto, enquanto a outra assinatura pode simplesmente ser uma fila onde os dados se acumulam para serem processados mais tarde."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Ao receber um vídeo, use o Azure AI Video Indexer para extrair insights (transcrição, pessoas, tópicos). Em paralelo, use o Data Factory para copiar o vídeo para uma conta de armazenamento de arquivamento.",
        "solution": ["blob-storage", "event-grid", [["video-indexer"], ["data-factory"]]],
        "availableServices": ["blob-storage", "event-grid", "video-indexer", "data-factory", "functions"],
        "explanation": "O upload do vídeo no Blob Storage dispara um evento no Event Grid. Duas assinaturas paralelas são acionadas: uma invoca uma Azure Function que envia o vídeo para a API do Video Indexer, e a outra aciona um pipeline do Data Factory que simplesmente copia o arquivo para a conta de arquivamento."
    },
    {
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete uma arquitetura de 'write-behind caching'. Uma API grava dados em um Cache Redis para resposta rápida e, em paralelo, coloca a mesma operação em uma fila do Service Bus para que uma Function a grave de forma assíncrona no banco de dados SQL principal.",
        "solution": ["api-management", "functions", [["redis"], ["service-bus", "functions", "sql-database"]]],
        "availableServices": ["api-management", "functions", "redis", "service-bus", "sql-database", "cosmos-db"],
        "explanation": "A API aciona uma Function. A Function executa duas tarefas assíncronas: grava no Cache Redis e envia uma mensagem para a fila do Service Bus. Ela retorna o sucesso imediatamente após a escrita no cache. Uma segunda Function, acionada pela fila, garante que os dados sejam persistidos de forma durável no SQL Database."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um novo conjunto de dados de clientes é carregado. Em paralelo, execute um Data Flow no Data Factory para limpar os dados e execute um notebook do Databricks para gerar estatísticas descritivas.",
        "solution": ["data-lake-storage", "data-factory", [["data-factory-data-flow"], ["databricks"]]],
        "availableServices": ["data-lake-storage", "data-factory", "databricks", "synapse", "functions"],
        "explanation": "Dentro de um único pipeline do Data Factory, você pode ter uma atividade de 'Execute Data Flow' e uma atividade de 'Execute Databricks Notebook' que não dependem uma da outra e são iniciadas a partir da mesma atividade inicial, permitindo sua execução paralela."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Ingira dados do Salesforce usando o Data Factory. Em paralelo, armazene uma cópia bruta no Data Lake e carregue os dados em uma tabela do Synapse Analytics para relatórios.",
        "solution": ["data-factory", "salesforce", [["data-lake-storage"], ["synapse"]]],
        "availableServices": ["data-factory", "salesforce", "data-lake-storage", "synapse", "purview"],
        "explanation": "Dentro de uma atividade de cópia do Data Factory, você pode usar o conector do Salesforce como fonte. Em seguida, você pode ter duas atividades de cópia paralelas que usam a mesma fonte: uma que tem o Data Lake como destino e outra que tem o Synapse Analytics como destino."
    },
    {
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Um modelo do Azure ML é treinado e registrado. Em paralelo, implante o modelo em um endpoint online para inferência em tempo real e em um endpoint em lote para processamento de grandes volumes, e notifique a equipe de MLOps.",
        "solution": ["machine-learning", "devops", [["machine-learning-online-endpoint"], ["machine-learning-batch-endpoint"], ["logic-apps"]]],
        "availableServices": ["machine-learning", "devops", "logic-apps", "aks", "container-apps"],
        "explanation": "Um pipeline do Azure DevOps é acionado pelo registro do novo modelo no workspace do Azure ML. Ele então executa três tarefas em paralelo: uma para implantar o modelo em um Managed Online Endpoint, uma para criar um Batch Endpoint, e uma para chamar um Logic App que notifica a equipe."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um novo evento é publicado no Event Grid. Encaminhe este evento para um Logic App para integração com sistemas legados e, em paralelo, para o Event Hubs para arquivamento e análise futura.",
        "solution": ["event-grid", [["logic-apps"], ["event-hubs"]]],
        "availableServices": ["event-grid", "logic-apps", "event-hubs", "functions", "service-bus"],
        "explanation": "O Event Grid suporta múltiplas 'assinaturas de evento' para um mesmo tópico. Você cria uma assinatura que tem o Logic App como endpoint e, em paralelo, cria outra assinatura que tem o Event Hubs como endpoint. Ambas recebem o evento de forma independente."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um pipeline precisa consultar dados de um pool SQL Serverless do Synapse e, em paralelo, de um cluster do Azure Data Explorer (Kusto). Combine os resultados e salve no Data Lake.",
        "solution": ["data-factory", [["synapse-serverless"], ["data-explorer"]], "data-lake-storage"],
        "availableServices": ["data-factory", "synapse-serverless", "data-explorer", "databricks", "data-lake-storage"],
        "explanation": "No Data Factory, você pode ter duas atividades 'Lookup' ou 'Copy' em paralelo, uma consultando o Synapse e a outra o Data Explorer. As saídas de ambas as atividades podem então ser passadas para uma atividade subsequente (como um Data Flow ou uma Function) para a combinação dos resultados."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Ao receber um arquivo de áudio, transcreva-o com o Azure AI Speech e, em paralelo, identifique o idioma falado. Armazene a transcrição e o idioma detectado.",
        "solution": ["blob-storage", "functions", [["cognitive-services-speech-transcription"], ["cognitive-services-speech-language-id"]]],
        "availableServices": ["blob-storage", "functions", "cognitive-services-speech-transcription", "cognitive-services-speech-language-id", "logic-apps"],
        "explanation": "Uma Azure Function é acionada pelo upload do áudio. Ela faz duas chamadas paralelas à API do Azure AI Speech: uma para o serviço de transcrição e outra para o serviço de identificação de idioma. Quando ambas as respostas chegam, a função as consolida e armazena."
    },
    {
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Desenvolva uma arquitetura de 'change data capture' que, ao detectar uma alteração em uma tabela do Cosmos DB, a replique para o Synapse Analytics (via Synapse Link) e, em paralelo, envie um evento para o Service Bus para que outros microsserviços possam reagir à alteração.",
        "solution": ["cosmos-db", [["synapse-link"], ["functions", "service-bus"]]],
        "availableServices": ["cosmos-db", "synapse-link", "functions", "service-bus", "event-hubs"],
        "explanation": "O Cosmos DB tem dois recursos que funcionam em paralelo. O Azure Synapse Link replica os dados continuamente para um pool analítico do Synapse. Em paralelo, o 'Change Feed' do Cosmos DB pode ser usado para acionar uma Azure Function que, por sua vez, publica a alteração em um tópico do Service Bus."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um novo pipeline é publicado no Data Factory. Em paralelo, acione um scan do Purview para mapear a linhagem de dados do novo pipeline e execute um script para atualizar a documentação em um wiki.",
        "solution": ["data-factory", "event-grid", [["purview", "functions"], ["devops", "functions"]]],
        "availableServices": ["data-factory", "event-grid", "purview", "functions", "devops"],
        "explanation": "A publicação no Data Factory pode gerar um evento. O Event Grid captura este evento e dispara duas Functions paralelas: uma que usa a API do Purview para iniciar um scan focado no novo pipeline, e outra que usa a API do Azure DevOps (ou similar) para atualizar uma página wiki."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Orquestre um pipeline no Databricks que executa dois notebooks em paralelo (um para processar dados de clientes, outro para produtos). Se ambos forem bem-sucedidos, execute um terceiro notebook para juntar os resultados.",
        "solution": ["databricks", [["databricks-notebook1"], ["databricks-notebook2"]], "databricks-notebook3"],
        "availableServices": ["databricks", "data-factory", "logic-apps"],
        "explanation": "Isso pode ser feito com os 'Workflows' do Databricks. Você pode criar um DAG onde a tarefa do notebook 3 tem como dependência a conclusão bem-sucedida das tarefas dos notebooks 1 e 2, que por sua vez não têm dependências entre si e podem ser executadas em paralelo pelo scheduler."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Um novo relatório é publicado no Power BI. Em paralelo, envie uma notificação para um canal do Teams com o link do relatório e acione uma automação para exportar o relatório como PDF e salvá-lo no SharePoint.",
        "solution": ["power-bi", "logic-apps", [["logic-apps-teams"], ["logic-apps-pdf"]]],
        "availableServices": ["power-bi", "logic-apps", "functions", "sharepoint"],
        "explanation": "Um Logic App pode ser acionado por um alerta do Power BI ou por um agendamento. Ele executa duas ramificações paralelas: uma usa o conector do Teams para postar uma mensagem e a outra usa a ação 'Export To File for Power BI Reports' e em seguida salva o arquivo no SharePoint."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Ao receber um e-mail com um anexo, extraia o texto do corpo do e-mail para análise de sentimento e, em paralelo, salve o anexo no Blob Storage para arquivamento.",
        "solution": ["logic-apps", [["cognitive-services", "sql-database"], ["blob-storage"]]],
        "availableServices": ["logic-apps", "cognitive-services", "sql-database", "blob-storage", "functions"],
        "explanation": "Um Logic App é acionado pela chegada de um novo e-mail. Dentro do fluxo, ele executa duas ações em paralelo. A primeira envia o corpo do e-mail para a API de Análise de Texto e salva o resultado no SQL. A segunda ação salva o anexo do e-mail diretamente no Blob Storage."
    },
    {
        "category": "data-analysis",
        "difficulty": 5,
        "question": "Uma consulta do Synapse Analytics identifica um grupo de clientes com alto risco de churn. Em paralelo, adicione esses clientes a uma campanha de retenção no Dynamics 365 e crie um segmento de audiência no Azure Communication Services para uma campanha de SMS.",
        "solution": ["synapse", "logic-apps", [["dynamics-365"], ["communication-services"]]],
        "availableServices": ["synapse", "logic-apps", "dynamics-365", "communication-services", "data-factory"],
        "explanation": "Um pipeline do Synapse ou Data Factory executa a consulta. Ao final, ele aciona um Logic App passando a lista de clientes. O Logic App, por sua vez, executa duas ações paralelas usando seus conectores: uma para adicionar os clientes à campanha no Dynamics e outra para criar a audiência no Communication Services."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Para uma análise de 'market basket', um pipeline do Synapse processa transações. Em paralelo, ele gera uma lista de produtos frequentemente comprados juntos e um modelo de propensão do Azure ML para prever a próxima compra.",
        "solution": ["synapse", [["synapse-spark"], ["machine-learning"]]],
        "availableServices": ["synapse", "machine-learning", "databricks", "power-bi"],
        "explanation": "Dentro de um pipeline do Synapse, você pode ter duas atividades paralelas. Uma atividade 'Synapse Notebook' executa um código Spark para a análise de cesta de compras. Em paralelo, outra atividade 'Machine Learning' aciona um pipeline do Azure ML para treinar o modelo de propensão usando os mesmos dados de entrada."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Ao receber uma imagem de um formulário manuscrito, use o Azure AI Document Intelligence para extrair os dados e, em paralelo, use o Azure AI Vision para arquivar uma miniatura da imagem.",
        "solution": ["blob-storage", "functions", [["form-recognizer"], ["cognitive-services-vision"]]],
        "availableServices": ["blob-storage", "functions", "form-recognizer", "cognitive-services-vision", "logic-apps"],
        "explanation": "Uma Azure Function é acionada pelo upload do formulário. Ela executa duas chamadas de API paralelas e assíncronas: uma para o Document Intelligence para a extração de dados estruturados e outra para o Vision para gerar a miniatura. Os resultados são então processados."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Um job do Stream Analytics detecta uma anomalia em tempo real. Em paralelo, envie um alerta para o PagerDuty e salve o evento anômalo no Data Lake para análise forense posterior.",
        "solution": ["stream-analytics", [["logic-apps-pagerduty"], ["data-lake-storage"]]],
        "availableServices": ["stream-analytics", "logic-apps-pagerduty", "data-lake-storage", "functions", "event-hubs"],
        "explanation": "O job do Stream Analytics é configurado com dois 'outputs' para os dados anômalos. Um output é um Logic App (via webhook) que tem um conector para o PagerDuty. O outro output, que recebe os mesmos dados em paralelo, é uma conta do Data Lake Storage."
    },
    {
        "category": "data-analysis",
        "difficulty": 5,
        "question": "Crie um sistema de análise de vídeo. Para um vídeo de entrada, em paralelo, use o Video Indexer para identificar pessoas e o Azure AI Vision for Video para detectar objetos. Junte os resultados para criar um índice de metadados completo.",
        "solution": ["blob-storage", "logic-apps", [["video-indexer"], ["cognitive-services-vision-video"]], "cosmos-db"],
        "availableServices": ["blob-storage", "logic-apps", "video-indexer", "cognitive-services-vision-video", "cosmos-db"],
        "explanation": "Um Logic App é acionado pelo upload do vídeo. Ele inicia duas ações de longa duração em paralelo: uma que envia o vídeo para o Video Indexer e outra para a API do Vision. O Logic App aguarda a conclusão de ambas e, em seguida, uma ação final consolida os metadados de pessoas e objetos no Cosmos DB."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Um usuário de negócio faz uma pergunta em linguagem natural no Power BI (usando Q&A). Em paralelo, o Power BI gera a visualização e registra a pergunta em um log do Log Analytics para análise de popularidade das perguntas.",
        "solution": ["power-bi", [["power-bi-visual"], ["log-analytics"]]],
        "availableServices": ["power-bi", "log-analytics", "synapse", "monitor"],
        "explanation": "O próprio Power BI lida com a geração do visual. Em paralelo, os logs de auditoria do Power BI, que podem ser configurados para serem enviados para o Log Analytics, capturam a atividade de Q&A. Isso permite que os administradores analisem quais perguntas estão sendo feitas, de forma desacoplada da interação do usuário."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Um modelo do Azure ML é retreinado. Em paralelo, execute uma avaliação do modelo em um conjunto de dados de teste e use o 'Responsible AI dashboard' para verificar por vieses (bias). O modelo só é registrado se ambos os passos forem aprovados.",
        "solution": ["machine-learning", "devops", [["machine-learning-evaluation"], ["machine-learning-responsible-ai"]]],
        "availableServices": ["machine-learning", "devops", "power-bi", "synapse"],
        "explanation": "Em um pipeline do Azure DevOps ou do Azure ML, após a etapa de treinamento, você pode ter duas etapas paralelas. Uma executa um script de avaliação de métricas (ex: acurácia). A outra executa os componentes do Responsible AI para gerar o dashboard de equidade. Uma etapa subsequente de 'gate' verifica os resultados de ambas antes de aprovar o registro do modelo."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Ao receber um documento legal, em paralelo, use o Azure AI Services para resumi-lo e para extrair as entidades contratuais (nomes, datas, valores).",
        "solution": ["functions", "cognitive-services", [["summarization"], ["entity-extraction"]]],
        "availableServices": ["functions", "cognitive-services", "form-recognizer", "logic-apps"],
        "explanation": "Uma Azure Function recebe o texto do documento. Ela faz duas chamadas paralelas à API do Azure AI Language: uma para o recurso de sumarização de documentos e outra para o recurso de reconhecimento de entidades nomeadas (NER). Os resultados são então combinados."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Para análise de portfólio, um pipeline é acionado. Em paralelo, ele busca os últimos preços de ações de uma API externa (via Functions) e os dados de transações do cliente de um banco de dados Synapse. Os dados são então combinados para calcular o valor do portfólio.",
        "solution": ["data-factory", [["functions"], ["synapse"]], "synapse-spark"],
        "availableServices": ["data-factory", "functions", "synapse", "databricks", "power-bi"],
        "explanation": "O Data Factory orquestra o fluxo. Ele executa duas atividades de busca de dados em paralelo: uma que chama uma Azure Function para obter os preços e outra que consulta o Synapse. As saídas de ambas as atividades são então usadas como entrada para uma atividade de Notebook Spark do Synapse para o cálculo final."
    },
    {
        "category": "data-analysis",
        "difficulty": 5,
        "question": "Um novo artigo científico é publicado. Em paralelo, use o Azure AI Search para indexar o conteúdo para busca, use o Azure AI Language para extrair as entidades-chave e os resumos, e use o Azure AI Speech para gerar uma versão em áudio do artigo.",
        "solution": ["blob-storage", "logic-apps", [["cognitive-search"], ["cognitive-services-language"], ["cognitive-services-speech"]]],
        "availableServices": ["blob-storage", "logic-apps", "cognitive-search", "cognitive-services-language", "cognitive-services-speech"],
        "explanation": "Um Logic App é acionado pelo novo artigo no Blob Storage. Ele executa três ações de IA em paralelo: uma para adicionar/atualizar o índice de busca, uma para extrair insights do texto, e uma para converter o texto em fala. Isso cria múltiplos ativos analíticos a partir de uma única fonte."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Ao receber uma imagem de um recibo, extraia os itens da linha e o total com o Document Intelligence e, em paralelo, identifique o logotipo do comerciante com o AI Vision.",
        "solution": ["functions", [["form-recognizer"], ["cognitive-services-vision"]]],
        "availableServices": ["functions", "form-recognizer", "cognitive-services-vision", "blob-storage"],
        "explanation": "A Function que recebe a imagem faz duas chamadas paralelas: uma para o modelo pré-construído de recibos do Azure AI Document Intelligence e outra para a API de reconhecimento de marca/logo do Azure AI Vision. Ambos os resultados são combinados para uma análise completa do recibo."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Um conjunto de dados é carregado para análise. Em paralelo, o Power BI é atualizado com os novos dados, e um Jupyter Notebook no Synapse é executado para gerar análises estatísticas mais profundas.",
        "solution": ["data-lake-storage", "data-factory", [["power-bi"], ["synapse-notebook"]]],
        "availableServices": ["data-lake-storage", "data-factory", "power-bi", "synapse-notebook", "databricks"],
        "explanation": "Um pipeline do Data Factory é acionado pelo novo arquivo no Data Lake. Ele executa duas atividades em paralelo: uma que atualiza o dataset do Power BI e outra que executa um Synapse Notebook para as análises programáticas, atendendo tanto a usuários de negócio quanto a cientistas de dados."
    },
    {
        "category": "data-analysis",
        "difficulty": 5,
        "question": "Para uma análise 'what-if', um usuário submete parâmetros via Power Apps. Em paralelo, uma simulação de Monte Carlo é executada no Azure Batch e um modelo de previsão é executado no Azure ML. Os resultados de ambos são retornados para visualização.",
        "solution": ["power-apps", "logic-apps", [["batch"], ["machine-learning"]], "power-bi"],
        "availableServices": ["power-apps", "logic-apps", "batch", "machine-learning", "power-bi"],
        "explanation": "A submissão no Power Apps aciona um Logic App. O Logic App inicia duas tarefas de computação pesada em paralelo: um job no Azure Batch para a simulação e uma chamada para um endpoint do Azure ML. O Logic App aguarda a conclusão de ambos, consolida os resultados e os envia para um dataset do Power BI para o usuário visualizar."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Um novo modelo de dados é publicado no Purview. Em paralelo, acione um pipeline do Data Factory para começar a popular os dados e envie uma notificação para a equipe de BI informando que o novo modelo está disponível para consumo.",
        "solution": ["purview", "event-grid", [["data-factory"], ["logic-apps"]]],
        "availableServices": ["purview", "event-grid", "data-factory", "logic-apps", "synapse"],
        "explanation": "O Purview pode emitir eventos para o Event Grid quando entidades do catálogo são atualizadas. Duas assinaturas de evento paralelas são criadas: uma que aciona o pipeline do Data Factory para iniciar o ETL e outra que aciona um Logic App para enviar a notificação para a equipe."
    },
    {
        "category": "cloud-operations",
        "difficulty": 1,
        "question": "Crie um balanceador de carga que distribua tráfego TCP para um conjunto de máquinas virtuais.",
        "solution": ["load-balancer", "vm"],
        "availableServices": ["load-balancer", "app-gateway", "front-door", "vm"],
        "explanation": "O Azure Load Balancer opera na camada 4 (TCP/UDP) do modelo OSI, sendo a escolha ideal para balancear tráfego que não é HTTP/HTTPS entre máquinas virtuais."
    },
    {
        "category": "cloud-operations",
        "difficulty": 2,
        "question": "Implemente uma forma segura de se conectar a uma VM privada em uma VNet sem expor nenhuma porta para a internet.",
        "solution": ["vm", "bastion"],
        "availableServices": ["vm", "bastion", "nsg", "vpn-gateway"],
        "explanation": "O Azure Bastion é um serviço PaaS que fornece um 'jump box' gerenciado. Ele permite que você se conecte às suas VMs via RDP ou SSH diretamente pelo portal do Azure, usando um túnel seguro, sem a necessidade de IPs públicos nas VMs."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Crie uma política do Azure que audite todas as contas de armazenamento que não têm o acesso via chave de acesso desabilitado.",
        "solution": ["policy", "storage-account"],
        "availableServices": ["policy", "monitor", "defender-for-cloud", "storage-account"],
        "explanation": "O Azure Policy pode ser usado para auditar e impor configurações de segurança. Existe uma política interna que verifica se a propriedade 'allowSharedKeyAccess' está habilitada, ajudando a garantir que a autenticação via Azure AD seja priorizada."
    },
    {
        "category": "cloud-operations",
        "difficulty": 1,
        "question": "Como você pode visualizar os custos da sua assinatura Azure, divididos por grupo de recursos ou por serviço?",
        "solution": ["cost-management"],
        "availableServices": ["cost-management", "advisor", "monitor", "policy"],
        "explanation": "O Microsoft Cost Management and Billing fornece ferramentas para monitorar, alocar e otimizar os custos. A ferramenta de 'Análise de custos' permite filtrar e agrupar despesas por vários critérios, como serviço, localização e tags."
    },
    {
        "category": "cloud-operations",
        "difficulty": 2,
        "question": "Configure uma VNet para se comunicar com outra VNet na mesma região de forma privada.",
        "solution": ["vnet", "peering"],
        "availableServices": ["vnet", "peering", "vpn-gateway", "expressroute"],
        "explanation": "O VNet Peering conecta duas redes virtuais do Azure. Uma vez pareadas, as VNets aparecem como uma só para fins de conectividade, e o tráfego entre elas usa a infraestrutura de backbone da Microsoft, não a internet pública."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Orquestre uma sequência de tarefas: primeiro, execute um script em um Automation Account; se for bem-sucedido, inicie uma Azure Function.",
        "solution": ["logic-apps", "automation-account", "functions"],
        "availableServices": ["logic-apps", "automation-account", "functions", "event-grid"],
        "explanation": "O Azure Logic Apps é ideal para orquestrar fluxos de trabalho entre diferentes serviços. Você pode usar um conector para iniciar um runbook no Automation Account e, com base no resultado, usar outro conector para chamar a Azure Function."
    },
    {
        "category": "cloud-operations",
        "difficulty": 2,
        "question": "Hospede uma aplicação em contêiner Docker sem gerenciar um cluster Kubernetes.",
        "solution": ["app-service", "container-registry"],
        "availableServices": ["app-service", "aks", "container-apps", "container-registry"],
        "explanation": "O Azure App Service for Containers permite executar contêineres diretamente na plataforma PaaS do App Service. Ele gerencia a infraestrutura subjacente, e você pode implantar uma imagem de um registro como o Azure Container Registry."
    },
    {
        "category": "cloud-operations",
        "difficulty": 1,
        "question": "Atribua uma identidade a uma Máquina Virtual para que ela possa acessar um Key Vault sem precisar de senhas ou segredos.",
        "solution": ["vm", "managed-identity", "key-vault"],
        "availableServices": ["vm", "managed-identity", "key-vault", "azure-ad"],
        "explanation": "Habilitando uma Identidade Gerenciada (Managed Identity) na VM, você cria uma identidade para ela no Azure AD. Você pode então conceder a essa identidade permissões de acesso no Key Vault, permitindo que a VM se autentique de forma segura."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Implemente um sistema de arquivos compartilhado e de alto desempenho para uma carga de trabalho de HPC (High-Performance Computing) no Azure.",
        "solution": ["vm", "azure-hpc-cache"],
        "availableServices": ["vm", "azure-hpc-cache", "blob-storage", "azure-files"],
        "explanation": "Para cargas de trabalho de HPC, que exigem latência extremamente baixa e alto IOPS/throughput, serviços como Azure NetApp Files ou o Lustre (através de parceiros) são as melhores opções. O Blob Storage não oferece a semântica de sistema de arquivos POSIX necessária."
    },
    {
        "category": "cloud-operations",
        "difficulty": 2,
        "question": "Crie um orçamento que notifique múltiplos stakeholders quando os custos atingirem 50%, 75% e 100% do valor definido.",
        "solution": ["budgets"],
        "availableServices": ["budgets", "cost-management", "advisor", "monitor"],
        "explanation": "No Azure Budgets, você pode criar múltiplos limites de alerta para um mesmo orçamento. Para cada percentual (50%, 75%, 100%), você pode configurar um 'Grupo de Ação' que notifica uma lista de e-mails ou outros endpoints."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Crie uma réplica de um banco de dados SQL do Azure em outra região para fins de recuperação de desastres e leitura.",
        "solution": ["sql-database", "geo-replication"],
        "availableServices": ["sql-database", "geo-replication", "site-recovery", "backup"],
        "explanation": "A Geo-replicação ativa do Azure SQL Database permite criar réplicas secundárias legíveis em diferentes regiões. Ela replica os dados de forma assíncrona e pode ser usada para failover em caso de um desastre regional."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Monitore a integridade dos arquivos e registros de um servidor Windows para detectar alterações não autorizadas.",
        "solution": ["vm", "defender-for-cloud"],
        "availableServices": ["vm", "defender-for-cloud", "monitor", "policy"],
        "explanation": "O Microsoft Defender for Cloud inclui um recurso de Monitoramento da Integridade do Arquivo (FIM). Ele usa o Log Analytics Agent para rastrear alterações em arquivos, chaves de registro e serviços do Windows, alertando sobre atividades suspeitas."
    },
    {
        "category": "cloud-operations",
        "difficulty": 2,
        "question": "Forneça acesso a arquivos de uma conta de armazenamento usando o protocolo SMB 3.0, como se fosse um compartilhamento de rede tradicional.",
        "solution": ["azure-files"],
        "availableServices": ["azure-files", "blob-storage", "data-lake-storage", "disk-storage"],
        "explanation": "O Azure Files oferece compartilhamentos de arquivos na nuvem totalmente gerenciados que são acessíveis através dos protocolos SMB e NFS. Eles podem ser montados por clientes Windows, Linux e macOS, tanto na nuvem quanto on-premises."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Use um serviço gerenciado para orquestrar e agendar a execução de um pipeline do Azure Data Factory.",
        "solution": ["data-factory"],
        "availableServices": ["data-factory", "logic-apps", "automation-account", "functions"],
        "explanation": "O próprio Azure Data Factory possui recursos de agendamento robustos. Você pode criar 'gatilhos' (triggers) baseados em tempo (agendamento), em eventos de armazenamento, ou em uma janela de tempo (tumbling window)."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Gerencie a configuração de um ambiente de desenvolvimento virtual para uma equipe usando um serviço PaaS.",
        "solution": ["dev-box"],
        "availableServices": ["dev-box", "vm", "avd", "codespaces"],
        "explanation": "O Microsoft Dev Box é um serviço gerenciado que permite criar estações de trabalho de desenvolvedor pré-configuradas e prontas para codificar na nuvem. Os administradores podem definir imagens e configurações padrão, e os desenvolvedores podem acessá-las sob demanda."
    },
    {
        "category": "cloud-operations",
        "difficulty": 2,
        "question": "Use uma ferramenta visual no portal do Azure para mover um arquivo de uma conta de armazenamento para outra.",
        "solution": ["storage-explorer"],
        "availableServices": ["storage-explorer", "azcopy", "data-factory", "blob-storage"],
        "explanation": "O Azure Storage Explorer, que está integrado ao portal do Azure e também disponível como um aplicativo de desktop, fornece uma interface gráfica para gerenciar o conteúdo de suas contas de armazenamento, incluindo copiar e mover arquivos."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Hospede um desktop virtual Windows 10/11 na nuvem, permitindo que os usuários acessem seu ambiente de trabalho de qualquer lugar.",
        "solution": ["avd"],
        "availableServices": ["avd", "vm", "dev-box", "citrix-on-azure"],
        "explanation": "O Azure Virtual Desktop (AVD) é um serviço de virtualização de desktop e aplicativo que é executado na nuvem. Ele permite provisionar desktops Windows multi-sessão ou pessoais para os usuários."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Projete uma solução de log centralizada que colete logs do AKS, App Services e VMs em um único workspace e crie dashboards de monitoramento.",
        "solution": ["aks", "app-service", "vm", "log-analytics", "monitor"],
        "availableServices": ["log-analytics", "monitor", "aks", "app-service", "vm", "application-insights"],
        "explanation": "Um Log Analytics Workspace central é o destino. A integração do AKS com o Azure Monitor, as configurações de diagnóstico do App Service e o agente do Azure Monitor nas VMs são todos configurados para enviar logs e métricas para este workspace. Os dashboards podem ser criados usando o Azure Monitor Workbooks."
    },
    {
        "category": "cloud-operations",
        "difficulty": 2,
        "question": "Crie um banco de dados NoSQL globalmente distribuído para uma aplicação que requer baixa latência de leitura e escrita em todo o mundo.",
        "solution": ["cosmos-db"],
        "availableServices": ["cosmos-db", "sql-database", "table-storage", "redis"],
        "explanation": "O Azure Cosmos DB foi projetado para distribuição global. Ele permite replicar dados para qualquer região do Azure e suporta escritas em múltiplas regiões, garantindo que as operações de leitura e escrita sejam atendidas pela cópia mais próxima do usuário."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Exponha múltiplos microsserviços (Functions, App Services, AKS) através de uma fachada unificada, aplicando políticas de segurança e transformação.",
        "solution": ["functions", "app-service", "aks", "api-management"],
        "availableServices": ["api-management", "app-gateway", "front-door", "functions", "app-service", "aks"],
        "explanation": "O Azure API Management atua como um gateway de API. Ele pode receber todas as requisições, aplicar políticas (autenticação, limitação de taxa, cache) e, em seguida, rotear as chamadas para os diversos serviços de backend, independentemente de onde eles estejam hospedados."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Implemente uma solução para que uma aplicação em uma VNet acesse um banco de dados SQL do Azure de forma privada, sem que o tráfego passe pela internet pública.",
        "solution": ["vnet", "sql-database", "private-endpoint"],
        "availableServices": ["vnet", "sql-database", "private-endpoint", "service-endpoint", "vpn-gateway"],
        "explanation": "Um Private Endpoint para o Azure SQL Database cria uma interface de rede com um endereço IP privado dentro da sua VNet. O tráfego da aplicação para o banco de dados é roteado através deste endpoint, permanecendo inteiramente na rede privada da Microsoft."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Crie um cluster de computação para executar jobs de Machine Learning de forma escalável.",
        "solution": ["machine-learning"],
        "availableServices": ["machine-learning", "databricks", "batch", "aks"],
        "explanation": "Dentro de um workspace do Azure Machine Learning, você pode criar um 'Cluster de Computação'. Este é um cluster gerenciado de VMs que pode escalar para cima e para baixo automaticamente, usado para executar jobs de treinamento de modelos de ML."
    },
    {
        "category": "cloud-operations",
        "difficulty": 2,
        "question": "Configure a autenticação de dois fatores (MFA) para todos os administradores da sua assinatura Azure.",
        "solution": ["azure-ad", "conditional-access"],
        "availableServices": ["azure-ad", "conditional-access", "policy", "defender-for-cloud"],
        "explanation": "Usando o Acesso Condicional (Conditional Access) do Azure AD, você pode criar uma política que exige a autenticação multifator (MFA) para usuários que tentam acessar o portal do Azure e têm uma função de administrador atribuída."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Armazene e gerencie variáveis e parâmetros para diferentes ambientes (Dev, QA, Prod) em um pipeline do Azure DevOps.",
        "solution": ["devops", "variable-groups", "key-vault"],
        "availableServices": ["devops", "variable-groups", "key-vault", "app-configuration"],
        "explanation": "Os 'Variable Groups' no Azure DevOps são usados para armazenar variáveis que podem ser compartilhadas entre múltiplos pipelines. Para segredos, você pode vincular um Variable Group a um Azure Key Vault, permitindo que o pipeline acesse os segredos de forma segura em tempo de execução."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Hospede um serviço de API GraphQL na Azure de forma gerenciada.",
        "solution": ["api-management", "app-service"],
        "availableServices": ["api-management", "app-service", "cosmos-db", "functions"],
        "explanation": "O Azure API Management tem suporte nativo para importar e gerenciar APIs GraphQL. Você pode hospedar a implementação do seu servidor GraphQL em um serviço como o App Service ou Functions, e usar o APIM para aplicar políticas, segurança e explorar o schema."
    },
    {
        "category": "cloud-operations",
        "difficulty": 5,
        "question": "Projete uma arquitetura para análise de dados confidenciais onde múltiplos parceiros podem colaborar em um conjunto de dados, mas não podem ver os dados brutos uns dos outros.",
        "solution": ["machine-learning", "confidential-computing"],
        "availableServices": ["machine-learning", "confidential-computing", "key-vault", "synapse"],
        "explanation": "Usando VMs de Computação Confidencial (Confidential Computing) com enclaves seguros (SGX), os dados podem ser processados na memória de forma criptografada. O Azure Machine Learning pode usar essas VMs para treinamento, garantindo que os dados permaneçam protegidos mesmo durante o uso."
    },
    {
        "category": "cloud-operations",
        "difficulty": 2,
        "question": "Use uma ferramenta para desenhar visualmente um fluxo de trabalho que integra o Twitter com o Office 365.",
        "solution": ["logic-apps"],
        "availableServices": ["logic-apps", "functions", "power-automate", "data-factory"],
        "explanation": "O Azure Logic Apps é a ferramenta ideal para isso, pois possui centenas de conectores pré-construídos, incluindo para serviços de terceiros como o Twitter e para o ecossistema da Microsoft como o Office 365, permitindo criar a integração de forma visual."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Crie um 'image factory' para produzir e distribuir imagens de VM personalizadas e seguras para toda a organização.",
        "solution": ["image-builder", "compute-gallery"],
        "availableServices": ["image-builder", "compute-gallery", "packer", "devops"],
        "explanation": "O Azure Image Builder automatiza a criação de imagens. Após a imagem ser criada e customizada (ex: com patches e software), ela pode ser distribuída para uma Azure Compute Gallery (anteriormente Shared Image Gallery), que permite o compartilhamento e o versionamento de imagens entre subscriptions."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Crie um banco de dados PostgreSQL com replicação e failover gerenciados para alta disponibilidade dentro de uma região.",
        "solution": ["postgresql-flexible-server"],
        "availableServices": ["postgresql-flexible-server", "vm", "site-recovery", "cosmos-db"],
        "explanation": "O Azure Database for PostgreSQL - Flexible Server oferece uma opção de alta disponibilidade com redundância de zona. Ele provisiona uma réplica de standby em uma zona de disponibilidade diferente e gerencia a replicação síncrona e o failover automático."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Configure um domínio personalizado com HTTPS para um site estático hospedado no Blob Storage.",
        "solution": ["blob-storage", "cdn"],
        "availableServices": ["blob-storage", "cdn", "app-gateway", "front-door"],
        "explanation": "Embora o Blob Storage possa servir o site, para usar um domínio personalizado com HTTPS, a melhor prática é usar o Azure CDN na frente. O CDN pode gerenciar o certificado SSL/TLS (ou você pode usar um seu) e apontar para a origem do site estático no armazenamento."
    },
    {
        "category": "cloud-operations",
        "difficulty": 5,
        "question": "Projete uma arquitetura de microsserviços orientada a eventos usando um service mesh para gerenciar a comunicação, resiliência e observabilidade entre os contêineres no AKS.",
        "solution": ["aks", "service-mesh", "monitor"],
        "availableServices": ["aks", "service-mesh", "api-management", "app-gateway", "monitor"],
        "explanation": "O AKS fornece o ambiente Kubernetes. Um service mesh (como o Istio ou Linkerd, instalados como add-ons) cria uma camada de rede dedicada. Ele gerencia o roteamento de tráfego, implementa padrões de resiliência como 'circuit breakers' e 'retries', e fornece telemetria detalhada para o Azure Monitor."
    },
    {
        "category": "cloud-operations",
        "difficulty": 2,
        "question": "Armazene e gerencie configurações centralizadas para uma aplicação, como feature flags, separadas do código.",
        "solution": ["app-configuration"],
        "availableServices": ["app-configuration", "key-vault", "devops", "config-file"],
        "explanation": "O Azure App Configuration é um serviço para gerenciar centralmente as configurações da aplicação e as feature flags. Ele permite atualizar as configurações sem reimplantar a aplicação e se integra com o Key Vault para referenciar segredos."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Crie uma arquitetura de 3 camadas (web, aplicação, dados) usando serviços PaaS para minimizar o gerenciamento de infraestrutura.",
        "solution": ["app-service", "functions", "sql-database"],
        "availableServices": ["app-service", "functions", "sql-database", "vm", "aks"],
        "explanation": "A camada web pode ser um App Service. A camada de aplicação pode ser implementada com Azure Functions (para lógica de negócio) ou outro App Service. A camada de dados usa um serviço gerenciado como o Azure SQL Database. Nenhuma VM precisa ser gerenciada diretamente."
    },
    {
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Use uma ferramenta visual para construir um pipeline que copia dados de uma API REST para uma tabela do SQL Database.",
        "solution": ["data-factory"],
        "availableServices": ["data-factory", "logic-apps", "functions", "databricks"],
        "explanation": "A interface de usuário do Azure Data Factory permite criar um pipeline com uma atividade de cópia. Você pode configurar a API REST como a fonte (source) e a tabela do SQL Database como o destino (sink), mapeando os campos visualmente."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Implemente um padrão de 'slowly changing dimension' (SCD) Tipo 2 em um Data Warehouse Synapse usando Data Factory.",
        "solution": ["data-factory", "synapse"],
        "availableServices": ["data-factory", "synapse", "databricks", "functions"],
        "explanation": "O Mapping Data Flows no Data Factory possui transformações como 'Lookup', 'Derived Column' e 'Conditional Split' que podem ser usadas para comparar os dados de entrada com os existentes no Synapse, identificar registros novos ou alterados e inserir as novas versões, implementando a lógica do SCD Tipo 2."
    },
    {
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Crie um pipeline que executa uma Stored Procedure em um banco de dados Synapse após a conclusão de um job de cópia de dados.",
        "solution": ["data-factory", "synapse"],
        "availableServices": ["data-factory", "synapse", "logic-apps", "functions"],
        "explanation": "No pipeline do Data Factory, você pode encadear atividades. Após a atividade 'Copy data', você pode adicionar uma atividade 'Stored Procedure' e criar uma dependência de 'sucesso' entre elas, garantindo que o procedimento só seja executado se a cópia for bem-sucedida."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Capture as alterações de uma tabela do Cosmos DB em tempo real e as materialize em uma view agregada em um Cache Redis para um dashboard.",
        "solution": ["cosmos-db", "functions", "redis"],
        "availableServices": ["cosmos-db", "functions", "redis", "stream-analytics", "synapse-link"],
        "explanation": "O 'Change Feed' do Cosmos DB pode acionar uma Azure Function para cada alteração. A função lê a alteração, atualiza o valor agregado correspondente (ex: um contador) e o grava no Cache Redis, que pode ser consultado rapidamente pelo dashboard."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Projete uma solução para processar 1TB de arquivos JSON com Spark no Databricks, otimizando a performance da leitura a partir do Data Lake.",
        "solution": ["data-lake-storage", "databricks"],
        "availableServices": ["data-lake-storage", "databricks", "synapse", "hdinsight"],
        "explanation": "O Databricks pode ler os arquivos JSON diretamente do Data Lake. Para otimizar, o ideal é usar o Autoloader do Databricks para ingestão incremental e salvar os dados em formato Delta Lake (Parquet otimizado), o que acelera drasticamente as consultas subsequentes."
    },
    {
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Crie um data warehouse que pode escalar a computação para zero quando não está em uso, a fim de economizar custos.",
        "solution": ["synapse-dedicated-pool"],
        "availableServices": ["synapse-dedicated-pool", "sql-database", "databricks", "vm"],
        "explanation": "Os pools SQL dedicados do Azure Synapse Analytics podem ser pausados. Quando pausados, a computação é liberada e você paga apenas pelo armazenamento de dados. Você pode retomar o pool quando precisar executar consultas novamente."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Ingira dados de um tópico Kafka on-premises e os replique para o Azure Event Hubs para processamento na nuvem.",
        "solution": ["kafka-on-prem", "event-hubs", "kafka-connect"],
        "availableServices": ["event-hubs", "hdinsight", "databricks", "kafka-connect"],
        "explanation": "O Azure Event Hubs expõe um endpoint compatível com a API do Kafka. Você pode usar o Kafka Connect com o conector do Event Hubs em seu ambiente on-premises para replicar os dados do seu tópico Kafka local para o Event Hubs na nuvem."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Use o Purview para aplicar automaticamente uma política que mascara colunas classificadas como 'CPF' para usuários que não pertencem a um grupo de segurança específico.",
        "solution": ["synapse", "purview", "azure-ad"],
        "availableServices": ["synapse", "purview", "data-factory", "azure-ad"],
        "explanation": "O Purview pode ser configurado com políticas de acesso a dados. Uma política pode definir que, para uma determinada classificação ('CPF'), uma ação de mascaramento dinâmico de dados (DDM) deve ser aplicada no Synapse, exceto para membros de um grupo específico do Azure AD."
    },
    {
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Crie um banco de dados NoSQL com a API do MongoDB de forma gerenciada no Azure.",
        "solution": ["cosmos-db-mongodb-api"],
        "availableServices": ["cosmos-db-mongodb-api", "vm-with-mongodb", "sql-database"],
        "explanation": "O Azure Cosmos DB oferece múltiplas APIs, incluindo uma API para MongoDB. Isso permite que você execute suas aplicações MongoDB no Azure usando o Cosmos DB como um backend PaaS, sem alterar o código da aplicação que usa drivers do MongoDB."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Desencadeie a execução de um pipeline do Data Factory somente quando dois arquivos diferentes (ex: `clientes.csv` e `pedidos.csv`) chegarem em um container do Blob Storage.",
        "solution": ["blob-storage", "logic-apps", "data-factory"],
        "availableServices": ["blob-storage", "logic-apps", "data-factory", "functions"],
        "explanation": "Um Logic App pode ser usado para orquestrar essa lógica. Ele pode ter dois gatilhos de blob separados. O fluxo de trabalho aguarda que ambos os gatilhos sejam disparados (usando variáveis de controle) e, somente então, ele chama a API para iniciar a execução do pipeline do Data Factory."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie uma arquitetura de streaming que ingere dados do IoT Hub, os enriquece com dados do SQL Database e detecta anomalias temporais, tudo em um único serviço.",
        "solution": ["iot-hub", "stream-analytics", "sql-database"],
        "availableServices": ["iot-hub", "stream-analytics", "sql-database", "functions", "databricks"],
        "explanation": "O Azure Stream Analytics é capaz de fazer tudo isso. Ele pode ter o IoT Hub como entrada, o SQL Database como uma 'entrada de referência' para enriquecimento via JOIN, e usar funções de detecção de anomalias baseadas em ML em sua consulta SQL."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Otimize o custo e a performance de um pipeline do Data Factory que processa dados em diferentes regiões, usando o Integration Runtime mais apropriado.",
        "solution": ["data-factory", "integration-runtime"],
        "availableServices": ["data-factory", "integration-runtime", "vnet", "expressroute"],
        "explanation": "Ao mover dados entre regiões, você deve criar um Azure Integration Runtime em cada região. Isso garante que a computação para a movimentação dos dados ocorra localmente na região de origem e destino, otimizando a performance e evitando custos de transferência de dados desnecessários."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Implemente um pipeline de dados que possa se recuperar de falhas, reprocessando apenas os arquivos que falharam em uma janela de tempo específica.",
        "solution": ["data-factory", "tumbling-window-trigger"],
        "availableServices": ["data-factory", "tumbling-window-trigger", "logic-apps", "databricks"],
        "explanation": "Usando um 'Tumbling Window Trigger' no Data Factory, você pode processar dados em fatias de tempo contíguas. Se uma janela falhar, ela não afeta as outras. Você pode re-executar manualmente a janela que falhou, e o pipeline reprocessará apenas os dados daquela fatia de tempo."
    },
    {
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Construa um pipeline de dados que processe dados PII em um ambiente seguro, usando o Data Factory em uma VNet gerenciada e Private Endpoints para se conectar a todas as fontes e destinos de dados.",
        "solution": ["data-factory", "data-lake-storage", "synapse", "vnet", "private-endpoint"],
        "availableServices": ["data-factory", "data-lake-storage", "synapse", "vnet", "private-endpoint"],
        "explanation": "O Data Factory pode ser configurado com um 'Managed Virtual Network' e um 'Managed Private Endpoint'. Isso garante que toda a movimentação de dados orquestrada pelo Data Factory ocorra dentro de uma rede isolada, conectando-se a outros serviços PaaS (como Data Lake e Synapse) através de seus Private Endpoints."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Execute um job do Spark no Synapse Analytics para analisar arquivos Parquet e criar uma nova tabela particionada por ano e mês.",
        "solution": ["data-lake-storage", "synapse-spark"],
        "availableServices": ["data-lake-storage", "synapse-spark", "databricks", "hdinsight"],
        "explanation": "Dentro de um workspace do Synapse, você pode criar um Notebook que usa um pool Spark. O código PySpark pode ler os arquivos Parquet do Data Lake, adicionar colunas de ano e mês, e usar `partitionBy('year', 'month')` ao escrever os dados de volta para o Data Lake, criando a estrutura de partições."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie uma solução para que os dados do Cosmos DB estejam disponíveis para análise no Synapse Analytics sem a necessidade de pipelines de ETL.",
        "solution": ["cosmos-db", "synapse-link"],
        "availableServices": ["cosmos-db", "synapse-link", "data-factory", "functions"],
        "explanation": "O Azure Synapse Link for Cosmos DB é um recurso que cria uma cópia dos seus dados operacionais em um 'analytical store' colunar, otimizado para análise. Isso permite que você execute consultas analíticas com o Synapse Serverless ou Spark sobre os dados do Cosmos DB quase em tempo real, sem impactar a performance transacional."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Automatize a criação e o descarte de um ambiente de desenvolvimento do Databricks para um job de ETL usando Azure DevOps.",
        "solution": ["databricks", "devops", "arm-template"],
        "availableServices": ["databricks", "devops", "arm-template", "bicep"],
        "explanation": "Um pipeline do Azure DevOps pode ter um estágio para provisionar a infraestrutura (workspace do Databricks, cluster) usando um template ARM/Bicep. Um estágio subsequente executa o job. Um estágio final de 'cleanup' destrói o grupo de recursos, garantindo que você pague pela computação apenas durante a execução."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Monitore a execução de um pipeline do Data Factory, coletando métricas de performance (duração, linhas lidas/escritas) e logs de erro em um local centralizado.",
        "solution": ["data-factory", "log-analytics", "monitor"],
        "availableServices": ["data-factory", "log-analytics", "monitor", "purview"],
        "explanation": "Você pode configurar o Data Factory para enviar seus logs de diagnóstico para um Log Analytics Workspace. Isso permite que você use consultas KQL para analisar a performance de todas as execuções de pipeline, criar dashboards no Azure Monitor e configurar alertas para falhas."
    },
    {
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete um data lakehouse usando Azure Databricks, seguindo a arquitetura medalhão (Bronze, Silver, Gold) para processamento e refinamento progressivo dos dados.",
        "solution": ["data-lake-storage", "databricks", "delta-lake"],
        "availableServices": ["data-lake-storage", "databricks", "delta-lake", "synapse", "data-factory"],
        "explanation": "Os dados brutos são ingeridos na camada Bronze (Delta Lake). Um job do Databricks limpa e valida os dados, salvando-os na camada Silver. Outro job agrega e enriquece os dados da camada Silver, criando os datasets prontos para consumo na camada Gold. Todo o processo usa tabelas Delta para garantir a qualidade e a confiabilidade."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Crie um gatilho no Data Factory que execute um pipeline a cada 15 minutos, mas apenas durante o horário comercial (9h às 18h, de segunda a sexta).",
        "solution": ["data-factory", "schedule-trigger"],
        "availableServices": ["data-factory", "schedule-trigger", "logic-apps", "automation-account"],
        "explanation": "Ao criar um 'Schedule Trigger' no Data Factory, além de definir a recorrência (a cada 15 minutos), você pode especificar opções avançadas, como os dias da semana e as horas do dia em que o gatilho deve estar ativo, permitindo a criação de agendamentos complexos."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Faça o deploy de um modelo de Machine Learning treinado como um endpoint em tempo real e integre-o a um pipeline do Synapse para pontuar dados em lote.",
        "solution": ["machine-learning", "synapse"],
        "availableServices": ["machine-learning", "synapse", "data-factory", "functions"],
        "explanation": "Após o deploy do modelo no Azure ML, o Synapse Analytics pode usar a função `PREDICT` em suas consultas T-SQL (ou código Spark) para chamar o endpoint do modelo. Isso permite integrar a pontuação do modelo diretamente no fluxo de transformação de dados dentro do Synapse."
    },
    {
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Qual serviço é a base para o Microsoft Fabric, unificando o armazenamento de dados em um formato aberto e único para todas as cargas de trabalho analíticas?",
        "solution": ["one-lake"],
        "availableServices": ["one-lake", "data-lake-storage", "synapse", "power-bi"],
        "explanation": "O OneLake é um data lake único, unificado e lógico para toda a organização, que vem automaticamente com cada tenant do Microsoft Fabric. Ele é construído sobre o Azure Data Lake Storage Gen2 e armazena dados em formato Delta Parquet."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Replique dados de um banco de dados PostgreSQL para o Azure SQL Database, incluindo o schema e os dados.",
        "solution": ["database-migration-service", "postgresql", "sql-database"],
        "availableServices": ["database-migration-service", "data-factory", "databricks", "postgresql", "sql-database"],
        "explanation": "O Azure Database Migration Service (DMS) pode ser usado para realizar migrações heterogêneas, como de PostgreSQL para Azure SQL. Ele pode avaliar a compatibilidade, migrar o schema e, em seguida, migrar os dados (seja offline ou online)."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Crie um pipeline que, ao receber um arquivo compactado, o descompacte, processe cada arquivo interno e, ao final, delete o arquivo compactado original.",
        "solution": ["data-factory", "blob-storage"],
        "availableServices": ["data-factory", "blob-storage", "functions", "logic-apps"],
        "explanation": "No Data Factory, você pode usar uma atividade 'Copy' para descompactar o arquivo. Em seguida, uma atividade 'ForEach' pode iterar sobre os arquivos extraídos e processá-los. Uma atividade final 'Delete', dependente do sucesso do ForEach, pode ser usada para remover o arquivo .zip original."
    },
    {
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Implemente uma solução de governança de dados para uma arquitetura de 'data mesh', usando o Purview para catalogar os produtos de dados de cada domínio e gerenciar o acesso.",
        "solution": ["purview", "synapse", "data-factory", "azure-ad"],
        "availableServices": ["purview", "synapse", "data-factory", "azure-ad", "databricks"],
        "explanation": "Cada domínio usa seus próprios serviços (Synapse, Data Factory) para criar produtos de dados. O Microsoft Purview atua como o plano de governança central, onde esses produtos são descobertos, classificados e onde as políticas de acesso são definidas. O acesso aos dados é então concedido a grupos do Azure AD, independentemente do domínio."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Crie um cluster Hadoop gerenciado na nuvem para executar jobs MapReduce existentes.",
        "solution": ["hdinsight"],
        "availableServices": ["hdinsight", "databricks", "synapse", "vm"],
        "explanation": "O Azure HDInsight é um serviço gerenciado que permite criar clusters com frameworks de código aberto, incluindo o Hadoop. Ele é ideal para migrar cargas de trabalho de MapReduce, Hive, HBase, etc., para a nuvem."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Desenvolva uma Azure Function em Python com dependências (como Pandas e NumPy) para realizar uma transformação de dados.",
        "solution": ["functions", "vscode", "requirements.txt"],
        "availableServices": ["functions", "vscode", "data-factory", "databricks"],
        "explanation": "Ao desenvolver a Azure Function localmente (ex: no VS Code), você pode especificar as dependências no arquivo `requirements.txt`. Durante o processo de implantação, essas dependências são instaladas automaticamente no ambiente da função no Azure, tornando-as disponíveis para o seu código."
    },
    {
        "category": "data-engineering",
        "difficulty": 2,
        "question": "Use uma ferramenta de linha de comando para sincronizar um diretório local com um container do Blob Storage, enviando apenas os arquivos novos ou modificados.",
        "solution": ["azcopy", "blob-storage"],
        "availableServices": ["azcopy", "storage-explorer", "robocopy", "blob-storage"],
        "explanation": "O comando `azcopy sync` é projetado para isso. Ele compara o diretório de origem com o de destino e copia apenas os arquivos que são diferentes, tornando o processo de sincronização muito mais eficiente do que uma cópia completa."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Crie um banco de dados que suporte a API do Cassandra de forma serverless e gerenciada.",
        "solution": ["cosmos-db-cassandra-api"],
        "availableServices": ["cosmos-db-cassandra-api", "hdinsight", "databricks", "vm"],
        "explanation": "O Azure Cosmos DB for Apache Cassandra oferece um banco de dados compatível com o protocolo Cassandra. Por ser serverless, ele escala automaticamente o throughput e o armazenamento com base na demanda, e você paga apenas pelos recursos que consome."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Faça o deploy de um modelo do Spark MLlib treinado no Databricks como um endpoint em tempo real para inferência.",
        "solution": ["databricks", "machine-learning", "aks"],
        "availableServices": ["databricks", "machine-learning", "aks", "synapse"],
        "explanation": "Após treinar o modelo no Databricks, você pode usar a integração com o MLflow e o Azure Machine Learning para registrá-lo. A partir do workspace do Azure ML, você pode então implantar o modelo em um serviço de inferência, como o AKS ou o Azure Container Instances (ACI), para servir predições em tempo real."
    },
    {
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Construa um pipeline que, ao receber um vídeo, use o Azure AI Video Indexer para gerar insights e, com base nos tópicos identificados, chame uma Azure Function para classificar o vídeo em uma taxonomia de negócio customizada.",
        "solution": ["blob-storage", "video-indexer", "logic-apps", "functions"],
        "availableServices": ["video-indexer", "logic-apps", "functions", "event-grid", "cognitive-services"],
        "explanation": "Um Logic App orquestra o fluxo. Ele é acionado pelo upload do vídeo e chama a API do Video Indexer. Ele então aguarda a conclusão (usando um loop de 'até que'). Após receber os insights (tópicos), ele os passa como entrada para uma Azure Function que contém a lógica de classificação customizada."
    },
    {
        "category": "data-analysis",
        "difficulty": 2,
        "question": "Crie um dashboard do Azure para monitorar a latência de uma API gerenciada pelo API Management e a contagem de requisições 4xx e 5xx.",
        "solution": ["api-management", "monitor", "azure-dashboard"],
        "availableServices": ["api-management", "monitor", "log-analytics", "application-insights"],
        "explanation": "O API Management envia métricas para o Azure Monitor. Você pode ir ao Azure Monitor, encontrar os gráficos para as métricas desejadas (latência, erros) e fixá-los (pin) em um Azure Dashboard compartilhado para criar um painel de monitoramento operacional (NOC)."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Use o Power BI para se conectar a um Log Analytics Workspace e criar um relatório sobre os 10 erros mais comuns de uma aplicação.",
        "solution": ["log-analytics", "power-bi"],
        "availableServices": ["log-analytics", "power-bi", "monitor", "data-explorer"],
        "explanation": "O Power BI pode se conectar ao Log Analytics como uma fonte de dados. Ele permite que você escreva (ou importe) uma consulta KQL para agregar os dados de log. O resultado dessa consulta é então usado para criar os visuais no Power BI."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Crie uma solução de análise interativa para um grande volume de logs de segurança, usando uma linguagem de consulta otimizada para séries temporais.",
        "solution": ["data-explorer", "power-bi"],
        "availableServices": ["data-explorer", "synapse", "log-analytics", "power-bi"],
        "explanation": "O Azure Data Explorer (Kusto) é altamente otimizado para análise de logs e séries temporais. Após ingerir os logs de segurança, você pode usar a linguagem KQL para consultas complexas e rápidas. O Power BI pode se conectar ao Data Explorer para visualização."
    },
    {
        "category": "data-analysis",
        "difficulty": 2,
        "question": "Em um relatório do Power BI, implemente a segurança em nível de linha (RLS) para garantir que um gerente de vendas só possa ver os dados de sua própria região.",
        "solution": ["power-bi", "azure-ad"],
        "availableServices": ["power-bi", "azure-ad", "synapse", "purview"],
        "explanation": "No Power BI Desktop, você pode criar 'funções' (roles) e definir filtros DAX para elas (ex: `[Region] = 'Nordeste'`). No serviço do Power BI, você mapeia essas funções a grupos ou usuários do Azure AD, garantindo que o filtro seja aplicado automaticamente quando o usuário acessa o relatório."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Use o Azure Machine Learning para treinar um modelo de regressão para prever o preço de imóveis e, em seguida, consuma esse modelo no Power BI para pontuar novos dados.",
        "solution": ["machine-learning", "power-bi"],
        "availableServices": ["machine-learning", "power-bi", "databricks", "synapse"],
        "explanation": "O Power BI tem uma integração com o Azure Machine Learning. Após treinar e implantar um modelo, você pode chamá-lo de dentro do Power Query no Power BI para aplicar o modelo a um fluxo de dados, criando novas colunas com as previsões."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Desenvolva uma solução de 'what-if analysis' no Power BI, permitindo que os usuários ajustem parâmetros (ex: % de desconto) em um slicer e vejam o impacto simulado nas vendas totais.",
        "solution": ["power-bi", "dax"],
        "availableServices": ["power-bi", "dax", "synapse", "machine-learning"],
        "explanation": "No Power BI, você pode criar um 'Parâmetro What-if', que gera uma tabela e um slicer. Você então cria uma medida DAX que usa o valor selecionado no slicer para calcular o resultado simulado (ex: `Vendas * (1 - ParametroDesconto)`), permitindo a análise interativa."
    },
    {
        "category": "data-analysis",
        "difficulty": 2,
        "question": "Analise o texto de e-mails para detectar o idioma e, se não for o português, traduza-o para o português.",
        "solution": ["functions", "cognitive-services"],
        "availableServices": ["functions", "cognitive-services", "logic-apps", "power-bi"],
        "explanation": "Uma Azure Function pode usar a API do Azure AI Language. Primeiro, ela chama o endpoint de 'detecção de idioma'. Se o idioma detectado não for 'pt', ela então chama o endpoint de 'tradução de texto' para obter a versão em português."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Crie um dashboard no Azure Monitor que combine métricas de uma VM (CPU, memória) e métricas de performance de uma aplicação (tempo de resposta) do Application Insights.",
        "solution": ["monitor", "application-insights", "vm"],
        "availableServices": ["monitor", "application-insights", "vm", "log-analytics"],
        "explanation": "Tanto as métricas da VM quanto as do Application Insights estão disponíveis no Azure Monitor. Você pode criar um 'Azure Dashboard' e fixar (pin) gráficos de ambos os serviços no mesmo painel, criando uma visão correlacionada da saúde da infraestrutura e da aplicação."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Crie um notebook no Synapse Analytics que lê dados de um Data Lake, executa uma análise estatística com PySpark e visualiza os resultados usando bibliotecas como Matplotlib.",
        "solution": ["synapse-spark", "data-lake-storage"],
        "availableServices": ["synapse-spark", "data-lake-storage", "databricks", "machine-learning"],
        "explanation": "O Azure Synapse Analytics suporta Notebooks que podem usar pools Spark. Você pode usar o Spark para carregar e manipular os dados do Data Lake, e as bibliotecas Python padrão de ciência de dados (Pandas, Matplotlib, Seaborn) para realizar e visualizar a análise dentro do próprio notebook."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Use o Azure AI Vision para analisar uma imagem e gerar uma descrição em texto (legenda) do que está acontecendo na imagem.",
        "solution": ["cognitive-services-vision", "functions"],
        "availableServices": ["cognitive-services-vision", "form-recognizer", "cognitive-search", "functions"],
        "explanation": "A API do Azure AI Vision possui um recurso de 'image captioning'. Você pode enviar uma imagem para a API (ex: a partir de uma Azure Function) e ela retornará uma descrição em linguagem natural da cena, como 'um cachorro brincando na grama'."
    },
    {
        "category": "data-analysis",
        "difficulty": 5,
        "question": "Projete uma solução de análise de logs em larga escala que seja otimizada tanto para busca de texto (Open Source) quanto para análise estruturada (Nativo Azure), usando uma única plataforma.",
        "solution": ["synapse", "data-explorer"],
        "availableServices": ["synapse", "data-explorer", "databricks", "log-analytics"],
        "explanation": "O Azure Synapse Analytics integra um 'Data Explorer pool' (baseado em Kusto). Isso permite que você ingira logs e dados de séries temporais e os analise com a linguagem KQL, que é otimizada para busca de texto e padrões. Ao mesmo tempo, você pode usar os pools SQL e Spark do Synapse para análises mais estruturadas sobre os mesmos dados."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Monitore os custos de uma consulta específica do Synapse Serverless e crie um alerta se o volume de dados processados por ela exceder um limite.",
        "solution": ["synapse-serverless", "log-analytics", "monitor"],
        "availableServices": ["synapse-serverless", "log-analytics", "monitor", "cost-management"],
        "explanation": "O Synapse envia logs de diagnóstico para o Log Analytics. Você pode criar uma 'Consulta de Log' no Azure Monitor que analisa esses logs para somar os 'data processed' de uma consulta específica. Essa consulta pode ser usada em uma 'Regra de Alerta' para notificar quando o limiar for excedido."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Transcreva uma gravação de áudio de uma reunião para texto e identifique quem falou em cada momento.",
        "solution": ["cognitive-services-speech"],
        "availableServices": ["cognitive-services-speech", "logic-apps", "functions", "video-indexer"],
        "explanation": "O serviço de Fala do Azure AI (Azure AI Speech) oferece um recurso chamado 'diarização'. Ao enviar um arquivo de áudio para a API de transcrição em lote com a diarização habilitada, ele não apenas transcreve o texto, mas também o segmenta, identificando as diferentes pessoas que falaram."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Crie um motor de busca para um catálogo de produtos que suporte filtros (facetas), sugestões de busca e ranqueamento customizado.",
        "solution": ["sql-database", "cognitive-search"],
        "availableServices": ["sql-database", "cognitive-search", "cosmos-db", "synapse"],
        "explanation": "O Azure AI Search é projetado para isso. Você pode criar um 'indexer' para extrair dados do SQL Database. No índice, você pode configurar quais campos são filtráveis (facetas), habilitar sugestões e criar perfis de pontuação para customizar a relevância dos resultados da busca."
    },
    {
        "category": "data-analysis",
        "difficulty": 5,
        "question": "Desenvolva uma solução de 'Responsible AI', avaliando um modelo de crédito no Azure ML para detectar vieses (bias) contra certos grupos demográficos e explicando suas previsões.",
        "solution": ["machine-learning", "responsible-ai-dashboard"],
        "availableServices": ["machine-learning", "responsible-ai-dashboard", "power-bi", "purview"],
        "explanation": "O Azure Machine Learning inclui o 'Responsible AI dashboard'. Ele permite analisar o modelo treinado em busca de equidade (fairness) entre subgrupos, entender a importância das features e gerar explicações para previsões individuais (model interpretability), ajudando a garantir que o modelo seja justo e transparente."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Use o Power BI para se conectar a um dataset do Databricks e agende a atualização do relatório para ocorrer diariamente.",
        "solution": ["databricks", "power-bi"],
        "availableServices": ["databricks", "power-bi", "data-factory", "logic-apps"],
        "explanation": "Após conectar o Power BI ao Databricks e publicar o relatório, você pode ir às configurações do conjunto de dados no serviço do Power BI. Lá, você pode inserir as credenciais e configurar uma 'atualização agendada' (scheduled refresh) para que o Power BI importe os dados do Databricks em um cronograma diário."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Crie um 'gêmeo digital' de um ambiente físico no Azure Digital Twins, ingira dados de sensores do IoT Hub e visualize o estado em tempo real.",
        "solution": ["iot-hub", "digital-twins", "functions", "power-bi"],
        "availableServices": ["digital-twins", "iot-hub", "functions", "power-bi", "synapse"],
        "explanation": "O Azure Digital Twins modela o ambiente. O IoT Hub recebe os dados dos sensores. Uma Azure Function é acionada por esses dados e atualiza as propriedades dos gêmeos digitais correspondentes. O Power BI pode então consultar a API do Digital Twins para visualizar o estado atual do ambiente modelado."
    },
    {
        "category": "data-analysis",
        "difficulty": 5,
        "question": "Implemente uma solução de análise de grafos para detecção de fraude, onde os dados das transações são armazenados no Cosmos DB e analisados para encontrar padrões de relacionamento suspeitos.",
        "solution": ["cosmos-db-graph-api", "power-bi"],
        "availableServices": ["cosmos-db-graph-api", "synapse", "databricks", "power-bi"],
        "explanation": "O Azure Cosmos DB suporta a API do Gremlin (Graph). Isso permite que você armazene dados como vértices e arestas. Você pode então executar consultas Gremlin para atravessar o grafo e identificar padrões complexos, como anéis de fraude. O Power BI pode se conectar para visualizar os resultados."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Analise uma conversa de chat para extrair a intenção do usuário (ex: 'comprar_produto', 'verificar_status') e as entidades relevantes (ex: 'produto A', 'pedido 123').",
        "solution": ["cognitive-services-language"],
        "availableServices": ["cognitive-services-language", "bot-service", "functions", "qna-maker"],
        "explanation": "O recurso de 'Conversational Language Understanding' (CLU) do Azure AI Language é projetado para isso. Você pode treiná-lo com exemplos para que ele possa, a partir de um texto, identificar a intenção geral e extrair as entidades específicas que você definiu."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Crie um dashboard de monitoramento para um cluster Databricks, mostrando o uso de DBU (Databricks Unit), o número de jobs ativos e a utilização do cluster.",
        "solution": ["databricks", "log-analytics", "monitor"],
        "availableServices": ["databricks", "log-analytics", "monitor", "power-bi"],
        "explanation": "Você pode configurar o Databricks para enviar seus logs de diagnóstico para um Log Analytics Workspace. A partir daí, você pode usar consultas KQL para extrair as métricas de interesse e criar um 'Workbook' do Azure Monitor para visualizar essas métricas em um dashboard interativo."
    }],
            GCP: [        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Crie uma máquina virtual (VM) no Compute Engine para hospedar um servidor web Apache.",
            "solution": ["compute-engine"],
            "availableServices": ["compute-engine", "gke", "cloud-run", "app-engine"],
            "explanation": "O Google Compute Engine (GCE) fornece VMs (IaaS), oferecendo controle total sobre o sistema operacional e a configuração do servidor, sendo a escolha direta para instalar softwares como o Apache."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Armazene arquivos de upload de usuários, como imagens e documentos, em um serviço de armazenamento de objetos escalável.",
            "solution": ["cloud-storage"],
            "availableServices": ["cloud-storage", "filestore", "persistent-disk", "cloud-sql"],
            "explanation": "O Google Cloud Storage é um serviço de armazenamento de objetos durável e altamente disponível, ideal para armazenar arquivos não estruturados como imagens e documentos."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Crie um banco de dados MySQL gerenciado para uma aplicação web WordPress.",
            "solution": ["cloud-sql"],
            "availableServices": ["cloud-sql", "firestore", "spanner", "bigtable"],
            "explanation": "O Cloud SQL é um serviço de banco de dados relacional totalmente gerenciado que simplifica a configuração, manutenção e administração de bancos de dados MySQL, PostgreSQL e SQL Server."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Crie uma função serverless que é acionada por uma requisição HTTP e retorna 'Hello World'.",
            "solution": ["cloud-functions"],
            "availableServices": ["cloud-functions", "cloud-run", "app-engine", "gke"],
            "explanation": "O Cloud Functions é uma plataforma de computação serverless orientada a eventos. Ele permite executar código em resposta a gatilhos, como uma requisição HTTP, sem gerenciar servidores."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Armazene uma chave de API de forma segura para que suas aplicações possam acessá-la sem expô-la no código.",
            "solution": ["secret-manager"],
            "availableServices": ["secret-manager", "kms", "cloud-storage", "iam"],
            "explanation": "O Secret Manager é um serviço projetado especificamente para armazenar segredos como chaves de API, senhas e certificados. Ele fornece controle de acesso granular via IAM e versionamento de segredos."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Exponha uma Cloud Function como uma API HTTP gerenciada, com controle de acesso e monitoramento.",
            "solution": ["cloud-functions", "api-gateway-gcp"],
            "availableServices": ["cloud-functions", "api-gateway-gcp", "cloud-run", "firestore", "pub-sub", "workflows"],
            "explanation": "A Cloud Function contém a lógica de backend. O API Gateway atua como a fachada, fornecendo recursos como autenticação (chaves de API, JWT), monitoramento e controle de taxa para a função."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Crie um tópico em um serviço de mensageria para publicar eventos de 'novo pedido criado'.",
            "solution": ["pub-sub"],
            "availableServices": ["pub-sub", "workflows", "tasks", "eventarc"],
            "explanation": "O Cloud Pub/Sub é um serviço de mensagens assíncrono e escalável. A criação de um 'tópico' permite que publicadores enviem mensagens sem saber quem são os consumidores (assinantes)."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Crie um alerta de faturamento para ser notificado por e-mail quando os custos do projeto ultrapassarem $500.",
            "solution": ["billing"],
            "availableServices": ["billing", "monitoring", "logging", "iam"],
            "explanation": "Na seção de Faturamento (Billing) do console do GCP, você pode criar 'orçamentos' (budgets) para seus projetos e configurar 'regras de alerta' que enviam notificações quando o custo atinge uma porcentagem do orçamento."
        },
        {
            "category": "cloud-operations",
            "difficulty": 1,
            "question": "Crie um repositório de código-fonte privado no GCP para armazenar o código de sua aplicação.",
            "solution": ["cloud-source-repositories"],
            "availableServices": ["cloud-source-repositories", "artifact-registry", "cloud-storage", "cloud-build"],
            "explanation": "O Cloud Source Repositories oferece repositórios Git privados e totalmente gerenciados, integrados a outros serviços do GCP como Cloud Build e Cloud Logging."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Crie um microsserviço em um contêiner que é acionado por uma mensagem no Pub/Sub e salva o resultado no Firestore.",
            "solution": ["pub-sub", "cloud-run", "firestore"],
            "availableServices": ["pub-sub", "cloud-run", "firestore", "cloud-functions", "cloud-sql", "api-gateway-gcp"],
            "explanation": "Uma 'assinatura push' do Pub/Sub pode ser configurada para invocar um endpoint HTTP de um serviço do Cloud Run sempre que uma nova mensagem chega. O serviço do Cloud Run processa a mensagem e a grava no Firestore."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Crie uma Cloud Function que é acionada sempre que um novo arquivo é adicionado a um bucket do Cloud Storage.",
            "solution": ["cloud-storage", "cloud-functions"],
            "availableServices": ["cloud-storage", "cloud-functions", "eventarc", "cloud-run"],
            "explanation": "O Cloud Functions possui um gatilho nativo para o Cloud Storage. A função é implantada para escutar eventos de criação de objetos em um bucket específico, executando o código automaticamente."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Hospede um site estático (HTML, CSS, JS) em um bucket do Cloud Storage e o sirva através de um balanceador de carga HTTP.",
            "solution": ["cloud-storage", "load-balancing"],
            "availableServices": ["cloud-storage", "load-balancing", "cdn", "compute-engine"],
            "explanation": "O bucket do Cloud Storage atua como o 'backend' para o Cloud Load Balancing. Isso permite que você use recursos avançados como um endereço IP global, certificados SSL gerenciados pelo Google e CDN."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Configure um pipeline de build simples que compila um aplicativo Go sempre que o código é enviado para o Cloud Source Repositories.",
            "solution": ["cloud-source-repositories", "cloud-build"],
            "availableServices": ["cloud-source-repositories", "cloud-build", "artifact-registry", "gke"],
            "explanation": "O Cloud Build pode ser configurado com 'gatilhos' (triggers) que monitoram um branch no Cloud Source Repositories. Um arquivo `cloudbuild.yaml` no repositório define as etapas para compilar o aplicativo Go."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Monitore o uso de CPU de uma máquina virtual e envie uma notificação para um canal do Slack se ele permanecer acima de 80% por 5 minutos.",
            "solution": ["compute-engine", "monitoring"],
            "availableServices": ["compute-engine", "monitoring", "logging", "profiler"],
            "explanation": "O Cloud Monitoring coleta métricas do Compute Engine. Você cria uma 'política de alerta' que monitora a utilização da CPU. A política é configurada para acionar uma notificação para um 'canal de notificação' (que pode ser configurado para o Slack) se a condição for atendida."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Configure uma regra de firewall em uma VPC para permitir tráfego HTTPS (porta 443) para VMs com uma tag específica.",
            "solution": ["vpc", "compute-engine"],
            "availableServices": ["vpc", "compute-engine", "cloud-armor", "load-balancing"],
            "explanation": "As regras de firewall da VPC do GCP podem usar 'tags de rede'. Você cria uma regra para permitir tráfego de entrada na porta TCP 443 e a aplica apenas a VMs que possuem a tag especificada (ex: 'web-server')."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Crie um registro privado para armazenar as imagens de contêiner da sua organização.",
            "solution": ["artifact-registry"],
            "availableServices": ["artifact-registry", "cloud-storage", "gke", "cloud-build"],
            "explanation": "O Artifact Registry é o serviço recomendado para armazenar artefatos de build, incluindo imagens Docker. Ele oferece repositórios privados, verificação de vulnerabilidades e integração com o ecossistema do GCP."
        },
        {
            "category": "cloud-operations",
            "difficulty": 2,
            "question": "Conceda a um desenvolvedor permissão para visualizar, mas não alterar, os recursos de um projeto.",
            "solution": ["iam"],
            "availableServices": ["iam", "secret-manager", "resource-manager", "billing"],
            "explanation": "No Cloud IAM (Identity and Access Management), você pode atribuir o papel (role) de 'Visualizador' (Viewer) ao desenvolvedor no nível do projeto. Este papel concede permissões de leitura para a maioria dos serviços do GCP."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Crie um pipeline de CI/CD que, a cada commit, constrói uma imagem de contêiner, a armazena no Artifact Registry e a implanta em um cluster GKE.",
            "solution": ["cloud-build", "artifact-registry", "gke"],
            "availableServices": ["cloud-build", "artifact-registry", "gke", "cloud-storage", "cloud-run", "monitoring"],
            "explanation": "O Cloud Build executa as etapas definidas em um arquivo `cloudbuild.yaml`: constrói a imagem Docker, a envia para o Artifact Registry e, por fim, usa o `kubectl` para aplicar os manifestos de implantação ao cluster GKE."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Implante uma aplicação web Python de forma escalável e totalmente gerenciada, sem se preocupar com a infraestrutura subjacente.",
            "solution": ["app-engine"],
            "availableServices": ["app-engine", "gke", "cloud-run", "compute-engine"],
            "explanation": "O App Engine é uma plataforma PaaS que gerencia automaticamente a infraestrutura, o escalonamento, o versionamento e o roteamento de tráfego para aplicações web, sendo ideal para um deploy rápido e escalável."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Centralize os logs de múltiplas VMs e serviços do GCP em um único local para análise e crie uma métrica a partir de uma consulta de log.",
            "solution": ["monitoring", "logging"],
            "availableServices": ["monitoring", "logging", "trace", "profiler"],
            "explanation": "O Cloud Logging coleta logs de vários serviços. Você pode criar um 'filtro de log' para isolar eventos específicos (ex: erros 500) e, a partir desse filtro, criar uma 'métrica baseada em log' no Cloud Monitoring para visualizar e alertar sobre a frequência desses eventos."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Provisione uma infraestrutura de rede e computação (VPC, Subnets, VMs) de forma declarativa e reproduzível.",
            "solution": ["deployment-manager"],
            "availableServices": ["deployment-manager", "terraform", "cloud-build", "gcloud-cli"],
            "explanation": "O Cloud Deployment Manager (ou Terraform) permite definir a infraestrutura como código (IaC). Você cria um arquivo de configuração (YAML para Deployment Manager) descrevendo os recursos, e a ferramenta provisiona tudo de forma automatizada."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Configure o autoescalonamento para um grupo de instâncias de VMs, adicionando novas máquinas quando a utilização média da CPU ultrapassar 70%.",
            "solution": ["compute-engine", "load-balancing"],
            "availableServices": ["compute-engine", "load-balancing", "monitoring", "gke"],
            "explanation": "Um 'Managed Instance Group' (MIG) no Compute Engine pode ser configurado com uma política de autoescalonamento baseada em métricas como a utilização da CPU. O Load Balancer distribui o tráfego entre as instâncias do grupo."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Crie um serviço no Cloud Run que busca dados de uma instância do Cloud SQL e os retorna como JSON, garantindo uma conexão segura entre eles.",
            "solution": ["cloud-run", "cloud-sql"],
            "availableServices": ["cloud-run", "cloud-sql", "firestore", "secret-manager"],
            "explanation": "A integração nativa entre Cloud Run e Cloud SQL permite uma conexão segura e autenticada via um socket Unix, sem a necessidade de expor o banco de dados à internet ou gerenciar IPs. As credenciais são gerenciadas via contas de serviço."
        },
        {
            "category": "cloud-operations",
            "difficulty": 3,
            "question": "Implemente um cache em memória (Redis) para acelerar o desempenho de uma aplicação web que faz muitas leituras no banco de dados.",
            "solution": ["gke", "memorystore"],
            "availableServices": ["gke", "memorystore", "cloud-sql", "firestore"],
            "explanation": "O Memorystore for Redis fornece uma instância Redis totalmente gerenciada. A aplicação rodando no GKE pode se conectar a essa instância para armazenar dados em cache, reduzindo a latência e a carga sobre o banco de dados principal."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Orquestre uma série de chamadas a diferentes microsserviços (Cloud Functions e Cloud Run) para completar um processo de negócio complexo, como o cadastro de um novo cliente.",
            "solution": ["workflows", "cloud-functions", "cloud-run"],
            "availableServices": ["workflows", "cloud-functions", "cloud-run", "pub-sub", "apigee", "firestore"],
            "explanation": "O Cloud Workflows permite definir fluxos de trabalho em YAML que orquestram chamadas a serviços HTTP. Você pode definir uma sequência de etapas que invocam diferentes Cloud Functions e serviços Cloud Run, tratando de retentativas e erros."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Proteja o acesso a uma aplicação web interna (rodando em uma VM) para que apenas funcionários autenticados da sua organização possam acessá-la, sem a necessidade de uma VPN.",
            "solution": ["compute-engine", "iap"],
            "availableServices": ["compute-engine", "iap", "vpc", "cloud-armor", "vpn"],
            "explanation": "O Identity-Aware Proxy (IAP) atua como um proxy de autenticação. Ele intercepta as requisições para a VM, verifica a identidade do usuário no Google e as permissões do IAM antes de permitir o acesso, implementando o acesso de confiança zero."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Crie um pipeline de entrega contínua que implanta uma nova versão de um serviço no GKE usando a estratégia Canary, liberando-a para 10% do tráfego inicialmente.",
            "solution": ["cloud-build", "gke", "anthos-service-mesh"],
            "availableServices": ["cloud-build", "gke", "anthos-service-mesh", "load-balancing", "monitoring"],
            "explanation": "O Cloud Build implanta a nova versão no GKE. Usando um service mesh como o Anthos Service Mesh (Istio), você pode configurar o roteamento de tráfego para dividir o tráfego entre a versão antiga e a nova (ex: 90% para a estável, 10% para a canary)."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Configure uma conexão de rede privada e dedicada entre seu data center on-premises e a sua VPC no GCP.",
            "solution": ["vpc", "interconnect"],
            "availableServices": ["vpc", "interconnect", "vpn", "cloud-router"],
            "explanation": "O Cloud Interconnect fornece uma conexão física direta e de alta largura de banda entre sua rede on-premises e a Virtual Private Cloud (VPC) do Google, oferecendo menor latência e maior confiabilidade do que uma conexão VPN baseada na internet."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Diagnostique problemas de latência em uma aplicação de microsserviços, rastreando uma única requisição através de várias chamadas de serviço.",
            "solution": ["gke", "trace"],
            "availableServices": ["gke", "trace", "monitoring", "logging", "profiler"],
            "explanation": "O Cloud Trace captura dados de latência de aplicações, como as que rodam no GKE. Ele gera visualizações em cascata que mostram como uma requisição se propaga por múltiplos serviços, permitindo identificar rapidamente qual chamada está causando o gargalo."
        },
        {
            "category": "cloud-operations",
            "difficulty": 4,
            "question": "Crie um gatilho universal que executa um serviço do Cloud Run sempre que um evento de auditoria específico (ex: criação de VM) é gerado, independentemente do serviço de origem.",
            "solution": ["auditing", "eventarc", "cloud-run"],
            "availableServices": ["auditing", "eventarc", "cloud-run", "pub-sub", "cloud-functions"],
            "explanation": "O Cloud Audit Logs gera entradas para eventos de sistema. O Eventarc pode usar esses logs como fonte para criar gatilhos que reagem a eventos específicos. Você pode configurar um gatilho do Eventarc para invocar um serviço do Cloud Run sempre que o evento de auditoria desejado ocorrer."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Projete uma aplicação web global, segura e de alta performance, utilizando balanceador de carga, CDN e proteção contra ataques web.",
            "solution": ["load-balancing", "cdn", "cloud-armor", "gke", "cloud-sql"],
            "availableServices": ["load-balancing", "cdn", "cloud-armor", "gke", "cloud-sql", "cloud-run", "memorystore"],
            "explanation": "O Cloud Load Balancing distribui o tráfego globalmente. O Cloud CDN armazena conteúdo em cache perto dos usuários para baixa latência. O Cloud Armor (WAF) protege contra ataques DDoS e de aplicação. O GKE hospeda a aplicação em contêineres e o Cloud SQL fornece o banco de dados relacional."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Crie uma arquitetura de recuperação de desastres para uma aplicação crítica, com failover automático entre duas regiões do GCP, usando um banco de dados globalmente consistente.",
            "solution": ["load-balancing", "gke", "spanner"],
            "availableServices": ["load-balancing", "gke", "spanner", "cloud-sql", "dns"],
            "explanation": "O Cloud Load Balancing pode detectar falhas em uma região e redirecionar o tráfego para a outra. O GKE executa a aplicação em ambas as regiões. O Cloud Spanner é um banco de dados relacional globalmente distribuído que oferece consistência forte, garantindo que os dados estejam sincronizados entre as regiões."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Projete e implemente uma 'landing zone' no GCP para uma grande empresa, usando uma hierarquia de organização, pastas e projetos com políticas de IAM e de rede centralizadas.",
            "solution": ["resource-manager", "iam", "vpc"],
            "availableServices": ["resource-manager", "iam", "vpc", "billing", "deployment-manager"],
            "explanation": "O Resource Manager permite criar a estrutura de Organização, Pastas e Projetos. O IAM é usado para aplicar políticas de permissão de forma hierárquica (herdada). Uma VPC Compartilhada (Shared VPC) permite centralizar o gerenciamento da rede, garantindo consistência e segurança."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Implemente uma estratégia de SRE (Site Reliability Engineering) para um serviço crítico, definindo SLOs (Service Level Objectives), monitorando SLIs (Service Level Indicators) e gerenciando o 'error budget'.",
            "solution": ["gke", "monitoring"],
            "availableServices": ["gke", "monitoring", "logging", "cloud-build"],
            "explanation": "O Cloud Monitoring é a ferramenta central para SRE no GCP. Você pode definir SLIs (como latência ou taxa de erro) para um serviço no GKE, estabelecer SLOs (metas de performance) e o Monitoring calculará automaticamente o 'error budget' restante, permitindo a criação de alertas quando ele estiver sendo consumido rapidamente."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Construa uma plataforma de operações de nuvem (FinOps) para otimizar e alocar custos do GCP em diferentes centros de custo, usando relatórios detalhados de faturamento e recomendações de otimização.",
            "solution": ["billing", "monitoring"],
            "availableServices": ["billing", "monitoring", "resource-manager", "iam"],
            "explanation": "O Cloud Billing permite exportar dados detalhados de uso e custo para o BigQuery para análise aprofundada. O Cloud Monitoring fornece dashboards de custos. Juntos, eles permitem visualizar, analisar e criar alertas, enquanto o Active Assist (parte do Monitoring) gera recomendações para otimização de custos."
        },
        {
            "category": "cloud-operations",
            "difficulty": 5,
            "question": "Gerencie clusters Kubernetes on-premises e em outras nuvens como se fossem clusters GKE nativos, usando um painel de controle unificado.",
            "solution": ["anthos", "gke"],
            "availableServices": ["anthos", "gke", "interconnect", "vpn", "monitoring"],
            "explanation": "O Anthos é a plataforma do Google para modernização de aplicações que estende os serviços do GCP, incluindo o GKE, para ambientes on-premises e multi-cloud. Ele permite anexar clusters Kubernetes externos e gerenciá-los de forma consistente a partir do console do Google Cloud."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Carregue um arquivo CSV do Cloud Storage para uma nova tabela no BigQuery usando a interface do console.",
            "solution": ["cloud-storage", "bigquery"],
            "availableServices": ["cloud-storage", "bigquery", "dataflow", "cloud-sql", "pub-sub"],
            "explanation": "O BigQuery se integra nativamente com o Cloud Storage. Você pode criar uma tabela no BigQuery e definir um arquivo no Cloud Storage como sua fonte de dados externa, ou executar um job de carga para importar os dados diretamente."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Crie um pipeline de ETL gráfico que move dados de um banco Cloud SQL para o BigQuery.",
            "solution": ["cloud-sql", "data-fusion", "bigquery"],
            "availableServices": ["cloud-sql", "data-fusion", "bigquery", "dataflow", "dataproc", "composer"],
            "explanation": "O Cloud Data Fusion oferece uma interface gráfica (sem código) para construir pipelines de ETL. Ele possui conectores nativos para fontes como o Cloud SQL e destinos como o BigQuery, facilitando a extração, transformação e carga de dados entre eles."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Execute uma consulta SQL no BigQuery para contar o número de registros em uma tabela pública.",
            "solution": ["bigquery"],
            "availableServices": ["bigquery", "cloud-sql", "spanner", "dataflow"],
            "explanation": "O BigQuery é o data warehouse analítico do GCP, projetado para executar consultas SQL sobre grandes volumes de dados de forma extremamente rápida. Ele possui conjuntos de dados públicos disponíveis para consulta direta."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Crie um pipeline simples usando o Data Transfer Service para copiar dados do Google Ads para o BigQuery diariamente.",
            "solution": ["bigquery-dts"],
            "availableServices": ["bigquery-dts", "data-factory", "data-fusion", "composer"],
            "explanation": "O BigQuery Data Transfer Service (DTS) automatiza a movimentação de dados de fontes de SaaS do Google (como Google Ads, YouTube) para o BigQuery em uma programação gerenciada, sem a necessidade de construir um pipeline."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Crie um cluster Dataproc com 1 nó mestre e 2 nós de trabalho para processamento ad-hoc.",
            "solution": ["dataproc"],
            "availableServices": ["dataproc", "dataflow", "gke", "composer"],
            "explanation": "O Dataproc é o serviço gerenciado de Hadoop e Spark do GCP. Ele permite provisionar rapidamente clusters com configurações específicas (número de mestres, workers) para executar jobs de processamento de big data."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Limpe e prepare um arquivo CSV de forma interativa usando uma interface visual, antes de carregá-lo no BigQuery.",
            "solution": ["dataprep", "bigquery"],
            "availableServices": ["dataprep", "bigquery", "data-fusion", "dataflow", "cloud-functions"],
            "explanation": "O Dataprep by Trifacta é uma ferramenta visual para exploração e preparação de dados. Ele sugere transformações e permite que o usuário limpe e estruture os dados de forma interativa, gerando um job do Dataflow por trás para aplicar as transformações em escala."
        },
        {
            "category": "data-engineering",
            "difficulty": 1,
            "question": "Publique uma mensagem JSON em um tópico do Pub/Sub usando a ferramenta de linha de comando gcloud.",
            "solution": ["pub-sub"],
            "availableServices": ["pub-sub", "bigquery", "cloud-storage", "tasks"],
            "explanation": "O Pub/Sub é o serviço de mensageria global e assíncrono do GCP. Ele permite que produtores enviem mensagens para um 'tópico' sem saber quem são os consumidores, facilitando a criação de sistemas desacoplados."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Orquestre um workflow diário com Airflow que executa um job do Dataflow para processar arquivos no Cloud Storage e carregar no BigQuery.",
            "solution": ["cloud-storage", "composer", "dataflow", "bigquery"],
            "availableServices": ["cloud-storage", "composer", "dataflow", "bigquery", "dataproc", "cloud-functions"],
            "explanation": "O Cloud Composer é o serviço gerenciado de Apache Airflow, ideal para orquestração de workflows. Um DAG no Composer pode definir uma sequência de tarefas: esperar por um arquivo no Cloud Storage, iniciar um job do Dataflow para processá-lo e, por fim, carregar o resultado no BigQuery."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Crie uma Cloud Function que é acionada por um novo arquivo em um bucket do Cloud Storage e insere seus dados em uma tabela do BigQuery.",
            "solution": ["cloud-storage", "cloud-functions", "bigquery"],
            "availableServices": ["cloud-storage", "cloud-functions", "bigquery", "dataflow", "composer"],
            "explanation": "Essa é uma arquitetura clássica de ETL serverless. O Cloud Storage emite um evento quando um novo objeto é criado. A Cloud Function é configurada para ser acionada por esse evento, lendo o arquivo e usando a API do BigQuery para inserir os dados na tabela de destino."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Desenvolva um job do Dataflow que lê dados de um tópico do Pub/Sub e os escreve em arquivos Parquet no Cloud Storage.",
            "solution": ["pub-sub", "dataflow", "cloud-storage"],
            "availableServices": ["pub-sub", "dataflow", "cloud-storage", "dataproc", "bigquery"],
            "explanation": "O Dataflow (Apache Beam) é ideal para processamento de streaming. Ele pode usar o Pub/Sub como uma fonte de dados contínua, aplicar transformações (se necessário) e usar o Cloud Storage como um 'sink' (destino) para armazenar os resultados em um formato colunar como Parquet, criando um data lake."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Configure um pipeline de streaming nativo que ingere mensagens do Pub/Sub diretamente em uma tabela do BigQuery sem código.",
            "solution": ["pub-sub", "bigquery"],
            "availableServices": ["pub-sub", "bigquery", "dataflow", "cloud-functions"],
            "explanation": "O BigQuery oferece uma funcionalidade de 'streaming inserts' e uma integração direta com o Pub/Sub. Você pode criar uma 'assinatura BigQuery' em um tópico do Pub/Sub que envia as mensagens diretamente para uma tabela, sem a necessidade de um serviço intermediário como Dataflow ou Functions."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Use o Data Fusion para extrair dados de um banco de dados Oracle on-premises e carregá-los no BigQuery.",
            "solution": ["data-fusion", "bigquery"],
            "availableServices": ["data-fusion", "bigquery", "dataflow", "composer", "cloud-sql"],
            "explanation": "O Data Fusion se destaca na conectividade com diversas fontes, incluindo bancos de dados on-premises como o Oracle. Ele usa um agente que pode ser instalado localmente para se conectar de forma segura à fonte e transferir os dados para a nuvem, onde o pipeline os processa e carrega no BigQuery."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Crie um job no Dataproc que executa um script PySpark para ler dados do Cloud Storage, contar as palavras e salvar o resultado de volta no Storage.",
            "solution": ["cloud-storage", "dataproc"],
            "availableServices": ["cloud-storage", "dataproc", "dataflow", "bigquery"],
            "explanation": "O Dataproc é o serviço ideal para executar jobs Spark existentes. Ele se integra nativamente ao Cloud Storage (usando o conector GCS), permitindo que um script PySpark leia arquivos de um bucket, processe os dados em memória no cluster e grave os resultados em outro bucket."
        },
        {
            "category": "data-engineering",
            "difficulty": 2,
            "question": "Crie uma tabela particionada por data no BigQuery para otimizar consultas baseadas em períodos de tempo.",
            "solution": ["bigquery"],
            "availableServices": ["bigquery", "cloud-sql", "spanner", "bigtable"],
            "explanation": "A partição de tabelas no BigQuery divide uma tabela grande em segmentos menores com base em uma coluna de data/timestamp. Ao consultar um intervalo de tempo específico, o BigQuery só escaneia as partições relevantes, reduzindo drasticamente o custo e o tempo da consulta."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Projete um pipeline de Big Data que ingere dados de streaming com Pub/Sub, processa com Spark no Dataproc e armazena os resultados no BigQuery.",
            "solution": ["pub-sub", "cloud-storage", "dataproc", "bigquery"],
            "availableServices": ["pub-sub", "cloud-storage", "dataproc", "bigquery", "dataflow", "composer", "data-fusion"],
            "explanation": "Para processamento de streaming com Spark, o Dataproc é a escolha. O pipeline usa Pub/Sub para ingestão, o Spark Streaming em um cluster Dataproc para processar os micro-batches de dados, e o conector Spark-BigQuery para carregar os resultados processados no data warehouse."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Crie um modelo de regressão linear usando apenas SQL no BigQuery para prever o preço de casas com base em seus atributos.",
            "solution": ["bigquery-ml"],
            "availableServices": ["bigquery-ml", "vertex-ai", "dataflow", "dataproc"],
            "explanation": "O BigQuery ML (BQML) permite que analistas de dados criem e executem modelos de machine learning diretamente no BigQuery usando sintaxe SQL. A instrução `CREATE MODEL` pode ser usada para treinar um modelo de regressão sobre uma tabela de dados, sem a necessidade de mover os dados para outra plataforma."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Implemente uma pipeline de streaming no Dataflow que calcula a média de valores de sensores em janelas de tempo de 5 minutos.",
            "solution": ["pub-sub", "dataflow"],
            "availableServices": ["pub-sub", "dataflow", "dataproc", "bigquery", "composer"],
            "explanation": "O Dataflow (Apache Beam) tem funcionalidades robustas para processamento de streaming, incluindo operações de 'windowing'. Ele pode agrupar eventos de um stream do Pub/Sub em janelas (fixas, deslizantes, de sessão) e aplicar agregações, como a média, sobre os eventos de cada janela."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Configure um DAG no Cloud Composer que tem múltiplas tarefas com dependências, como 'ingestão -> transformação -> validação -> carga'.",
            "solution": ["composer", "bigquery"],
            "availableServices": ["composer", "bigquery", "dataflow", "workflows", "cloud-functions"],
            "explanation": "O Cloud Composer (Airflow) é projetado para gerenciar workflows como código (DAGs). Você pode definir tarefas (operadores) e usar a sintaxe `>>` ou `set_downstream` para estabelecer dependências, garantindo que a transformação só ocorra após a ingestão, e a carga só após a validação."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Use o Data Fusion para replicar continuamente alterações de um banco de dados MySQL (Change Data Capture - CDC) para o BigQuery.",
            "solution": ["cloud-sql", "data-fusion", "bigquery"],
            "availableServices": ["cloud-sql", "data-fusion", "bigquery", "datastream", "dataflow"],
            "explanation": "O Data Fusion suporta a captura de dados de alteração (CDC) de bancos de dados como o MySQL. Ele pode ler os logs binários do banco de origem (Cloud SQL) para capturar inserções, atualizações e exclusões em tempo real e replicar essas alterações em uma tabela do BigQuery."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Armazene e analise dados de séries temporais de alta vazão, como métricas de mercado financeiro ou de IoT.",
            "solution": ["pub-sub", "bigtable"],
            "availableServices": ["pub-sub", "bigtable", "bigquery", "spanner", "cloud-sql"],
            "explanation": "O Cloud Bigtable é um banco de dados NoSQL de colunas largas, otimizado para alta taxa de ingestão e baixa latência de leitura, ideal para cargas de trabalho de séries temporais. A ingestão pode ser feita via Pub/Sub para desacoplamento e escalabilidade."
        },
        {
            "category": "data-engineering",
            "difficulty": 3,
            "question": "Crie uma view materializada no BigQuery para acelerar um dashboard que executa consultas agregadas complexas frequentemente.",
            "solution": ["bigquery"],
            "availableServices": ["bigquery", "cloud-sql", "looker", "dataflow"],
            "explanation": "Uma view materializada no BigQuery pré-calcula e armazena o resultado de uma consulta. Quando uma consulta no dashboard corresponde à definição da view, o BigQuery pode ler os resultados pré-calculados em vez de processar a tabela base inteira, resultando em performance muito superior."
        },
        {
            "category": "data-engineering",
            "difficulty": 4,
            "question": "Crie um pipeline de ML que treina um modelo no Vertex AI usando dados do BigQuery e o implanta em um endpoint para previsões em tempo real.",
            "solution": ["bigquery", "vertex-ai", "cloud-functions"],
            "availableServices": ["bigquery", "vertex-ai", "cloud-functions", "cloud-storage", "dataflow", "api-gateway-gcp"],
            "explanation": "O Vertex AI é a plataforma unificada de ML do GCP. Ele pode usar dados diretamente do BigQuery para treinar um modelo customizado. Após o treinamento, o modelo pode ser implantado em um endpoint, que pode ser invocado, por exemplo, por uma Cloud Function para obter previsões."
        },
        {
            "category": "data-engineering",
            "difficulty": 4,
            "question": "Construa um data lake governado, usando o Dataplex para gerenciar a segurança, metadados e qualidade dos dados em múltiplos buckets do Cloud Storage.",
            "solution": ["cloud-storage", "dataplex", "bigquery"],
            "availableServices": ["cloud-storage", "dataplex", "bigquery", "data-catalog", "composer"],
            "explanation": "O Dataplex é um serviço de malha de dados inteligente que permite descobrir, gerenciar, monitorar e governar dados em escala. Ele organiza dados do Cloud Storage e BigQuery em 'lagos' e 'zonas', aplicando políticas de segurança e qualidade de forma centralizada."
        },
        {
            "category": "data-engineering",
            "difficulty": 4,
            "question": "Desenvolva um pipeline de CI/CD para um job do Dataflow, automatizando testes e a implantação de templates usando Cloud Build.",
            "solution": ["dataflow", "cloud-storage", "cloud-build"],
            "availableServices": ["dataflow", "cloud-storage", "cloud-build", "composer", "gke"],
            "explanation": "Para CI/CD com Dataflow, o Cloud Build automatiza o processo. Ele pode executar testes unitários no código do pipeline, compilar o código e, em seguida, executar o comando para criar um 'Flex Template' do Dataflow, armazenando a imagem do contêiner e a especificação do template no Cloud Storage."
        },
        {
            "category": "data-engineering",
            "difficulty": 4,
            "question": "Implemente uma 'feature store' para modelos de machine learning, usando o Dataproc para calcular features em lote e o Bigtable para servi-las com baixa latência.",
            "solution": ["dataproc", "bigtable", "vertex-ai"],
            "availableServices": ["dataproc", "bigtable", "vertex-ai-feature-store", "bigquery", "dataflow"],
            "explanation": "Uma feature store requer processamento em lote e serviço de baixa latência. O Dataproc (Spark) é ideal para calcular features em grande escala a partir de dados brutos. O Bigtable é perfeito para armazenar essas features e servi-las rapidamente para modelos de ML em produção (gerenciados pelo Vertex AI)."
        },
        {
            "category": "data-engineering",
            "difficulty": 4,
            "question": "Projete uma arquitetura de streaming que enriquece eventos do Pub/Sub em tempo real com dados de uma tabela do Bigtable, usando Dataflow.",
            "solution": ["pub-sub", "dataflow", "bigtable"],
            "availableServices": ["pub-sub", "dataflow", "bigtable", "bigquery", "memorystore"],
            "explanation": "O Dataflow pode ler um stream principal do Pub/Sub. Para cada evento, ele pode fazer uma busca (lookup) de baixa latência em uma tabela do Bigtable para obter dados adicionais (enriquecimento). O evento enriquecido pode então ser enviado para um destino final."
        },
        {
            "category": "data-engineering",
            "difficulty": 4,
            "question": "Crie um perímetro de segurança usando VPC Service Controls para impedir que dados sensíveis do BigQuery e Cloud Storage sejam exfiltrados.",
            "solution": ["bigquery", "cloud-storage", "vpc-service-controls"],
            "availableServices": ["bigquery", "cloud-storage", "vpc-service-controls", "iam", "cloud-armor"],
            "explanation": "O VPC Service Controls cria um 'perímetro' virtual em torno de projetos e serviços do GCP. Ele impede que dados de serviços protegidos (como BigQuery e Cloud Storage) sejam acessados ou copiados para fora do perímetro, protegendo contra exfiltração de dados."
        },
        {
            "category": "data-engineering",
            "difficulty": 5,
            "question": "Desenvolva uma arquitetura de banco de dados para uma aplicação global de alta transação que requer consistência forte.",
            "solution": ["load-balancing", "cloud-run", "spanner"],
            "availableServices": ["load-balancing", "cloud-run", "spanner", "cloud-sql", "bigtable", "firestore"],
            "explanation": "O Cloud Spanner é o único banco de dados relacional, globalmente distribuído e fortemente consistente. Ele é projetado para escalar horizontalmente entre regiões, sendo a escolha ideal para aplicações globais que rodam em Cloud Run e precisam de garantias transacionais ACID."
        },
        {
            "category": "data-engineering",
            "difficulty": 5,
            "question": "Projete uma arquitetura Kappa no GCP, usando uma única tecnologia (Dataflow) para processar dados tanto em lote quanto em tempo real a partir de uma fonte unificada (Pub/Sub).",
            "solution": ["pub-sub", "dataflow", "bigquery"],
            "availableServices": ["pub-sub", "dataflow", "bigquery", "dataproc", "composer"],
            "explanation": "A arquitetura Kappa simplifica a arquitetura Lambda usando um único pipeline de processamento. O Dataflow (Beam) é ideal para isso, pois seu modelo de programação unificado pode processar dados de um stream do Pub/Sub em tempo real ou reprocessar dados históricos (lote) da mesma fonte com o mesmo código."
        },
        {
            "category": "data-engineering",
            "difficulty": 5,
            "question": "Crie uma arquitetura de 'Data Mesh' no GCP, onde diferentes domínios publicam seus 'produtos de dados' (tabelas do BigQuery) e os tornam detectáveis e governados através do Dataplex.",
            "solution": ["bigquery", "dataplex", "iam"],
            "availableServices": ["bigquery", "dataplex", "iam", "data-catalog", "composer", "api-gateway-gcp"],
            "explanation": "No Data Mesh, domínios são donos de seus dados. Eles podem usar o BigQuery para criar e manter seus 'produtos de dados'. O Dataplex atua como a camada de governança central, permitindo descobrir, gerenciar e aplicar políticas de segurança (IAM) a esses produtos de forma unificada, mesmo que estejam em projetos diferentes."
        },
        {
            "category": "data-engineering",
            "difficulty": 5,
            "question": "Implemente um pipeline de MLOps completo com Vertex AI Pipelines, que automatiza o pré-processamento de dados com Dataflow, o treinamento, a avaliação e o deploy condicional de um modelo.",
            "solution": ["dataflow", "bigquery", "vertex-ai-pipelines"],
            "availableServices": ["dataflow", "bigquery", "vertex-ai-pipelines", "composer", "cloud-build"],
            "explanation": "O Vertex AI Pipelines é a ferramenta de orquestração de ML do GCP. Ele permite definir um fluxo de trabalho (pipeline) como código, onde cada etapa (componente) realiza uma tarefa, como executar um job do Dataflow para preparação de dados do BigQuery, treinar um modelo e, com base na sua acurácia, decidir se ele deve ser implantado ou não."
        },
        {
            "category": "data-engineering",
            "difficulty": 5,
            "question": "Desenvolva uma solução de FinOps para uma plataforma de dados, otimizando custos de BigQuery (slots, armazenamento), Dataproc (clusters efêmeros, VMs preemptivas) e Dataflow (scaling).",
            "solution": ["bigquery", "dataproc", "dataflow", "billing"],
            "availableServices": ["bigquery", "dataproc", "dataflow", "billing", "monitoring"],
            "explanation": "FinOps envolve a otimização contínua de custos. Isso inclui usar slots reservados ou autoscaling no BigQuery, configurar clusters Dataproc para serem efêmeros e usarem VMs preemptivas (Spot VMs), e ajustar os parâmetros de autoscaling do Dataflow. Os dados do Cloud Billing exportados para o BigQuery são usados para analisar os resultados."
        },
        {
            "category": "data-engineering",
            "difficulty": 5,
            "question": "Projete uma arquitetura híbrida que ingere e processa dados on-premises, mas usa o Cloud Composer para orquestração e o BigQuery para análise centralizada, conectado via Interconnect.",
            "solution": ["interconnect", "composer", "bigquery"],
            "availableServices": ["interconnect", "composer", "bigquery", "anthos", "dataproc"],
            "explanation": "O Cloud Interconnect fornece a conexão de rede privada e de alta performance. O Cloud Composer pode orquestrar tarefas tanto na nuvem quanto on-premises (usando agentes). Os dados processados localmente podem ser transferidos para o Cloud Storage e, em seguida, carregados no BigQuery para análise unificada."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Conecte uma ferramenta de BI a um data warehouse para criar um dashboard interativo sobre dados de vendas.",
            "solution": ["bigquery", "looker"],
            "availableServices": ["bigquery", "looker", "cloud-storage", "cloud-sql", "dataflow", "pub-sub"],
            "explanation": "O Looker é a plataforma de BI do Google Cloud, projetada para se integrar perfeitamente com o BigQuery. Ele permite criar modelos de dados (LookML) sobre as tabelas do BigQuery e construir dashboards interativos para análise de negócios."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Execute uma consulta SQL no BigQuery para encontrar os 10 produtos mais vendidos no último mês.",
            "solution": ["bigquery"],
            "availableServices": ["bigquery", "cloud-sql", "spanner", "looker"],
            "explanation": "O BigQuery é um data warehouse analítico otimizado para consultas SQL agregadas em grandes volumes de dados. Uma consulta com `GROUP BY`, `SUM` e `ORDER BY` é a maneira padrão e eficiente de realizar essa análise."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Crie um relatório simples no Looker conectando a uma tabela de clientes no Cloud SQL.",
            "solution": ["cloud-sql", "looker"],
            "availableServices": ["cloud-sql", "looker", "bigquery", "cloud-storage"],
            "explanation": "O Looker possui conectores para diversos bancos de dados, incluindo o Cloud SQL (MySQL, Postgres). Ele pode se conectar diretamente à instância do Cloud SQL para executar consultas e visualizar os dados em relatórios."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Use o 'Connected Sheets' para puxar 1 milhão de linhas de uma tabela do BigQuery para o Google Sheets para análise ad-hoc.",
            "solution": ["bigquery", "google-sheets"],
            "availableServices": ["bigquery", "google-sheets", "looker", "cloud-storage"],
            "explanation": "O Connected Sheets é uma funcionalidade do Google Sheets que permite criar uma conexão direta com o BigQuery. Ele executa as consultas no BigQuery e exibe apenas os resultados na planilha, permitindo a análise de grandes volumes de dados com as ferramentas familiares do Sheets."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Crie um gráfico de barras no Looker para comparar as vendas entre diferentes regiões.",
            "solution": ["looker"],
            "availableServices": ["looker", "bigquery", "data-studio", "google-sheets"],
            "explanation": "O Looker é uma ferramenta de visualização de dados. Após conectar a uma fonte de dados, você pode selecionar as dimensões (região) e medidas (vendas) para criar facilmente diversos tipos de gráficos, como o de barras, para análise comparativa."
        },
        {
            "category": "data-analysis",
            "difficulty": 1,
            "question": "Salve os resultados de uma consulta complexa do BigQuery em uma nova tabela para facilitar o acesso futuro.",
            "solution": ["bigquery"],
            "availableServices": ["bigquery", "cloud-storage", "looker", "dataflow"],
            "explanation": "O BigQuery permite que você especifique uma tabela de destino para os resultados de uma consulta. Isso materializa os resultados, o que é útil para salvar agregações complexas e permitir que outros usuários consultem os dados já processados de forma mais rápida."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Analise dados semi-estruturados (JSON) armazenados no Cloud Storage diretamente com SQL, sem a necessidade de um ETL.",
            "solution": ["cloud-storage", "bigquery"],
            "availableServices": ["cloud-storage", "bigquery", "looker", "dataflow", "dataproc", "cloud-functions"],
            "explanation": "O BigQuery pode criar 'tabelas externas' que apontam para dados no Cloud Storage. Ele pode inferir o esquema de arquivos JSON (ou você pode defini-lo) e permitir que você execute consultas SQL diretamente nos arquivos, como se fossem uma tabela nativa."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Crie uma 'view' no BigQuery para simplificar uma consulta complexa com múltiplos JOINs, facilitando a análise no Looker.",
            "solution": ["bigquery", "looker"],
            "availableServices": ["bigquery", "looker", "cloud-sql", "dataflow"],
            "explanation": "Uma 'view' no BigQuery é uma consulta salva que pode ser consultada como uma tabela. Ao criar uma view que encapsula a lógica complexa (JOINs, filtros), você expõe uma 'tabela' simplificada para o Looker, facilitando a criação de relatórios e a manutenção da lógica de negócios."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Use as funções GEO do BigQuery para encontrar todas as lojas localizadas dentro de um raio de 10km de um determinado CEP.",
            "solution": ["bigquery"],
            "availableServices": ["bigquery", "looker", "maps-platform", "cloud-sql"],
            "explanation": "O BigQuery tem suporte nativo a tipos de dados e funções geoespaciais (GIS). Funções como `ST_GEOGPOINT` (para criar pontos) e `ST_DWITHIN` (para verificar a distância) permitem realizar análises de geolocalização complexas diretamente via SQL."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Construa um dashboard no Looker que combina dados de vendas do BigQuery com metas de vendas de uma planilha do Google Sheets.",
            "solution": ["bigquery", "google-sheets", "looker"],
            "availableServices": ["bigquery", "google-sheets", "looker", "cloud-storage", "data-fusion"],
            "explanation": "O Looker pode se conectar a múltiplas fontes de dados simultaneamente. Ele pode consultar a tabela de vendas no BigQuery e, ao mesmo tempo, conectar-se ao Google Sheets para buscar as metas, permitindo a criação de visualizações que combinam e comparam os dados de ambas as fontes."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Crie um campo calculado no Looker para derivar uma nova métrica, como 'preço médio por item', a partir de colunas existentes.",
            "solution": ["looker"],
            "availableServices": ["looker", "bigquery", "sql", "data-prep"],
            "explanation": "O Looker (através do LookML) permite definir a lógica de negócios na camada de modelagem. Você pode criar uma nova 'medida' (measure) que é calculada a partir de outras, como `sum(vendas) / count(itens)`, sem precisar alterar a tabela original no banco de dados."
        },
        {
            "category": "data-analysis",
            "difficulty": 2,
            "question": "Transcreva o áudio de arquivos de chamadas de suporte armazenados no Cloud Storage para texto, a fim de prepará-los para análise de sentimento.",
            "solution": ["cloud-storage", "speech-to-text-ai"],
            "availableServices": ["cloud-storage", "speech-to-text-ai", "natural-language-ai", "cloud-functions"],
            "explanation": "A API Speech-to-Text pode processar arquivos de áudio, incluindo aqueles armazenados no Cloud Storage. Ela reconhece a fala e a converte em texto, que é o primeiro passo necessário para qualquer análise subsequente de conteúdo, como a análise de sentimento."
        },
        {
            "category": "data-analysis",
            "difficulty": 3,
            "question": "Crie uma arquitetura para análise de imagens. Faça o upload de imagens, use um serviço de IA para extrair rótulos e salve os metadados no BigQuery.",
            "solution": ["cloud-storage", "cloud-functions", "vision-ai", "bigquery"],
            "availableServices": ["cloud-storage", "cloud-functions", "vision-ai", "bigquery", "natural-language-ai", "pub-sub"],
            "explanation": "Nesta arquitetura orientada a eventos, o upload de uma imagem no Cloud Storage aciona uma Cloud Function. A função envia a imagem para a API Vision AI, que retorna rótulos (tags) sobre o conteúdo da imagem. A função então insere esses rótulos, junto com o nome do arquivo, em uma tabela do BigQuery para análise."
        },
        {
            "category": "data-analysis",
            "difficulty": 3,
            "question": "Use o BigQuery ML para treinar um modelo de classificação que prevê a probabilidade de um cliente cancelar o serviço (churn).",
            "solution": ["bigquery-ml"],
            "availableServices": ["bigquery-ml", "vertex-ai", "looker", "dataflow"],
            "explanation": "O BigQuery ML é ideal para criar modelos preditivos diretamente sobre os dados do data warehouse. Com uma única instrução SQL `CREATE MODEL`, você pode treinar um modelo de classificação (como regressão logística ou árvore de decisão) sobre os dados históricos dos clientes para prever o churn."
        },
        {
            "category": "data-analysis",
            "difficulty": 3,
            "question": "Analise o sentimento (positivo, negativo, neutro) de milhares de comentários de produtos armazenados em uma tabela do BigQuery.",
            "solution": ["bigquery", "cloud-functions", "natural-language-ai"],
            "availableServices": ["bigquery", "cloud-functions", "natural-language-ai", "looker", "dataflow"],
            "explanation": "Para aplicar uma função de IA a dados existentes no BigQuery, uma abordagem comum é exportar os dados para o Cloud Storage, processá-los com uma Cloud Function que chama a API Natural Language para obter o sentimento de cada comentário, e carregar os resultados de volta para uma nova tabela no BigQuery."
        },
        {
            "category": "data-analysis",
            "difficulty": 3,
            "question": "Crie um modelo de dados no Looker (LookML) para definir a lógica de negócios, como dimensões, medidas e relações, para que os usuários possam explorar os dados de forma autônoma.",
            "solution": ["bigquery", "looker"],
            "availableServices": ["bigquery", "looker", "data-catalog", "google-sheets"],
            "explanation": "O LookML é o coração do Looker. Ele permite que um analista defina um modelo semântico sobre os dados brutos do BigQuery. Isso cria uma plataforma de self-service onde usuários de negócio podem arrastar e soltar dimensões e medidas pré-definidas para criar seus próprios relatórios, sem precisar escrever SQL."
        },
        {
            "category": "data-analysis",
            "difficulty": 3,
            "question": "Construa um funil de conversão no Looker para visualizar como os usuários navegam pelo seu site e onde eles abandonam o processo.",
            "solution": ["bigquery", "looker"],
            "availableServices": ["bigquery", "looker", "google-analytics", "firebase"],
            "explanation": "O Looker possui tipos de visualização específicos para análise de funil. Assumindo que os dados de eventos de cliques ou de navegação do site (muitas vezes exportados do Google Analytics) estão no BigQuery, você pode modelar as etapas do funil no Looker para visualizar as taxas de conversão e abandono em cada etapa."
        },
        {
            "category": "data-analysis",
            "difficulty": 3,
            "question": "Crie uma análise de coorte para entender a retenção de clientes ao longo do tempo, usando dados de transação no BigQuery.",
            "solution": ["bigquery", "looker"],
            "availableServices": ["bigquery", "looker", "dataflow", "cloud-sql"],
            "explanation": "A análise de coorte agrupa usuários com base em quando eles começaram (ex: mês da primeira compra). O BigQuery pode ser usado para realizar as transformações de SQL necessárias para agrupar os dados em coortes. O Looker pode então visualizar esses dados em um gráfico de coorte, mostrando a retenção percentual ao longo do tempo."
        },
        {
            "category": "data-analysis",
            "difficulty": 4,
            "question": "Construa uma plataforma de análise que combina dados em lote e streaming, permitindo consultas unificadas no BigQuery e visualização no Looker.",
            "solution": ["pub-sub", "dataflow", "bigquery", "looker"],
            "availableServices": ["pub-sub", "dataflow", "bigquery", "looker", "cloud-storage", "composer", "dataproc"],
            "explanation": "O Pub/Sub ingere os dados em tempo real. O Dataflow processa o stream e o carrega em uma tabela do BigQuery. Dados em lote podem ser carregados na mesma tabela (ou em outra). O BigQuery pode unir e consultar ambos os conjuntos de dados de forma transparente, e o Looker visualiza o resultado unificado."
        },
        {
            "category": "data-analysis",
            "difficulty": 4,
            "question": "Incorpore previsões de um modelo do BigQuery ML diretamente em um dashboard do Looker para mostrar a probabilidade de compra de cada cliente.",
            "solution": ["bigquery-ml", "looker"],
            "availableServices": ["bigquery-ml", "looker", "vertex-ai", "cloud-functions"],
            "explanation": "Após treinar um modelo com BigQuery ML, você pode usar a função `ML.PREDICT` em suas consultas SQL. No Looker, você pode criar uma tabela derivada ou uma medida que usa essa função para buscar as previsões em tempo real e exibi-las junto com os outros dados do cliente no dashboard."
        },
        {
            "category": "data-analysis",
            "difficulty": 4,
            "question": "Crie um dashboard em tempo real no Looker que exibe métricas de vendas à medida que os eventos chegam via Pub/Sub e são processados pelo Dataflow.",
            "solution": ["pub-sub", "dataflow", "bigquery", "looker"],
            "availableServices": ["pub-sub", "dataflow", "bigquery", "looker", "monitoring"],
            "explanation": "O pipeline de streaming (Pub/Sub -> Dataflow -> BigQuery) alimenta os dados continuamente. No Looker, você pode configurar os dashboards para se atualizarem em intervalos curtos (ex: a cada minuto), consultando o BigQuery para obter os dados mais recentes e proporcionando uma visão quase em tempo real."
        },
        {
            "category": "data-analysis",
            "difficulty": 4,
            "question": "Analise as relações entre entidades (pessoas, organizações, locais) em um grande volume de notícias e visualize as conexões.",
            "solution": ["bigquery", "natural-language-ai", "looker"],
            "availableServices": ["bigquery", "natural-language-ai", "looker", "vision-ai", "dataflow"],
            "explanation": "A API Natural Language pode analisar texto para extrair entidades e suas relações. Os resultados podem ser armazenados em tabelas no BigQuery. O Looker pode então usar visualizações de rede ou Sankey para mostrar como as diferentes entidades se conectam, revelando insights sobre as relações nos dados."
        },
        {
            "category": "data-analysis",
            "difficulty": 4,
            "question": "Otimize a performance de dashboards lentos no Looker, habilitando o BigQuery BI Engine para aceleração em memória.",
            "solution": ["bigquery", "bi-engine", "looker"],
            "availableServices": ["bigquery", "bi-engine", "looker", "memorystore", "dataflow"],
            "explanation": "O BI Engine é um serviço de análise em memória para o BigQuery. Quando habilitado, ele armazena em cache os dados mais utilizados por dashboards do Looker (e outras ferramentas de BI). Isso permite que as consultas sejam respondidas em segundos ou menos, melhorando drasticamente a interatividade do dashboard."
        },
        {
            "category": "data-analysis",
            "difficulty": 5,
            "question": "Implemente um sistema de governança de dados para um data lake no Cloud Storage, catalogando todos os ativos de dados e gerenciando metadados técnicos e de negócios.",
            "solution": ["cloud-storage", "bigquery", "data-catalog", "looker"],
            "availableServices": ["cloud-storage", "bigquery", "data-catalog", "looker", "dataplex", "composer"],
            "explanation": "O Data Catalog pode escanear automaticamente fontes como Cloud Storage e BigQuery para criar um catálogo de metadados pesquisável. Nele, os analistas podem adicionar tags e documentação de negócios. O Looker pode se integrar ao Data Catalog para que os usuários possam entender a origem e o significado dos dados que estão analisando."
        },
        {
            "category": "data-analysis",
            "difficulty": 5,
            "question": "Projete uma plataforma de BI self-service e governada, onde usuários de negócio podem explorar dados no Looker com segurança, baseados em um modelo LookML bem definido e com acesso controlado via IAM.",
            "solution": ["bigquery", "looker", "iam"],
            "availableServices": ["bigquery", "looker", "iam", "data-catalog", "dataplex"],
            "explanation": "Esta arquitetura usa o BigQuery como fonte de dados. O Looker fornece a camada semântica (LookML) que traduz o banco de dados em termos de negócio. O IAM do GCP é usado para controlar quem pode acessar quais projetos e conjuntos de dados do BigQuery, garantindo que os usuários do Looker só vejam os dados aos quais têm permissão."
        },
        {
            "category": "data-analysis",
            "difficulty": 5,
            "question": "Crie uma solução de análise preditiva para uma grande rede de varejo, usando Vertex AI para treinar modelos complexos de previsão de demanda e BigQuery/Looker para visualizar os resultados por loja.",
            "solution": ["bigquery", "vertex-ai", "looker"],
            "availableServices": ["bigquery", "vertex-ai", "looker", "bigquery-ml", "dataflow", "composer"],
            "explanation": "Para modelos complexos, o Vertex AI oferece mais flexibilidade que o BQML. Os dados históricos são extraídos do BigQuery para treinar um modelo de previsão no Vertex AI. As previsões geradas pelo modelo são então carregadas de volta no BigQuery, onde podem ser unidas com os dados reais e visualizadas no Looker para análise comparativa."
        },
        {
            "category": "data-analysis",
            "difficulty": 5,
            "question": "Implemente segurança em nível de linha (Row-Level Security) em uma solução de análise, garantindo que os gerentes regionais só possam ver os dados de suas respectivas regiões no BigQuery e no Looker.",
            "solution": ["bigquery", "looker"],
            "availableServices": ["bigquery", "looker", "iam", "data-catalog"],
            "explanation": "A segurança pode ser implementada em duas camadas. No BigQuery, você pode criar políticas de acesso em nível de linha que filtram os dados com base no usuário que está consultando. Alternativamente, no Looker, você pode usar 'access filters' baseados em atributos do usuário para aplicar a mesma lógica de segurança na camada de BI, garantindo que os dashboards mostrem apenas os dados permitidos."
        },
        {
            "category": "data-analysis",
            "difficulty": 5,
            "question": "Desenvolva uma arquitetura de 'Data Mesh' para análise, onde diferentes domínios publicam seus dados no BigQuery como 'produtos', que são descobertos via Data Catalog e consumidos de forma federada no Looker.",
            "solution": ["bigquery", "data-catalog", "looker"],
            "availableServices": ["bigquery", "data-catalog", "looker", "dataplex", "api-gateway-gcp"],
            "explanation": "Nesta arquitetura, cada domínio de negócio gerencia seus próprios projetos e tabelas no BigQuery (os 'produtos de dados'). O Data Catalog atua como o ponto central de descoberta. O Looker pode então se conectar a múltiplos projetos do BigQuery, permitindo que os analistas criem relatórios que unem e analisam dados de diferentes domínios de forma transparente."
        },
            {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Um novo commit é feito em um repositório. Em paralelo, inicie um build no Cloud Build, uma varredura de vulnerabilidades no Artifact Registry e envie uma notificação para um canal de chat.",
        "solution": ["cloud-source-repositories", "cloud-build", [["cloud-build-build"], ["artifact-registry-scan"], ["cloud-functions", "chat"]]],
        "availableServices": ["cloud-source-repositories", "cloud-build", "artifact-registry-scan", "cloud-functions", "chat", "gke"],
        "explanation": "Um gatilho do Cloud Build inicia um pipeline. A configuração do build (`cloudbuild.yaml`) pode definir etapas que, se não dependerem umas das outras, podem ser executadas em paralelo. A notificação pode ser uma etapa final ou um evento separado acionado pelo Pub/Sub a partir do build."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Quando um novo bucket do Cloud Storage é criado, em paralelo, aplique uma política de IAM padrão, habilite o versionamento de objetos e configure um alerta de custo para o bucket.",
        "solution": ["cloud-storage", "eventarc", [["iam", "cloud-functions"], ["cloud-storage", "cloud-functions"], ["billing", "cloud-functions"]]],
        "availableServices": ["cloud-storage", "eventarc", "iam", "cloud-functions", "billing", "monitoring"],
        "explanation": "Um gatilho do Eventarc para o evento de criação de bucket invoca três Cloud Functions independentes em paralelo. Cada função é responsável por uma tarefa de configuração via API: uma para IAM, uma para as configurações do bucket e uma para criar um alerta de orçamento filtrado."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Um alerta do Cloud Monitoring indica alta latência em um serviço do GKE. Em paralelo, inicie uma ação de escalonamento horizontal do pod, envie uma notificação para o PagerDuty e crie um snapshot de um Persistent Disk para análise forense.",
        "solution": ["monitoring", "pub-sub", [["gke"], ["cloud-functions", "pagerduty"], ["persistent-disk", "cloud-functions"]]],
        "availableServices": ["monitoring", "pub-sub", "gke", "cloud-functions", "pagerduty", "persistent-disk"],
        "explanation": "A política de alerta do Monitoring publica uma mensagem em um tópico do Pub/Sub. Este tópico tem três assinaturas paralelas: uma que aciona uma Cloud Function para escalar o GKE via API, outra para notificar o PagerDuty e uma terceira para iniciar a criação do snapshot do disco."
    },
    {
        "category": "cloud-operations",
        "difficulty": 5,
        "question": "Para uma resposta a incidentes, uma Cloud Function é acionada. Em paralelo, ela deve isolar uma VM (alterando suas regras de firewall), coletar um snapshot da memória da VM (usando uma ferramenta de terceiros) e revogar as credenciais de curta duração da conta de serviço associada.",
        "solution": ["cloud-functions", "compute-engine", [["vpc", "firewall"], ["compute-engine-script"], ["iam"]]],
        "availableServices": ["cloud-functions", "compute-engine", "vpc", "firewall", "iam", "security-command-center"],
        "explanation": "A Function orquestradora executa três ações assíncronas em paralelo: 1) aplica uma tag de 'quarentena' na VM que uma regra de firewall usa para bloquear todo o tráfego, 2) executa um script remoto para o memory dump, e 3) usa a API do IAM para revogar tokens da conta de serviço."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Um novo usuário é provisionado no Cloud Identity. Em paralelo, adicione-o a um grupo padrão no Google Groups, crie uma pasta para ele no Google Drive e envie um e-mail de boas-vindas.",
        "solution": ["cloud-identity", "eventarc", [["google-groups", "cloud-functions"], ["google-drive", "cloud-functions"], ["gmail", "cloud-functions"]]],
        "availableServices": ["cloud-identity", "eventarc", "cloud-functions", "google-groups", "google-drive", "gmail"],
        "explanation": "O Eventarc pode capturar eventos de auditoria do Cloud Identity. O gatilho invoca três Cloud Functions em paralelo, cada uma usando as APIs do Workspace para interagir com Groups, Drive e Gmail para completar o processo de onboarding do usuário."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Ao implantar uma nova versão de um serviço no Cloud Run, em paralelo, execute um teste de integração, inicie um 'canary release' de 10% e atualize a documentação da API.",
        "solution": ["cloud-build", "cloud-run", [["cloud-build-test"], ["cloud-run-canary"], ["api-gateway-gcp", "cloud-build"]]],
        "availableServices": ["cloud-build", "cloud-run", "api-gateway-gcp", "artifact-registry", "workflows"],
        "explanation": "Dentro de um pipeline do Cloud Build, após o deploy inicial, você pode ter etapas paralelas: uma executa um script de teste, outra usa o comando `gcloud run services update-traffic` para iniciar o canary, e uma terceira atualiza a especificação da OpenAPI no API Gateway."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Uma mensagem é enviada para um tópico do Pub/Sub. Ela deve ser processada por uma Cloud Function para lógica de negócio e, em paralelo, arquivada no Cloud Storage para auditoria.",
        "solution": ["pub-sub", [["cloud-functions"], ["cloud-storage"]]],
        "availableServices": ["pub-sub", "cloud-functions", "cloud-storage", "dataflow", "bigquery"],
        "explanation": "O Pub/Sub suporta múltiplas 'assinaturas' para um único tópico. Uma assinatura 'push' pode acionar a Cloud Function. Em paralelo, outra assinatura, do tipo 'Export to Cloud Storage', pode ser criada para arquivar todas as mensagens de forma nativa e confiável."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Um Workflow é acionado para orquestrar um processo. Em paralelo, ele busca dados de um cliente de uma API externa e busca o histórico de pedidos desse cliente no Firestore. Somente após ambas as buscas terminarem, ele prossegue.",
        "solution": ["workflows", [["http-request"], ["firestore-lookup"]], "next-step"],
        "availableServices": ["workflows", "firestore", "cloud-functions", "api-gateway-gcp"],
        "explanation": "O Cloud Workflows tem uma sintaxe de `parallel` que permite definir múltiplos ramos (branches) que são executados simultaneamente. O workflow aguardará a conclusão de todas as etapas paralelas (a chamada HTTP e a leitura no Firestore) antes de continuar para a próxima etapa de consolidação."
    },
    {
        "category": "cloud-operations",
        "difficulty": 5,
        "question": "Para uma implantação multirregional, um pipeline do Cloud Build implanta uma imagem de contêiner em clusters GKE em duas regiões diferentes em paralelo. Se ambas as implantações forem bem-sucedidas, o Cloud DNS é atualizado para incluir o novo endpoint.",
        "solution": ["cloud-build", [["gke-region1"], ["gke-region2"]], "cloud-dns"],
        "availableServices": ["cloud-build", "gke", "cloud-dns", "load-balancing", "artifact-registry"],
        "explanation": "O pipeline do Cloud Build pode ter duas etapas `gcloud container clusters get-credentials` e `kubectl apply` para cada região. Essas etapas podem ser executadas em paralelo. Uma etapa final, que depende do sucesso de ambas as etapas de implantação, atualiza o registro no Cloud DNS."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Ao final do dia, um Cloud Scheduler aciona um job. Em paralelo, o job desliga as VMs de desenvolvimento, cria snapshots dos discos de produção e gera um relatório de logs do dia.",
        "solution": ["scheduler", "cloud-functions", [["compute-engine"], ["persistent-disk"], ["logging"]]],
        "availableServices": ["scheduler", "cloud-functions", "compute-engine", "persistent-disk", "logging"],
        "explanation": "O Cloud Scheduler aciona uma Cloud Function 'orquestradora'. Esta função inicia três operações assíncronas em paralelo: listar e parar VMs com uma tag específica, listar e criar snapshots de discos, e executar uma consulta de log para gerar o relatório."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Um novo segredo é adicionado ao Secret Manager. Em paralelo, reinicie um serviço do Cloud Run para que ele carregue o novo segredo e atualize uma política de IAM para conceder acesso a uma nova conta de serviço.",
        "solution": ["secret-manager", "eventarc", [["cloud-run", "cloud-functions"], ["iam", "cloud-functions"]]],
        "availableServices": ["secret-manager", "eventarc", "cloud-run", "cloud-functions", "iam"],
        "explanation": "O Eventarc captura o evento de criação de segredo. Duas Cloud Functions são acionadas em paralelo: uma usa a CLI `gcloud` para forçar uma nova revisão do serviço Cloud Run, e a outra usa a API do IAM para modificar a política de acesso ao segredo."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Um evento de negócio é recebido via API Gateway. Ele é enviado para uma fila do Cloud Tasks para processamento assíncrono e, em paralelo, é registrado em um log de auditoria no BigQuery.",
        "solution": ["api-gateway-gcp", "cloud-functions", [["tasks"], ["bigquery"]]],
        "availableServices": ["api-gateway-gcp", "cloud-functions", "tasks", "bigquery", "pub-sub"],
        "explanation": "A API Gateway aciona uma Cloud Function. A função executa duas tarefas assíncronas: cria uma nova tarefa no Cloud Tasks para o processamento de longa duração e, simultaneamente, insere um registro do evento na tabela de auditoria do BigQuery usando a API de streaming."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Para conformidade, quando uma nova regra de firewall é criada, em paralelo, envie um alerta para a equipe de segurança e inicie um job do Forseti para analisar o impacto da nova regra na postura de segurança geral.",
        "solution": ["vpc", "eventarc", [["pub-sub", "cloud-functions"], ["forseti", "cloud-functions"]]],
        "availableServices": ["vpc", "eventarc", "pub-sub", "cloud-functions", "forseti", "security-command-center"],
        "explanation": "O Eventarc captura o evento de criação da regra de firewall. Em paralelo, uma Cloud Function envia uma notificação e outra usa a API do Forseti (uma ferramenta de governança de código aberto) para iniciar um scan e analisar as implicações da nova regra."
    },
    {
        "category": "cloud-operations",
        "difficulty": 5,
        "question": "Um ataque é detectado pelo Cloud Armor. Em paralelo, bloqueie o IP de origem, inicie um 'packet mirroring' para análise de tráfego forense e escale a capacidade do backend para absorver o tráfego de mitigação.",
        "solution": ["cloud-armor", "monitoring", "pub-sub", [["cloud-armor-policy"], ["packet-mirroring"], ["managed-instance-group"]]],
        "availableServices": ["cloud-armor", "monitoring", "pub-sub", "packet-mirroring", "managed-instance-group"],
        "explanation": "Um alerta do Cloud Armor/Monitoring publica no Pub/Sub. Três assinaturas independentes são acionadas: uma Function que atualiza a política do Cloud Armor para bloquear o IP, uma Function que configura o Packet Mirroring para a VM alvo, e uma que ajusta o tamanho do Managed Instance Group."
    },
    {
        "category": "cloud-operations",
        "difficulty": 3,
        "question": "Quando um novo projeto é criado no GCP, em paralelo, associe-o a uma conta de faturamento, aplique um conjunto padrão de APIs (ex: Compute, Storage) e configure um contato de e-mail para alertas de faturamento.",
        "solution": ["resource-manager", "eventarc", [["billing", "cloud-functions"], ["service-usage", "cloud-functions"], ["billing-notifications", "cloud-functions"]]],
        "availableServices": ["resource-manager", "eventarc", "billing", "cloud-functions", "service-usage"],
        "explanation": "O Eventarc, escutando os logs de auditoria do Resource Manager, aciona três Cloud Functions em paralelo para automatizar o setup do novo projeto: uma para o faturamento, uma para habilitar APIs e uma para configurar notificações."
    },
    {
        "category": "cloud-operations",
        "difficulty": 4,
        "question": "Um novo cluster GKE é criado. Em paralelo, instale um agente de monitoramento (ex: Datadog), um agente de segurança (ex: Falco) e configure o Backup for GKE.",
        "solution": ["gke", "eventarc", [["cloud-build-monitoring"], ["cloud-build-security"], ["backup-for-gke", "cloud-functions"]]],
        "availableServices": ["gke", "eventarc", "cloud-build", "cloud-functions", "backup-for-gke"],
        "explanation": "O evento de criação do cluster GKE no Eventarc aciona três fluxos paralelos. Dois deles são pipelines do Cloud Build que usam Helm/kubectl para instalar os agentes. O terceiro é uma Cloud Function que usa a API para configurar o serviço Backup for GKE para o novo cluster."
    },
    {
        "category": "cloud-operations",
        "difficulty": 5,
        "question": "Ao detectar um vazamento de credenciais via Security Command Center, em paralelo, revogue a chave de API vazada no Secret Manager, force o logout do usuário associado no Cloud Identity e inicie uma varredura de todos os recursos para detectar uso indevido.",
        "solution": ["security-command-center", "pub-sub", [["secret-manager", "cloud-functions"], ["cloud-identity", "cloud-functions"], ["asset-inventory", "cloud-functions"]]],
        "availableServices": ["security-command-center", "pub-sub", "secret-manager", "cloud-identity", "asset-inventory", "cloud-functions"],
        "explanation": "O achado do Security Command Center é exportado para o Pub/Sub. Três Cloud Functions com a mesma assinatura reagem em paralelo: uma desabilita o segredo, uma força o logout do usuário e uma terceira usa o Cloud Asset Inventory para buscar por atividades recentes da credencial comprometida."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um stream de eventos chega ao Pub/Sub. Em paralelo, processe-o com Dataflow para análise em tempo real e armazene-o no Cloud Storage para arquivamento e reprocessamento em lote.",
        "solution": ["pub-sub", [["dataflow", "bigquery"], ["cloud-storage"]]],
        "availableServices": ["pub-sub", "dataflow", "cloud-storage", "bigquery", "dataproc"],
        "explanation": "O Pub/Sub permite múltiplas assinaturas em um tópico. Uma assinatura é consumida por um pipeline de streaming do Dataflow que processa e grava os resultados no BigQuery. Uma segunda assinatura, do tipo 'Export to Cloud Storage', armazena os mesmos eventos brutos no GCS, de forma paralela e independente."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um pipeline do Cloud Composer conclui com sucesso. Em paralelo, atualize uma tabela de metadados de execução no BigQuery e envie uma notificação para um canal do Slack.",
        "solution": ["composer", [["bigquery-operator"], ["slack-operator"]]],
        "availableServices": ["composer", "bigquery-operator", "slack-operator", "dataflow", "cloud-functions"],
        "explanation": "Dentro de um DAG do Airflow (Cloud Composer), você pode definir tarefas que não têm dependências entre si. A tarefa final do seu pipeline principal pode ter duas tarefas downstream paralelas: um `BigQueryExecuteQueryOperator` para o log e um `SlackWebhookOperator` para a notificação."
    },
    {
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Ingira dados de CDC do AlloyDB usando o Datastream. Em paralelo, replique as alterações para o BigQuery para análise e para o Cloud Spanner para uma API de baixa latência.",
        "solution": ["alloydb", "datastream", [["bigquery"], ["cloud-storage", "dataflow", "spanner"]]],
        "availableServices": ["alloydb", "datastream", "bigquery", "dataflow", "spanner", "cloud-storage"],
        "explanation": "O Datastream captura as alterações do AlloyDB e as grava no Cloud Storage. A partir daí, dois processos paralelos são acionados: a integração nativa do Datastream com o BigQuery para replicar os dados e um pipeline do Dataflow que lê os mesmos arquivos de alteração do GCS e os aplica ao Cloud Spanner."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um novo arquivo Avro chega ao Cloud Storage. Em paralelo, inicie um job do Dataproc para processá-lo com Spark e inicie um scan do Dataplex para catalogar e verificar a qualidade dos dados.",
        "solution": ["cloud-storage", "eventarc", [["dataproc"], ["dataplex"]]],
        "availableServices": ["cloud-storage", "eventarc", "dataproc", "dataplex", "dataflow", "bigquery"],
        "explanation": "O Eventarc captura o evento de criação do arquivo e aciona duas ações paralelas: um webhook que chama a API do Dataproc para submeter um job Spark, e outro que chama a API do Dataplex para iniciar uma tarefa de scan de dados e qualidade sobre o novo ativo."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Uma mensagem de 'novo pedido' é publicada em um tópico do Pub/Sub. Crie duas assinaturas: uma para uma Cloud Function que processa o pagamento e outra para uma que atualiza o inventário.",
        "solution": ["pub-sub", [["cloud-functions-payment"], ["cloud-functions-inventory"]]],
        "availableServices": ["pub-sub", "cloud-functions-payment", "cloud-functions-inventory", "workflows", "tasks"],
        "explanation": "O padrão 'fan-out' do Pub/Sub é perfeito para isso. O tópico recebe o evento do pedido. Duas assinaturas independentes são criadas, cada uma acionando uma Cloud Function diferente. Isso desacopla os microsserviços de pagamento e inventário, permitindo que eles processem o evento em paralelo."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Ao receber um vídeo, use a Video Intelligence AI para extrair rótulos e detectar mudanças de cena. Em paralelo, use o Dataflow para transcodificar o vídeo em diferentes resoluções.",
        "solution": ["cloud-storage", "eventarc", [["video-intelligence-ai"], ["dataflow"]]],
        "availableServices": ["cloud-storage", "eventarc", "video-intelligence-ai", "dataflow", "cloud-functions"],
        "explanation": "O upload do vídeo no GCS aciona dois fluxos paralelos via Eventarc. O primeiro invoca uma Cloud Function que chama a API de anotação da Video Intelligence AI. O segundo inicia um pipeline do Dataflow que usa bibliotecas como FFmpeg para realizar a transcodificação."
    },
    {
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete uma arquitetura de cache 'write-around'. Uma API grava dados diretamente no Bigtable e, em paralelo, envia uma mensagem de invalidação para um tópico do Pub/Sub, que notifica os serviços para invalidarem seus caches no Memorystore.",
        "solution": ["api", "cloud-run", [["bigtable"], ["pub-sub", "memorystore"]]],
        "availableServices": ["cloud-run", "bigtable", "pub-sub", "memorystore", "spanner"],
        "explanation": "O serviço em Cloud Run executa duas tarefas assíncronas: a escrita no Bigtable (a fonte da verdade) e a publicação da chave do dado alterado no Pub/Sub. Múltiplos serviços que usam o Memorystore como cache podem ter assinaturas neste tópico para saberem quando invalidar suas entradas de cache."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um pipeline do Data Fusion conclui a ingestão de dados. Em paralelo, ele aciona uma consulta no BigQuery para materializar uma agregação e chama um webhook para notificar um sistema externo.",
        "solution": ["data-fusion", [["bigquery"], ["webhook"]]],
        "availableServices": ["data-fusion", "bigquery", "webhook", "cloud-functions", "composer"],
        "explanation": "No final de um pipeline do Data Fusion, você pode adicionar 'post-run actions'. É possível configurar múltiplas ações para serem executadas em paralelo, como um 'BigQuery Executor' para rodar a consulta e um 'HTTP Caller' para acionar o webhook."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Ingira dados do Salesforce usando um template do Dataflow. Em paralelo, armazene uma cópia bruta no Cloud Storage e carregue os dados em uma tabela do BigQuery.",
        "solution": ["dataflow", "salesforce", [["cloud-storage"], ["bigquery"]]],
        "availableServices": ["dataflow", "salesforce", "cloud-storage", "bigquery", "data-fusion"],
        "explanation": "Dentro de um pipeline do Apache Beam (Dataflow), você pode ter uma única fonte (Salesforce) e, após a leitura inicial, criar duas ramificações no seu PCollection. Uma ramificação usa um 'sink' para gravar os dados brutos no GCS, e a outra ramificação usa o sink do BigQuery para gravar os mesmos dados, permitindo processamento paralelo."
    },
    {
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Um modelo do Vertex AI é retreinado. Em paralelo, implante-o em um endpoint online para inferência em tempo real e inicie uma execução de 'Batch Prediction' para pontuar dados históricos, e registre as métricas do novo modelo.",
        "solution": ["vertex-ai", "eventarc", [["vertex-ai-endpoint"], ["vertex-ai-batch"], ["monitoring", "cloud-functions"]]],
        "availableServices": ["vertex-ai", "eventarc", "monitoring", "cloud-functions", "cloud-build"],
        "explanation": "O evento de conclusão do treinamento no Vertex AI é capturado pelo Eventarc. Três Cloud Functions reagem em paralelo: uma usa o SDK para criar um endpoint e implantar o modelo, outra inicia um job de predição em lote, e uma terceira extrai as métricas de avaliação do job de treinamento e as publica no Cloud Monitoring."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Ao receber um evento de negócio no Pub/Sub, inicie um Cloud Workflow para orquestração complexa e, em paralelo, enfileire uma tarefa no Cloud Tasks para um processamento de baixa prioridade.",
        "solution": ["pub-sub", [["workflows"], ["tasks"]]],
        "availableServices": ["pub-sub", "workflows", "tasks", "cloud-functions", "eventarc"],
        "explanation": "Duas assinaturas 'push' são criadas para o mesmo tópico do Pub/Sub. Uma tem como alvo um endpoint HTTP que inicia o Cloud Workflow. A outra tem como alvo uma Cloud Function que cria uma nova tarefa no Cloud Tasks. Ambos os sistemas reagem ao mesmo evento de forma paralela e desacoplada."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Um pipeline do Composer precisa executar dois jobs do Dataproc em paralelo (um para processar dados de clientes, outro para produtos). Se ambos forem bem-sucedidos, ele executa um job do BigQuery para juntar os resultados.",
        "solution": ["composer", [["dataproc-job1"], ["dataproc-job2"]], "bigquery-job"],
        "availableServices": ["composer", "dataproc", "bigquery", "dataflow"],
        "explanation": "Em um DAG do Airflow (Composer), você pode definir as dependências das tarefas. O `DataprocSubmitJobOperator` para clientes e produtos não teriam dependências entre si. O `BigQueryExecuteQueryOperator` teria como dependência upstream a conclusão bem-sucedida de ambos os jobs do Dataproc."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Ao receber um arquivo de áudio, transcreva-o com a API Speech-to-Text e, em paralelo, identifique o locutor (speaker diarization). Armazene a transcrição segmentada por locutor.",
        "solution": ["cloud-storage", "cloud-functions", "speech-to-text-ai"],
        "availableServices": ["cloud-storage", "cloud-functions", "speech-to-text-ai", "natural-language-ai"],
        "explanation": "A API Speech-to-Text do Google Cloud pode realizar a transcrição e a diarização de locutor em uma única chamada de API assíncrona, mas conceitualmente são duas tarefas paralelas. A Cloud Function, acionada pelo upload, faz a chamada e, ao receber o resultado, armazena a transcrição já com os rótulos de quem falou o quê."
    },
    {
        "category": "data-engineering",
        "difficulty": 5,
        "question": "Projete uma arquitetura de banco de dados que suporte, em paralelo, transações OLTP de alta performance (via Cloud SQL) e consultas analíticas (via BigQuery), mantendo os dados sincronizados.",
        "solution": ["cloud-sql", "datastream", "bigquery"],
        "availableServices": ["cloud-sql", "datastream", "bigquery", "spanner", "dataflow"],
        "explanation": "O Cloud SQL serve como o banco de dados transacional. O Datastream é usado para capturar as alterações (CDC) do Cloud SQL em tempo real. O Datastream então replica essas alterações para o BigQuery de forma contínua, permitindo que as consultas analíticas pesadas sejam executadas no BigQuery sem impactar a performance do banco de dados OLTP."
    },
    {
        "category": "data-engineering",
        "difficulty": 3,
        "question": "Um novo pipeline é catalogado no Data Catalog. Em paralelo, acione um job do Dataflow para começar a popular os dados e envie uma notificação para a equipe de governança de dados.",
        "solution": ["data-catalog", "eventarc", [["dataflow"], ["pub-sub", "cloud-functions"]]],
        "availableServices": ["data-catalog", "eventarc", "dataflow", "pub-sub", "cloud-functions", "dataplex"],
        "explanation": "O Eventarc pode ser configurado para reagir a eventos de auditoria do Data Catalog. O evento de criação aciona duas rotas paralelas: uma que inicia o pipeline do Dataflow e outra que publica uma mensagem no Pub/Sub para notificação assíncrona via Cloud Function."
    },
    {
        "category": "data-engineering",
        "difficulty": 4,
        "question": "Orquestre um pipeline no Data Fusion que lê dados de uma fonte, e em paralelo, os grava no BigQuery e no Spanner.",
        "solution": ["data-fusion", "source", [["bigquery"], ["spanner"]]],
        "availableServices": ["data-fusion", "bigquery", "spanner", "cloud-sql"],
        "explanation": "Na UI do Data Fusion, após o estágio de transformação, você pode arrastar dois 'sinks' (destinos) diferentes, um para o BigQuery e outro para o Spanner, e conectá-los a partir do mesmo estágio anterior. O Data Fusion executará as duas operações de escrita em paralelo."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Um novo conjunto de dados de vendas é carregado no BigQuery. Em paralelo, atualize um dashboard no Looker e execute um pipeline do Vertex AI para retreinar um modelo de previsão de demanda.",
        "solution": ["bigquery", "eventarc", [["looker", "cloud-functions"], ["vertex-ai"]]],
        "availableServices": ["bigquery", "eventarc", "looker", "vertex-ai", "cloud-functions", "dataflow"],
        "explanation": "O Eventarc pode ser acionado por jobs de carga do BigQuery. Ele invoca duas ações paralelas: uma Cloud Function que usa a API do Looker para acionar a atualização de um dashboard, e outra que inicia a execução de um pipeline de treinamento no Vertex AI."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Ao receber um post de um fórum, use a API Natural Language para analisar o sentimento e, em paralelo, para extrair as entidades mencionadas. Armazene ambos os resultados.",
        "solution": ["pub-sub", "cloud-functions", "natural-language-ai", [["sentiment"], ["entities"]]],
        "availableServices": ["pub-sub", "cloud-functions", "natural-language-ai", "bigquery"],
        "explanation": "Uma única Cloud Function, acionada pelo Pub/Sub, recebe o texto. Ela faz duas chamadas assíncronas e paralelas para a API Natural Language: uma para a análise de sentimento e outra para o reconhecimento de entidades. Quando ambas as respostas retornam, a função as consolida e armazena."
    },
    {
        "category": "data-analysis",
        "difficulty": 5,
        "question": "Um analista submete uma consulta no BigQuery. Em paralelo à execução da consulta, um sistema de FinOps (via Cloud Function) analisa o custo estimado da consulta e a cancela se exceder um limite, enquanto outro sistema registra a consulta para auditoria.",
        "solution": ["bigquery", "eventarc", [["cloud-functions-finops"], ["logging", "bigquery-audit"]]],
        "availableServices": ["bigquery", "eventarc", "cloud-functions-finops", "logging", "iam"],
        "explanation": "O Eventarc, escutando o log de auditoria de início de job do BigQuery, aciona uma Cloud Function de FinOps que analisa os 'query plan' e pode cancelar o job. Em paralelo, o próprio Cloud Logging já captura o log de auditoria, que pode ter um 'sink' configurado para enviá-lo a uma tabela de auditoria do BigQuery."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Crie um sistema de recomendação de notícias. Para um novo artigo, em paralelo, use a API Natural Language para categorizá-lo e a API Vision AI para analisar a imagem principal. Os resultados combinados são usados para alimentar o motor de recomendação.",
        "solution": ["cloud-storage", "cloud-functions", [["natural-language-ai"], ["vision-ai"]], "vertex-ai"],
        "availableServices": ["cloud-storage", "cloud-functions", "natural-language-ai", "vision-ai", "vertex-ai"],
        "explanation": "Uma Cloud Function é acionada pelo novo artigo. Ela executa duas chamadas de API de IA em paralelo: uma para categorizar o texto e outra para extrair rótulos da imagem. Com os resultados combinados, ela chama o sistema de recomendação (ex: Vertex AI Matching Engine) para atualizar as recomendações."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Um relatório agendado é gerado no Looker. Em paralelo, envie o relatório por e-mail para os diretores e exporte os dados brutos do relatório para o Cloud Storage para arquivamento.",
        "solution": ["looker", "webhook", "cloud-functions", [["email-api"], ["cloud-storage"]]],
        "availableServices": ["looker", "webhook", "cloud-functions", "cloud-storage", "pub-sub"],
        "explanation": "O agendador do Looker pode ser configurado para chamar um webhook. O webhook aciona uma Cloud Function. A função então usa a API do Looker para executar duas ações paralelas: uma para enviar o relatório por e-mail e outra para buscar os dados subjacentes e salvá-los no Cloud Storage."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Para uma análise de vídeo, ao receber um arquivo, em paralelo, use a API Speech-to-Text para obter a transcrição e a API Video Intelligence para detectar objetos. Junte os resultados para criar metadados pesquisáveis.",
        "solution": ["cloud-storage", "workflows", [["speech-to-text-ai"], ["video-intelligence-ai"]], "bigquery"],
        "availableServices": ["cloud-storage", "workflows", "speech-to-text-ai", "video-intelligence-ai", "bigquery"],
        "explanation": "Um Cloud Workflow é acionado pelo upload. Ele usa um passo 'parallel' para iniciar as duas análises de IA assíncronas. O workflow aguarda a conclusão de ambas, combina a transcrição e os rótulos de objeto, e então insere os metadados consolidados em uma tabela do BigQuery."
    },
    {
        "category": "data-analysis",
        "difficulty": 5,
        "question": "Ao detectar uma fraude com um modelo do BigQuery ML, em paralelo, adicione o usuário a uma lista de 'observação' no Firestore, inicie um Cloud Workflow para o processo de revisão humana e gere um alerta no Security Command Center.",
        "solution": ["bigquery-ml", "eventarc", [["firestore", "cloud-functions"], ["workflows"], ["security-command-center", "cloud-functions"]]],
        "availableServices": ["bigquery-ml", "eventarc", "firestore", "workflows", "security-command-center", "cloud-functions"],
        "explanation": "A execução da predição em lote do BQML gera um log que pode ser capturado pelo Eventarc. Três ações paralelas são acionadas: uma Function para atualizar o Firestore, uma chamada para iniciar o Workflow de revisão e outra Function para criar um 'finding' customizado no Security Command Center."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Um usuário envia um feedback por texto. Em paralelo, traduza o feedback para o inglês com a API Translate e identifique as entidades-chave com a API Natural Language.",
        "solution": ["cloud-functions", [["translate-ai"], ["natural-language-ai"]]],
        "availableServices": ["cloud-functions", "translate-ai", "natural-language-ai", "pub-sub"],
        "explanation": "Uma única Cloud Function recebe o texto. Ela faz duas chamadas de API assíncronas e paralelas para os serviços de IA: uma para tradução e outra para reconhecimento de entidades. Isso permite que ambas as análises aconteçam simultaneamente, otimizando o tempo de resposta."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Para uma análise de campanha, uma DAG do Composer é acionada. Em paralelo, ela busca dados de performance do Google Ads (via BigQuery DTS) e dados de engajamento de um banco de dados interno (via Dataflow). Após ambos terminarem, uma tarefa final une os dados.",
        "solution": ["composer", [["bigquery-dts-operator"], ["dataflow-operator"]], "bigquery-join-operator"],
        "availableServices": ["composer", "bigquery-dts-operator", "dataflow-operator", "bigquery-join-operator"],
        "explanation": "No DAG do Airflow (Composer), as tarefas que acionam o BigQuery DTS e o Dataflow são independentes. A tarefa final, que executa uma consulta de junção no BigQuery, é configurada para ter como 'upstream' a conclusão bem-sucedida de ambas as tarefas de ingestão, garantindo que elas sejam executadas em paralelo."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Ao receber uma imagem de um produto, use a API Vision AI para obter rótulos de classificação e, em paralelo, use a mesma API para detectar o texto (OCR) na embalagem do produto.",
        "solution": ["cloud-storage", "cloud-functions", "vision-ai", [["label-detection"], ["text-detection"]]],
        "availableServices": ["cloud-storage", "cloud-functions", "vision-ai", "natural-language-ai"],
        "explanation": "A API Vision AI pode realizar múltiplas tarefas. Uma Cloud Function acionada pelo upload pode fazer uma única chamada à API solicitando, em paralelo, os recursos de 'Label Detection' e 'Text Detection' (OCR). A API processa ambas as solicitações e retorna um único resultado com ambas as análises."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Um novo conjunto de dados geoespaciais (GeoJSON) é carregado no Cloud Storage. Em paralelo, carregue-o no BigQuery para análise SQL e gere um mapa de calor visual usando uma ferramenta de mapeamento.",
        "solution": ["cloud-storage", "eventarc", [["bigquery", "cloud-functions"], ["maps-platform", "cloud-functions"]]],
        "availableServices": ["cloud-storage", "eventarc", "bigquery", "maps-platform", "cloud-functions"],
        "explanation": "O Eventarc aciona duas Cloud Functions em paralelo a partir do upload. A primeira carrega os dados GeoJSON para uma tabela do BigQuery que suporta o tipo de dados GEOGRAPHY. A segunda usa uma biblioteca do lado do servidor (ex: com a API do Google Maps) para gerar uma imagem estática de um mapa de calor."
    },
    {
        "category": "data-analysis",
        "difficulty": 5,
        "question": "Para análise de portfólio, uma Cloud Function é acionada. Em paralelo, ela busca dados de mercado em tempo real de uma fonte Pub/Sub e dados de posição do cliente de um banco Spanner. Com ambos os dados, ela calcula o risco e o retorna.",
        "solution": ["cloud-functions", [["pub-sub-pull"], ["spanner-query"]], "risk-calculation"],
        "availableServices": ["cloud-functions", "pub-sub", "spanner", "bigquery", "workflows"],
        "explanation": "A Cloud Function inicia duas operações de I/O assíncronas em paralelo: uma para puxar a última mensagem do Pub/Sub e outra para consultar o Spanner. Usando promessas (em Node.js) ou corrotinas (em Python), a função aguarda a conclusão de ambas as buscas antes de executar a lógica de cálculo de risco com os dados combinados."
    },
    {
        "category": "data-analysis",
        "difficulty": 3,
        "question": "Um analista precisa de dados do Google Analytics e do Salesforce. Em paralelo, use o BigQuery DTS para buscar os dados do GA e o Dataflow para buscar os dados do Salesforce, unificando ambos no BigQuery.",
        "solution": [["bigquery-dts", "google-analytics"], ["dataflow", "salesforce"], "bigquery"],
        "availableServices": ["bigquery-dts", "dataflow", "bigquery", "composer", "salesforce", "google-analytics"],
        "explanation": "Essa arquitetura, orquestrada por um Cloud Composer, pode iniciar duas tarefas de ingestão em paralelo. Uma aciona uma transferência do BigQuery Data Transfer Service para o Google Analytics. A outra inicia um pipeline do Dataflow que usa o conector do Salesforce. Ambas as tarefas gravam seus resultados em tabelas separadas no BigQuery."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Ao receber um documento, extraia o texto com a Document AI e, em paralelo, identifique o idioma do documento com a API Translate. Use o idioma detectado para guiar a análise de entidades.",
        "solution": ["cloud-storage", "cloud-functions", [["document-ai"], ["translate-ai"]], "natural-language-ai"],
        "availableServices": ["cloud-storage", "cloud-functions", "document-ai", "translate-ai", "natural-language-ai"],
        "explanation": "Uma Cloud Function é acionada e faz duas chamadas paralelas: uma para extrair o texto bruto e outra, muito mais rápida, para detectar o idioma. O resultado da detecção de idioma é então usado como um parâmetro na chamada subsequente à API Natural Language para a análise de entidades, garantindo maior precisão."
    },
    {
        "category": "data-analysis",
        "difficulty": 5,
        "question": "Um modelo de 'propensity to buy' é executado no Vertex AI. Para os clientes com alta propensão, em paralelo, adicione-os a uma audiência de remarketing no Google Ads e envie seus detalhes para uma campanha de e-mail marketing.",
        "solution": ["vertex-ai", "pub-sub", [["google-ads-api", "cloud-functions"], ["email-marketing-api", "cloud-functions"]]],
        "availableServices": ["vertex-ai", "pub-sub", "cloud-functions", "google-ads-api", "bigquery"],
        "explanation": "O job de predição em lote do Vertex AI publica os IDs dos clientes com alta propensão em um tópico do Pub/Sub. Duas Cloud Functions, com a mesma assinatura, reagem em paralelo: uma usa a API do Google Ads para adicionar os clientes à audiência, e a outra usa a API do serviço de e-mail marketing para iniciar a campanha."
    },
    {
        "category": "data-analysis",
        "difficulty": 4,
        "question": "Para análise de concorrência, um pipeline é executado. Em paralelo, ele extrai menções da marca da web (usando uma API de terceiros) e analisa o sentimento de tweets recentes (usando a API Natural Language). Os resultados são armazenados no BigQuery.",
        "solution": ["composer", [["http-operator", "cloud-functions"], ["twitter-api", "natural-language-ai", "cloud-functions"]], "bigquery"],
        "availableServices": ["composer", "cloud-functions", "natural-language-ai", "bigquery", "http-operator"],
        "explanation": "Um DAG do Cloud Composer pode ter duas ramificações paralelas. Uma usa o `HttpOperator` para chamar a API da web. A outra usa o `PythonOperator` para chamar as APIs do Twitter e da Natural Language. Uma tarefa final, que depende do sucesso de ambas, consolida e carrega os dados no BigQuery."
    }]
        };


        // --- STATE ---
        let currentPlatform = null;
        let currentObjective = null;
        let selectedDifficulty = null;
        let currentScenario = null;
        let draggedServiceId = null;
        
        let currentQuiz = {
            questions: [],
            currentIndex: 0,
            totalQuestions: 5,
            results: []
        };

        // --- DOM ELEMENTS ---
        const platformSelectionScreen = document.getElementById('platform-selection-screen');
        const objectiveSelectionScreen = document.getElementById('objective-selection-screen');
        const difficultySelectionScreen = document.getElementById('difficulty-selection-screen');
        const mainSimulationScreen = document.getElementById('main-simulation-screen');
        const platformName = document.getElementById('platform-name');
        const scenarioQuestion = document.getElementById('scenario-question');
        const servicesPalette = document.getElementById('services-palette');
        const pipelineCanvas = document.getElementById('pipeline-canvas');
        const feedbackModal = document.getElementById('feedback-modal');
        const feedbackTitle = document.getElementById('feedback-title');
        const feedbackMessage = document.getElementById('feedback-message');
        const feedbackButton = document.getElementById('feedback-button');
        const pixModal = document.getElementById('pix-modal');
        const confirmExitModal = document.getElementById('confirm-exit-modal');
        const quizSummaryModal = document.getElementById('quiz-summary-modal');
        const summaryContent = document.getElementById('summary-content');
        const quizProgress = document.getElementById('quiz-progress');

        function selectPlatform(platform) {
            currentPlatform = platform;
            platformSelectionScreen.classList.add('hidden');
            objectiveSelectionScreen.classList.remove('hidden');
        }

        function selectObjective(objective) {
            currentObjective = objective;
            objectiveSelectionScreen.classList.add('hidden');
            difficultySelectionScreen.classList.remove('hidden');
        }

        function selectDifficulty(level) {
            selectedDifficulty = level;
            if(generateQuiz()){
                showPixModal('start');
            }
        }

        function generateQuiz() {
            currentQuiz.totalQuestions = 5; 
            let availableScenarios = scenarios[currentPlatform].filter(s => s.category === currentObjective && s.difficulty === selectedDifficulty);
            
            availableScenarios.sort(() => 0.5 - Math.random());
            currentQuiz.questions = availableScenarios.slice(0, 2); 
            currentQuiz.totalQuestions = currentQuiz.questions.length;
            currentQuiz.currentIndex = 0;
            currentQuiz.results = [];

            if (currentQuiz.questions.length === 0) {
               alert(`Nenhuma questão encontrada para a plataforma, objetivo e nível selecionados. Por favor, tente outra combinação.`);
               return false;
            }
            
            return true;
        }
        
        function showPixModal(context) {
            const primaryBtn = document.getElementById('pix-primary-button');
            const secondaryBtn = document.getElementById('pix-secondary-button');

            if (context === 'start') {
                primaryBtn.textContent = 'Iniciar Simulado';
                primaryBtn.onclick = () => {
                    pixModal.classList.add('hidden');
                    difficultySelectionScreen.classList.add('hidden');
                    mainSimulationScreen.classList.remove('hidden');
                    platformName.textContent = currentPlatform;
                    startChallenge(); 
                };
                secondaryBtn.textContent = 'Fechar';
                 secondaryBtn.onclick = () => pixModal.classList.add('hidden');

            } else if (context === 'end') {
                primaryBtn.textContent = 'Jogar Novamente';
                primaryBtn.onclick = () => {
                    pixModal.classList.add('hidden');
                    confirmExit(); 
                };
                secondaryBtn.textContent = 'Fechar';
                 secondaryBtn.onclick = () => pixModal.classList.add('hidden');
            }
            pixModal.classList.remove('hidden');
        }


        function startChallenge() {
            if (currentQuiz.questions.length === 0) return;

            currentScenario = currentQuiz.questions[currentQuiz.currentIndex];
            quizProgress.textContent = `Questão ${currentQuiz.currentIndex + 1} de ${currentQuiz.totalQuestions}`;
            
            scenarioQuestion.textContent = currentScenario.question;
            populateServices();
            createDropZones();
        }

        function nextQuestion() {
            feedbackModal.classList.add('hidden');
            currentQuiz.currentIndex++;
            if (currentQuiz.currentIndex < currentQuiz.totalQuestions) {
                startChallenge();
            } else {
                showQuizSummary();
            }
        }
        
        function goBack(event) {
            if(event) event.preventDefault();
            const isPlaying = currentPlatform !== null && !mainSimulationScreen.classList.contains('hidden');
            if (isPlaying) {
                confirmExitModal.classList.remove('hidden');
            } else {
                confirmExit();
            }
        }

        function confirmExit() {
            confirmExitModal.classList.add('hidden');
            quizSummaryModal.classList.add('hidden');
            mainSimulationScreen.classList.add('hidden');
            difficultySelectionScreen.classList.add('hidden');
            objectiveSelectionScreen.classList.add('hidden');
            platformSelectionScreen.classList.remove('hidden');

            currentPlatform = null;
            currentObjective = null;
            selectedDifficulty = null;
            currentQuiz.results = [];
            currentQuiz.totalQuestions = 5;
        }
        
        function populateServices() {
            servicesPalette.innerHTML = '';
            const platformServices = services[currentPlatform];
            
            currentScenario.availableServices.forEach(serviceId => {
                const service = platformServices[serviceId];
                if (!service) return;
                const serviceEl = document.createElement('div');
                serviceEl.id = serviceId;
                serviceEl.dataset.originalId = serviceId;
                serviceEl.draggable = true;
                serviceEl.className = 'service-icon flex flex-col items-center justify-center p-3 bg-slate-200 dark:bg-slate-700 rounded-lg shadow-sm hover:bg-slate-300 dark:hover:bg-slate-600';
                
                const invertClass = service.invertInDark ? 'dark:brightness-0 dark:invert' : '';

                const iconHtml = `
                    <div class="h-1/2 flex items-center justify-center">
                        <img src="${service.icon}" class="max-h-full max-w-full object-contain pointer-events-none filter ${invertClass}" alt="${service.name} icon" onerror="this.onerror=null;this.src='https://placehold.co/40x40/E2E8F0/475569?text=?';this.classList.remove('dark:invert');">
                    </div>
                `;

                serviceEl.innerHTML = `
                    ${iconHtml}
                    <div class="font-semibold text-xs sm:text-sm mt-1 text-center text-slate-800 dark:text-slate-200 pointer-events-none">${service.name}</div>
                `;
                serviceEl.addEventListener('dragstart', handleDragStart);
                servicesPalette.appendChild(serviceEl);
            });
        }
        
        function createDropZones() {
            pipelineCanvas.innerHTML = '';
            renderPipelineSegment(pipelineCanvas, currentScenario.solution);
        }

        function renderPipelineSegment(parentElement, solutionSegment) {
            solutionSegment.forEach((step, index) => {
                const stepContainer = document.createElement('div');
                stepContainer.className = 'pipeline-step flex items-center justify-center flex-1 min-w-0 h-full';

                if (typeof step === 'object' && !Array.isArray(step) && step.orchestrator) {
                    stepContainer.classList.add('self-stretch');
                    const orchestratorWrapper = document.createElement('div');
                    orchestratorWrapper.className = 'orchestrator-container w-full h-full flex flex-col border-2 border-dashed border-indigo-400 dark:border-indigo-600 rounded-xl p-4 bg-indigo-50 dark:bg-indigo-900/30';
                    const header = document.createElement('div');
                    header.className = 'flex items-center gap-2 mb-4 pb-4 border-b border-indigo-200 dark:border-indigo-800';
                    
                    const orchestratorDropZone = createSingleDropZone();
                    orchestratorDropZone.classList.add('orchestrator-target');
                    
                    header.appendChild(orchestratorDropZone);
                    
                    const innerCanvas = document.createElement('div');
                    innerCanvas.className = 'inner-canvas flex flex-nowrap items-center justify-start w-full flex-grow bg-white/50 dark:bg-slate-800/50 p-2 rounded-lg gap-2';
                    
                    orchestratorWrapper.appendChild(header);
                    orchestratorWrapper.appendChild(innerCanvas);
                    renderPipelineSegment(innerCanvas, step.flow);
                    stepContainer.appendChild(orchestratorWrapper);
                } else if (Array.isArray(step)) {
                    stepContainer.classList.add('flex-col', 'gap-2', 'self-stretch', 'py-2', 'justify-center');
                    step.forEach(branch => {
                        const branchContainer = document.createElement('div');
                        branchContainer.className = 'branch-container flex flex-nowrap items-center w-full gap-2';
                        branch.forEach((serviceId, serviceIndex) => {
                            const wrapper = document.createElement('div');
                            wrapper.className = 'flex-1 min-w-0';
                            const dropZone = createSingleDropZone();
                            wrapper.appendChild(dropZone);
                            branchContainer.appendChild(wrapper);

                            if (serviceIndex < branch.length - 1) {
                                branchContainer.appendChild(createArrow());
                            }
                        });
                        stepContainer.appendChild(branchContainer);
                    });
                } else {
                    const dropZone = createSingleDropZone();
                    stepContainer.appendChild(dropZone);
                }
                
                parentElement.appendChild(stepContainer);

                if (index < solutionSegment.length - 1) {
                    parentElement.appendChild(createArrow());
                }
            });
        }

        function createSingleDropZone() {
            const dropZone = document.createElement('div');
            dropZone.className = `drop-zone flex-shrink-0 flex items-center justify-center rounded-lg p-2 bg-white dark:bg-slate-800/50 border-slate-300 dark:border-slate-600 w-full max-w-[6rem] mx-auto`;
            dropZone.addEventListener('dragover', handleDragOver);
            dropZone.addEventListener('dragleave', handleDragLeave);
            dropZone.addEventListener('drop', handleDrop);
            return dropZone;
        }

        function createArrow() {
            const arrow = document.createElement('div');
            arrow.className = 'arrow mx-2 flex-shrink-0 text-slate-400 dark:text-slate-500';
            arrow.innerHTML = '&#10140;';
            return arrow;
        }
        
        function handleDragStart(e) {
            const serviceIcon = e.target.closest('.service-icon');
            if (serviceIcon) {
                draggedServiceId = serviceIcon.id;
                 e.dataTransfer.setData('text/plain', serviceIcon.id);
                 e.dataTransfer.effectAllowed = 'move';
            }
        }

        function handleDragOver(e) {
            e.preventDefault();
            const dropZone = e.target.closest('.drop-zone');
            if(dropZone) dropZone.classList.add('drag-over');
        }
        
        function handleDragLeave(e) {
            const dropZone = e.target.closest('.drop-zone');
            if(dropZone) dropZone.classList.remove('drag-over');
        }

        function handleDrop(e) {
            e.preventDefault();
            const dropZone = e.target.closest('.drop-zone');
            if (!dropZone) return;
            dropZone.classList.remove('drag-over');

            const draggedElId = e.dataTransfer.getData('text/plain');
            const draggedEl = document.getElementById(draggedElId);

            if (!draggedEl) return;

            const sourceContainer = draggedEl.parentElement;

            if (sourceContainer.id === 'services-palette') {
                if (dropZone.children.length > 0) return;
                const originalId = draggedEl.dataset.originalId;
                const clone = draggedEl.cloneNode(true);
                clone.id = `clone-${originalId}-${Date.now()}`;
                clone.dataset.originalId = originalId;
                clone.className = 'service-icon relative flex flex-col items-center justify-center p-2 bg-slate-200 dark:bg-slate-700 rounded-lg shadow-sm w-full h-full';
                
                const removeBtn = document.createElement('button');
                removeBtn.innerHTML = '&times;';
                removeBtn.className = 'absolute top-1 right-1 bg-red-500 text-white rounded-full h-5 w-5 flex items-center justify-center text-xs hover:bg-red-700 transition-colors z-10';
                removeBtn.onclick = (event) => event.target.parentElement.remove();
                clone.appendChild(removeBtn);

                clone.addEventListener('dragstart', handleDragStart);
                dropZone.appendChild(clone);
            } 
            else if (sourceContainer.classList.contains('drop-zone')) {
                if (dropZone.children.length > 0) {
                    const existingEl = dropZone.children[0];
                    sourceContainer.appendChild(existingEl);
                    dropZone.appendChild(draggedEl);
                } else {
                    dropZone.appendChild(draggedEl);
                }
            }
        }
        
        function resetPipeline() {
            createDropZones();
        }

        function evaluateSolution() {
            function parseCanvas(canvasElement) {
                const solutionSegment = [];
                const children = Array.from(canvasElement.children).filter(el => el.classList.contains('pipeline-step'));
                
                children.forEach(container => {
                    const orchestratorWrapper = container.querySelector(':scope > .orchestrator-container');
                    if (orchestratorWrapper) {
                        const innerCanvas = orchestratorWrapper.querySelector('.inner-canvas');
                        const orchestratorDropZone = orchestratorWrapper.querySelector('.orchestrator-target .service-icon');
                        const orchestratorId = orchestratorDropZone ? orchestratorDropZone.dataset.originalId : null;
                        solutionSegment.push({
                            orchestrator: orchestratorId,
                            flow: parseCanvas(innerCanvas)
                        });
                    } else if (container.querySelectorAll(':scope > .branch-container').length > 0) {
                        const branches = [];
                        container.querySelectorAll(':scope > .branch-container').forEach(branch => {
                             const servicesInBranch = Array.from(branch.querySelectorAll('.drop-zone')).map(zone => {
                                const serviceEl = zone.querySelector('.service-icon');
                                return serviceEl ? serviceEl.dataset.originalId : null;
                            });
                            branches.push(servicesInBranch);
                        });
                        branches.sort((a,b) => a.join(',').localeCompare(b.join(',')));
                        solutionSegment.push(branches);
                    } else {
                        const serviceEl = container.querySelector('.drop-zone .service-icon');
                        solutionSegment.push(serviceEl ? serviceEl.dataset.originalId : null);
                    }
                });
                return solutionSegment;
            }

            const userSolution = parseCanvas(pipelineCanvas);
            const correctSolution = JSON.parse(JSON.stringify(currentScenario.solution));
            
            function sortSolution(solutionArray) {
                if (!solutionArray) return;
                solutionArray.forEach(step => {
                    if (Array.isArray(step)) {
                        step.sort((a, b) => a.join(',').localeCompare(b.join(',')));
                    } else if (typeof step === 'object' && step.flow) {
                        sortSolution(step.flow);
                    }
                });
            }

            sortSolution(correctSolution);
            
            const isCorrect = JSON.stringify(userSolution) === JSON.stringify(correctSolution);
            
            currentQuiz.results.push({
                question: currentScenario.question,
                solution: currentScenario.solution,
                userSolution: userSolution,
                isCorrect: isCorrect
            });

            showFeedback(isCorrect, userSolution, currentScenario.solution);
        }

       function createSolutionFlowHtml(solutionArray) {
            const platformServices = services[currentPlatform];
            const flowContainer = document.createElement('div');
            flowContainer.className = 'flex flex-nowrap items-center justify-start w-full p-2 bg-slate-100 dark:bg-slate-900/50 rounded-lg overflow-x-auto gap-2';

            function renderSegmentTo(parent, segment) {
                segment.forEach((step, index) => {
                    const stepContainer = document.createElement('div');
                    stepContainer.className = 'flex items-center justify-center flex-1 min-w-max';

                    if (typeof step === 'object' && !Array.isArray(step) && step.orchestrator) {
                        stepContainer.className += ' flex flex-col border border-indigo-400 p-2 rounded-md bg-slate-50 dark:bg-slate-700/50 gap-1 mx-2 self-stretch';
                        const header = document.createElement('div');
                        header.className = 'flex items-center gap-2 font-semibold text-indigo-800 dark:text-indigo-200 text-sm';
                        header.textContent = 'Orquestrado por:';
                        header.appendChild(createServiceElement(step.orchestrator, true));
                        stepContainer.appendChild(header);
                        
                        const innerFlowContainer = document.createElement('div');
                        innerFlowContainer.className = 'flex flex-nowrap items-center justify-start w-full mt-2 border-t border-indigo-200 dark:border-indigo-800 pt-2 gap-2';
                        stepContainer.appendChild(innerFlowContainer);
                        
                        renderSegmentTo(innerFlowContainer, step.flow);
                    } else if (Array.isArray(step)) {
                        stepContainer.className += ' flex flex-col gap-2 p-2 justify-center w-full';
                        step.forEach(branch => {
                            const branchContainer = document.createElement('div');
                            branchContainer.className = 'flex flex-nowrap items-center w-full gap-2';
                            branch.forEach((serviceId, serviceIndex) => {
                                const wrapper = document.createElement('div');
                                wrapper.className = 'flex-1 min-w-0';
                                const serviceEl = createServiceElement(serviceId);
                                wrapper.appendChild(serviceEl);
                                branchContainer.appendChild(wrapper);

                                if (serviceIndex < branch.length - 1) {
                                    branchContainer.appendChild(createArrowElement());
                                }
                            });
                            stepContainer.appendChild(branchContainer);
                        });
                    } else {
                        stepContainer.className += ' flex items-center justify-center';
                        stepContainer.appendChild(createServiceElement(step));
                    }
                    parent.appendChild(stepContainer);

                    if (index < segment.length - 1) {
                        parent.appendChild(createArrowElement());
                    }
                });
            }
            
            function createServiceElement(serviceId, isSmall = false) {
                const service = platformServices[serviceId];
                const serviceEl = document.createElement('div');
                const sizeClass = isSmall ? 'w-16 h-16 text-xs' : 'w-full max-w-[5rem] mx-auto text-xs';
        
                if(serviceId && service) {
                    serviceEl.className = `flex flex-col items-center justify-center bg-white dark:bg-slate-700 p-1 rounded shadow-sm text-slate-800 dark:text-slate-200 aspect-square ${sizeClass}`;
                    const invertClass = service.invertInDark ? 'dark:brightness-0 dark:invert' : '';
                    serviceEl.innerHTML = `
                        <img src="${service.icon}" class="h-1/2 object-contain filter ${invertClass}" alt="${service.name}" onerror="this.onerror=null;this.src='https://placehold.co/32x32/E2E8F0/475569?text=?';this.classList.remove('dark:invert');">
                        <span class="text-center mt-1">${service.name}</span>
                    `;
                } else {
                     serviceEl.className = `flex flex-col items-center justify-center bg-slate-200 dark:bg-slate-600 p-1 rounded shadow-sm aspect-square ${sizeClass}`;
                     serviceEl.innerHTML = `<span class="font-bold">[Vazio]</span>`;
                }
                return serviceEl;
            }
        
            function createArrowElement() {
                 const arrow = document.createElement('div');
                 arrow.className = 'arrow text-2xl mx-2 flex-shrink-0 text-slate-400 dark:text-slate-500';
                 arrow.innerHTML = '&#10140;';
                 return arrow;
            }
        
            renderSegmentTo(flowContainer, solutionArray);
            return flowContainer;
        }


        function showFeedback(isCorrect, userSolution, correctSolution) {
            feedbackModal.classList.remove('hidden');
            
            const userSolutionHtml = createSolutionFlowHtml(userSolution).outerHTML;
            const correctSolutionHtml = createSolutionFlowHtml(correctSolution).outerHTML;
            const defaultExplanation = 'Esta sequência garante um fluxo de dados lógico e eficiente para o problema proposto, onde cada etapa prepara a informação para a seguinte.';
            
            const isLastQuestion = currentQuiz.currentIndex + 1 >= currentQuiz.totalQuestions;

            if (isCorrect) {
                feedbackTitle.textContent = "Parabéns!";
                feedbackTitle.className = "text-3xl font-bold mb-4 text-center text-green-500";
                
                feedbackMessage.innerHTML = `
                    <div class="text-left space-y-4">
                        <p>Você construiu o pipeline corretamente!</p>
                        <div>
                            <p class="font-semibold text-slate-800 dark:text-white">Sua solução:</p>
                            <div class="mt-1 bg-green-100 dark:bg-green-900/50 p-3 rounded-lg text-green-800 dark:text-green-300 text-sm">${correctSolutionHtml}</div>
                        </div>
                        <div>
                            <p class="font-semibold text-slate-800 dark:text-white">Por que essa é a solução correta?</p>
                            <p class="mt-1 text-slate-600 dark:text-slate-300 text-sm">${currentScenario.explanation || defaultExplanation}</p>
                        </div>
                    </div>`;

                feedbackButton.textContent = isLastQuestion ? "Ver Resumo" : "Próximo Desafio";
                feedbackButton.onclick = nextQuestion;
                feedbackButton.className = "bg-green-600 hover:bg-green-700 text-white font-bold py-3 px-8 rounded-lg transition-colors w-full mt-6";
            } else {
                feedbackTitle.textContent = "Análise da Solução";
                feedbackTitle.className = "text-3xl font-bold mb-4 text-center text-orange-500";
                
                feedbackMessage.innerHTML = `
                    <div class="text-left space-y-4">
                         <p>Sua solução não está totalmente correta. A resposta certa foi apresentada para seu aprendizado.</p>
                         <div>
                            <p class="font-semibold text-slate-800 dark:text-white">Sua Resposta:</p>
                            <div class="mt-1 bg-red-100 dark:bg-red-900/50 p-3 rounded-lg text-red-800 dark:text-red-300 text-sm">${userSolutionHtml}</div>
                        </div>
                        <div>
                            <p class="font-semibold text-slate-800 dark:text-white">A Resposta Correta é:</p>
                            <div class="mt-1 bg-green-100 dark:bg-green-900/50 p-3 rounded-lg text-green-800 dark:text-green-300 text-sm">${correctSolutionHtml}</div>
                        </div>
                        <div>
                            <p class="font-semibold text-slate-800 dark:text-white">Por quê?</p>
                            <p class="mt-1 text-slate-600 dark:text-slate-300 text-sm">${currentScenario.explanation || defaultExplanation}</p>
                        </div>
                    </div>`;
                
                feedbackButton.textContent = isLastQuestion ? "Ver Resumo" : "Próxima Questão";
                feedbackButton.onclick = nextQuestion;
                feedbackButton.className = "bg-indigo-600 hover:bg-indigo-700 dark:bg-indigo-500 dark:hover:bg-indigo-600 text-white font-bold py-3 px-8 rounded-lg transition-colors w-full mt-6";
            }
        }

        function showQuizSummary() {
            mainSimulationScreen.classList.add('hidden');
            quizSummaryModal.classList.remove('hidden');
            summaryContent.innerHTML = ''; 

            currentQuiz.results.forEach((result, index) => {
                const resultCard = document.createElement('div');
                resultCard.className = 'p-4 border border-slate-200 dark:border-slate-700 rounded-lg';
                
                const questionTitle = document.createElement('h3');
                questionTitle.className = 'font-semibold text-lg mb-2 text-slate-800 dark:text-white';
                questionTitle.textContent = `Questão ${index + 1}: ${result.question}`;

                const statusIcon = document.createElement('span');
                statusIcon.className = `ml-2 font-bold ${result.isCorrect ? 'text-green-500' : 'text-red-500'}`;
                statusIcon.textContent = result.isCorrect ? '✓ Correto' : '✗ Incorreto';
                questionTitle.appendChild(statusIcon);
                
                const solutionTitle = document.createElement('p');
                solutionTitle.className = 'font-semibold mt-4 mb-2 text-slate-800 dark:text-white';
                solutionTitle.textContent = 'Fluxo correto da arquitetura:';

                const solutionFlow = createSolutionFlowHtml(result.solution);

                resultCard.appendChild(questionTitle);
                resultCard.appendChild(solutionTitle);
                resultCard.appendChild(solutionFlow);
                summaryContent.appendChild(resultCard);
            });
        }

    </script>

</body>
</html>

